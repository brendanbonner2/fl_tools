
    <html>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
    <link rel="stylesheet" type="css" href="styles/latex.css">
            <script type="text/x-mathjax-config">
        MathJax.Hub.Config({"HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"] },
            tex2jax: { inlineMath: [ ["$", "$"], ["\\\\(","\\\\)"] ], displayMath: [ ["$$","$$"], ["\\[", "\\]"] ], processEscapes: true, ignoreClass: "tex2jax_ignore|dno" },
            TeX: { noUndefined: { attributes: { mathcolor: "red", mathbackground: "#FFEEEE", mathsize: "90%" } } },
            messageStyle: "none"
        });
    </script>    
    <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/2.2-latest/MathJax.js?config=TeX-AMS_HTML"></script>       
    <body>
    <h1>Data Analytics and Data Mining</h1><p style="page-break-before: always"><h2>1.1&emsp;Welcome to the course</h2><div class="u-typography-bold-intro">
<p>Welcome to CA683 Data Analytics and Data Mining Course by Dublin City University.</p>
<p>If you watched the video above, you have already met the Lead Educator, Dr Andrew McCarren.  Over the next four topics, Andrew will introduce you to the basics of Data Analytics and Data Mining.</p>
<p>This course is part of the MSc in Computing (Artificial Intelligence) developed by Dublin City University, in collaboration with Technology Ireland ICT Skillnet and Technology Ireland Software Skillnet.</p>
<p>Data Analytics helps us to understand the huge volume of information, uncover patterns, trends, and better understand customers and markets. This course consists of five sections.</p>
<h3 id="course-syllabus">Course syllabus</h3>
<ol>
<li><strong>Introduction to Data Analytics</strong> will provide you with an overview of the basics of data analytics and data mining along with their relationship to statistics. During this course, you will be introduced to probability, regression and analysis of variance along with practical examples of their use.</li>
<li><strong>Pre-processing data and feature impact calculation</strong> explores techniques used to handle missing data, to detect anomalies for univariate and multivariate data sources, to transform and reduce datasets.</li>
<li><strong>Feature engineering</strong> explores and implements techniques that can help reduce the dimensionality of a dataset, and cluster datasets using unsupervised machine learning techniques.</li>
<li><strong>Introduction to processing unstructured data</strong> introduces learners to several approaches that allow them to incorporate images, text and graph data sources into data analysis.</li>
<li><strong>Point estimation &amp; feature impact calculation</strong> examines the Generalised Linear Models and two methods of parameter estimation.</li>
</ol>
<h3 id="introduction-to-data-mining-and-data-analytics---course-structure">Introduction to Data Mining and Data Analytics - Course Structure</h3>
<p>This the first of five courses in Data Analytics and Data Mining series is divided into three sections:</p>
<ol>
<li>
<p>The first topic will introduce you to the basic terminology and concepts that you should be familiar with to move forward with your learning. We will discuss what data mining and data analytics are and what their relationship is to statistics. We will also discuss the critical milestones in data analytics history. We close this topic by discussing several data analytics/mining standardisation processes and exploring how these standards can be used.</p>
</li>
<li>
<p>Topics 2 and 3 will do a quick recap on the introduction to probability, regression and analysis of variance. We have also included some examples to help you practice.</p>
</li>
<li>
<p>The final topic will bring it all together and synthesise this course material. It is only a few steps, but don’t misinterpret that - it will require loads of work. In that topic, you will need to work on the project.  You will be presented with a problem, and you will have an opportunity to solve it by applying your knowledge.</p>
</li>
</ol>
<p><img alt="Statement of funding from Skillnet Ireland informing that: This course has been grant-aided by skillnet Ireland. Technology Ireland ICT Skillnet provide advanced training and development activities for technical and engineering staff in the ICT. Participants may be contacted by Skillnet Ireland to confirm that thy attend this course.  
Information from susan.kelly@ictskillnet.com" sizes="(min-width: 1695px) 729px, (min-width: 680px) 648px, 320px" src="https://ugc.futurelearn.com/uploads/assets/14/0b/hero_140bfdc0-2f0f-408d-bf43-4ed1e4240646.jpg" srcset="https://ugc.futurelearn.com/uploads/assets/14/0b/small_hero_140bfdc0-2f0f-408d-bf43-4ed1e4240646.jpg 320w, https://ugc.futurelearn.com/uploads/assets/14/0b/hero_140bfdc0-2f0f-408d-bf43-4ed1e4240646.jpg 648w, https://ugc.futurelearn.com/uploads/assets/14/0b/large_hero_140bfdc0-2f0f-408d-bf43-4ed1e4240646.jpg 729w, https://ugc.futurelearn.com/uploads/assets/14/0b/large_hero_140bfdc0-2f0f-408d-bf43-4ed1e4240646.jpg 2x"/></p>
<hr/>
<h5 id="copyright-disclaimer">Copyright Disclaimer</h5>
<p><sub>All images used on this course are copyright by <a href="https://www.shutterstock.com/">Shutterstock</a> unless otherwise stated. Dublin City University does not knowingly intend or attempt to offend or violate any copyright or intellectual property rights of any entity. If images posted here are in violation of copyright law, please contact us and we will gladly remove the offending images immediately.</sub></p>
</div><p><h2>1.2&emsp;Assessment details</h2><div class="u-typography-bold-intro">
<h3 id="assessment-details">Assessment details</h3>
<p>CA683 Data Analytics and Data Mining is assessed through continuous assessment and a written exam.</p>
<p><strong>Continuous assessment</strong> - <strong>25%</strong></p>
<p>The continuous assessment portion of this course will account for 25% of the overall marks. Due date: <strong>Sunday 19th April</strong>.</p>
<p>You will be required to write a report/paper that examines the use, implementation and evaluation of a number of DM/DA techniques on the dataset of your choosing. The overarching focus of the project is to produce a critical analysis of the chosen techniques to answer a small scale research question.</p>
<p>In your analysis, you will follow either the KDD (Knowledge Discovery in Databases) or CRISP DM methodologies. To answer your research question make sure you do an extensive literature review and ensure you have over 100,000 records in your dataset.</p>
<p>The aim of the project is to help you incorporate the course material with your project and to keep on top of the work without falling behind.</p>
<h4 id="video-presentations">Video Presentations</h4>
<p>Starting from the second course (Pre-processing data and feature impact calculation), we will require you to <strong>record and post a short video</strong> (3 minutes max). You will also be able to watch and comment on each other’s presentations. 
We will post more detailed instructions in the next course, but in general, we will ask you to relate in the video to the material you are learning but also that you are not afraid to read ahead.</p>
<p><strong>Note</strong>: Throughout the course, you might also be asked to complete a number of quizzes, tests and exercises. These will not form part of your final mark (unless stated otherwise). I will be dropping into the comments to discuss these quizzes and exercises with all of you during the course.</p>
<h4 id="written-exam---75">Written exam - 75%</h4>
<p>Your written exam will account for 75% of the course mark. Sample exam papers (with solutions) are available on the <a href="https://loop.dcu.ie/enrol/index.php?id=44596">Loop page</a> for this course.</p>
</div><p><h2>1.3&emsp;Getting started </h2><div class="u-typography-bold-intro">
<p>Let’s get started with some necessary housekeeping.</p>
<p>If this is the first course you are starting on FutureLearn, you might need some basic introduction to the platform.</p>
<p>You may find it useful to go through this <a href="https://www.futurelearn.com/using-futurelearn">Using FutureLearn guide</a>  and read this <a href="https://about.futurelearn.com/blog/in-course-navigation/">FutureLearn blog post</a> explaining the in-course navigation.</p>
<h4 id="social-learning---discussions-and-peer-support">Social Learning - Discussions and Peer Support</h4>
<p>FutureLearn believes in the power of learning through conversation. Learners can leave comments and have a conversation on nearly every step in a course - just click on the pink plus symbol (+) to open the comments area. You’ll also notice Discussion steps - they offer a more structured dialogue on important topics.
We encourage you to ask questions, leave comments and participate in discussions. To get the most out of the interactive and social learning features of this course, we recommend reading this FutureLearn blog about <a href="https://about.futurelearn.com/blog/5-tips-tools-social-learning">tips and tools for social learning</a>.</p>
<h4 id="basic-rules-of-online-communication">Basic rules of online communication</h4>
<p>Leave comments, no essays (unless you are asked to write an essay!). Comments should be brief and to the point.</p>
<p>Remember that not all learners are native English speakers. Explain any acronyms you use and try to avoid jargon.</p>
<p>Be polite when you disagree with others. If you have to argue a point, make sure it is the idea, not the person you are criticising.   Remember that other learners come from different cultures and backgrounds; they are of different age and experience. When posting comments, please remember that these will be visible to all the learners registered on the course.  Make sure you only share information that you would be happy to be publicly visible. Don’t say anything that you wouldn’t say in person.</p>
<p>There is an option to report messages and users that are not adhering to the basic rules of online communication. If you find offensive content click the ‘Report’ flag icon. FutureLearn moderators will review it and remove if the comment is in breach of rules.</p>
<h4 id="additional-resources">Additional Resources</h4>
<p>From time to time you might see two additional sections at the end of the step - <strong>Downloads</strong> and <strong>See Also</strong>.</p>
<p><strong>Downloads</strong> are usually PDFs and are provided to help your learning. They might include transcripts, extracts, articles and information sheets that you may want to save for future reference.</p>
<p><strong>See Also</strong> are links to external pages, also provided to help your learning. Remember that any links that you click on within the FutureLearn platform will open in the same window and you will need to use your browser ‘back’ button to get to the course.</p>
<h4 id="progress-tracking">Progress Tracking</h4>
<p>When you are ready to move forward, make sure to click on the pink ‘<strong>Mark as complete</strong>’ button at the bottom of each step. This will update your progress page so you’ll be able to track which steps you’ve completed.</p>
<h4 id="quizzes">Quizzes</h4>
<p>To test your knowledge we’ve provided end-of-activity and end-of topic quizzes. Quizzes are included purely to help you learn.</p>
<h4 id="is-there-anything-else">Is there anything else?</h4>
<p>If you have read all the resources provided above, and still have questions, go to the <a href="https://about.futurelearn.com/about/faq/?category=course-sign-up-and-completion">FutureLearn’s FAQs</a>. You can also contact FutureLearn using the support buttons below.</p>
</div><p><h2>1.4&emsp;Meet and greet</h2><div class="u-typography-bold-intro">
<p>At this stage, you should know that we encourage learners to collaborate and communicate online. We usually start each course by engaging you in a discussion.</p>
<p>Now, that you’ve met the Lead Educator and you know each other from other courses, we would like you to tell us what to expect from this particular course. Introduce yourself to the Lead Educator in the comments area below and tell us a bit about yourself.</p>
<p><strong>These questions might guide your responses</strong>:</p>
<ul>
<li>Why are you interested in this course?<br/></li>
<li>What do you already know about Data Analytics, and what do you hope to learn?<br/></li>
<li>What aspect of the course are you most looking forward to?<br/></li>
</ul>
<p><em>Don’t forget to mark this Step as complete before you move on</em>.</p>
</div><p><h2>1.5&emsp;Welcome to Topic 1</h2><div class="u-typography-bold-intro">
<p>Welcome to the first topic of the Introduction to Data Analytics and Data Mining course by Dublin City University.</p>
<p>This topic will introduce you to the basic terminology and concepts related to this course. We will discuss what data mining and data analytics are and we will talk about their relationship to statistics. We will also look at the different events in analytics history, data analytics and the data mining standardisation processes.</p>
<p>After completing Topic 1, you will be able to :</p>
<ul>
<li>Define data analytics.<br/></li>
<li>Explain the relationship between data analytics and statistics.<br/></li>
<li>Discuss the history of data analytics and list the main milestones in its history.<br/></li>
<li>Work comfortably with the chosen software.<br/></li>
</ul>
<p>Throughout this course, we will be jumping between the FutureLearn platform where we cover most of the theory behind data analysis and data mining, and Google Collaboration (Colab) on your Google drive for some hands-on experience and Python coding.</p>
<p>In the next few steps, we will complete some necessary housekeeping. We will explain step by step what to install, where to find it and how to use it to prepare you to start learning.</p>
<p>Remember if you have any questions about any concepts covered within this topic, you can post them in the comments section in the <a href="https://www.futurelearn.com/courses/intro-to-data-analytics/1/steps/656491">review step</a>.</p>
</div><p><h2>1.6&emsp;Prerequisites - are you ready for this module?</h2><div class="u-typography-bold-intro">
<p>Commencing your data analytics journey can be a daunting experience. You probably realise by now that you will need to have some experience using a programming language, and it also helps if you have a basic understanding of some statistical concepts.</p>
<p>We recommend you become familiar with the Python programming language. You may, over time, introduce yourself to the R programming language, but from the perspective of completing the course material, start with Python. As you are DCU students, you will have access to Google Colabs. This is a really useful platform as it gives you free access to Graphical Processing Units (GPUs). GPUs are tremendously useful when implementing machine learning algorithms, as they speed up the time to convergence.</p>
<p>I would also recommend getting a statistical software package such as SPSS (also free to DCU students). There are many packages out there but get one with an easy to use GUI. You may think this is a bit strange, but it really speeds up your data mining when you can manipulate data quickly, and it helps to check if you are implementing your Python code correctly.</p>
<p>I regularly use both SPSS and JMP to sift through my data and examine it for various characteristics. I will implement some of these algorithms packages on small sample sets of data to make sure my overall results make sense. Think about how a child learns. They will feel, touch and smell the object when they come across it. You need to do the same for your data.</p>
<p>We also use some statistical tests and concepts to examine our data. These cover basic regression, <a href="https://en.wikipedia.org/wiki/Student%27s_t-test">T-tests</a> and <a href="https://en.wikipedia.org/wiki/Analysis_of_variance">ANOVA</a>. I have also created some slides that you can use to recap some of your basic statistics that are available in the PDF at the bottom of this step.</p>
</div><p><h2>1.7&emsp;Data analyst toolkit</h2><div class="u-typography-bold-intro">
<p>Let’s think for a moment about what is expected from a data analyst.</p>
<p>Leading data capture and data analysis is an obvious thing. But it is much more complicated - analysts also need to be able to understand the data and speak a common language to interpret the needs of the organisation. Some degree of imagination, persistence, a little bit of innovation are also useful skills. You will find that there is rarely an exact solution for your problems but “nearly correct” solutions. So, you need to think like a detective in a CSI series and open your mind to the most bizarre findings.  These are essential requirements, but to run a successful analytics project, you will need more. As a data analyst, you will need to be able to choose wisely from a wide range of tools that can help to explore, analyse and visualise data.</p>
<p>In this course, you will have an opportunity to work with some of the tools and packages. We will be making recommendations, but we would also like to hear if there are some go-to packages that we do not mention. We encourage you to try, test and decide what tools are most suitable for you.</p>
<h3 id="the-data-analyst-toolkit">The data analyst toolkit</h3>
<p>Throughout the Data Analytics and Data Mining module, you will be required to work with data and complete data analytics projects. To do that, it will really help if you use some tools that allow you to play with the data and have easy access to a <strong>graphical user interface</strong> (GUI). There are many examples of useful statistical and data analysis tools that are available: SPSS, JMP, Minitab to name a few.</p>
<p>In the next steps, you will receive detailed step by step instructions on how to download SPSS. As we said earlier SPSS is free to DCU students. You can, of course, use any other software that you feel comfortable with. If you decide to use different software, please try to choose something that does not rely on programming alone. I recommend taking a sample of data into the packages to understand the scope of the data.</p>
</div><p><h2>1.8&emsp;SPSS statistical software</h2><div class="u-typography-bold-intro">
<p>Statistical Package for Social Sciences (SPSS) is a statistical analysis program most widely used in behavioural sciences. It was created for social sciences and psychology. Today SPSS is used in a wide range of disciplines mainly because of its user-friendly interface (and of course its data handling capacity!)</p>
<p>DCU students have free access to SPSS, and we will be using it in this course. As mentioned in the previous step, you can choose to use different software. Please try to choose one that does not rely on programming alone.</p>
<h3 id="how-to-install-and-use-spss">How to install and use SPSS?</h3>
<ol>
<li>
<p>To download and install SPSS, please follow the <a href="https://www.dcu.ie/iss/software/SPSS.shtml">link</a>, log in with your DCU credentials and follow the instructions.<br/></p>
</li>
<li>
<p>When you have managed to get SPSS installed, please watch the video below. It will help you get started with SPSS. Dr Todd Daniel from Missouri State University introduces the SPSS workspace, how to navigate between Data View and Variable View, how to create variables, and how to modify properties of variables.<br/></p>
</li>
</ol>
<p>Please note that the video explains one of the earlier releases of SPSS, that is not available to download from DCU students apps. But don’t worry - they are very similar!</p>
<p>You can also skip through whatever parts of this video that you do not find relevant to you.</p>
<p></p><div><div class="u-responsive-embed">

</div>
<p class="u-italic">
  This is an additional video, hosted on YouTube.
</p>
</div>
<p>We hope that everything went well. If you had any problems during the download or installation, please note that you can always contact the <a href="https://www.dcu.ie/iss/index.shtml">Information Systems Services</a>.</p>
<p>If you decided to use a different program, let us know what you picked and what were the reasons? Share your comments and links in the comments section.</p>
</div><p><h2>1.9&emsp;Python 3.6+ and Google Colab</h2><div class="u-typography-bold-intro">
<p>In this course, we will use Python as the main programming language and Google Colaboratory will be the platform where we implement all our algorithms.</p>
<h4 id="python">Python</h4>
<p>Python will be used as the main programming language in this course. We assume that you are familiar with it. 
Python 2.7, 3.6 and 3.7 are extensively used in the data analytics community. R is also used in the data analytics community, but for this course, we will apply all our techniques in Python.</p>
<h4 id="google-colaboratory">Google Colaboratory</h4>
<p>Google Colabs is a platform that resembles Jupyter Notebooks. As some of you may know you can write markup in the middle of a Jupyter notebook. This makes the explanation and learning process relatively easy compared to typical programming environments. The advantage with Google Colabs is we get the use of a GPU and this makes the data analytics journey a lot easier and faster.</p>
<p>To access Google Colab make sure you are logged into Gmail with your DCU account. You were given an email address ending with the “@mail.dcu.ie”.  You will need to log into Gmail using this email and then go to Google drive. I am giving step-by-step instructions in the next step.</p>
<p>The Google Colab file allows you to execute Python code without any requirements on your workstation. Just be warned that access to a (GPU),  is limited to 12 hours but this can be incredibly useful before you consume a large amount of cloud resources.  You may also want to use cloud resources that are available to you through work. This is fine as long as you have permission and you do not expect support from DCU.</p>
<p>Have a look at this YouTube video by Jake Vanderplass that introduces Google Colabs.</p>
<p></p><div><div class="u-responsive-embed">

</div>
<p class="u-italic">
  This is an additional video, hosted on YouTube.
</p>
</div>
</div><p><h2>1.10&emsp;Google Colab example</h2><div class="u-typography-bold-intro">
<p>In this step, you are going to get an opportunity to see how Google Colab works. Don’t worry - we will start with a very simple task!</p>
<p>I mentioned previously, that in this course, you will be navigating between the FutureLearn platform and Google Colaboratory. It will be clearly stated every time when you will be required to move between platforms.</p>
<h4 id="google-colab-task">Google Colab task:</h4>
<p>Follow the instructions below to see how Google Colabs work on a selected runtime environment.</p>
<ol>
<li>Make sure you are logged in to your Google account.<br/></li>
<li>Create a folder on your Google Drive and call it ‘CA652 Colab Examples’.<br/></li>
<li>Go to the folder and hit the right-click button. You should see the screen below.<br/>
<img alt="Screenshot of Google Drive interface, middle of page shows a list of options to upload content “new folder”, “Upload files”, “upload folder”" sizes="(min-width: 1695px) 729px, (min-width: 680px) 648px, 320px" src="https://ugc.futurelearn.com/uploads/assets/31/a8/hero_31a8b373-6600-457c-a978-1d701bc5c06e.png" srcset="https://ugc.futurelearn.com/uploads/assets/31/a8/small_hero_31a8b373-6600-457c-a978-1d701bc5c06e.png 320w, https://ugc.futurelearn.com/uploads/assets/31/a8/hero_31a8b373-6600-457c-a978-1d701bc5c06e.png 648w, https://ugc.futurelearn.com/uploads/assets/31/a8/large_hero_31a8b373-6600-457c-a978-1d701bc5c06e.png 729w, https://ugc.futurelearn.com/uploads/assets/31/a8/large_hero_31a8b373-6600-457c-a978-1d701bc5c06e.png 2x"/></li>
<li>Hit the ‘more’ button and then ‘Google Colabs’.  You are now in Google Colabs.<br/></li>
<li>Now try and do the following:<br/></li>
</ol>
<ul>
<li>
<p>Create a Colab notebook and call it Example1. In this notebook, you should be able to insert text and code.</p>
</li>
<li>
<p>Insert the following text: “You are now in Google Colabs</p>
</li>
<li>
<p>Write a piece of code that creates a variable x and give it a value of 10. Then multiply x by 10. Set this to y. Print y. What runtime environment are you using? Change this and see what happens.</p>
</li>
</ul>
<h4 id="reflection">Reflection:</h4>
<p>Did you have any issues with accessing and creating files?</p>
<p>Did you manage to get the answer to be 100?</p>
<p>Did you manage to change the run time environment to Python 2?</p>
<p><strong>Share your feedback in the comments section below and don’t be afraid to help others who might have problems with this task</strong>.</p>
</div><p><h2>1.11&emsp;What is data mining?</h2><div class="u-typography-bold-intro">
<p>I hope you are all set and ready to start your journey through data analytics and mining!</p>
<p><em><strong>Data mining</strong> is the process of discovering patterns in large data sets involving methods at the intersection of machine learning, statistics, and database systems.   Data mining is an interdisciplinary subfield of computer science and statistics with an overall goal to extract information (with intelligent methods) from a data set and transform the information into a comprehensible structure for further use.  Data mining is the analysis step of the knowledge discovery in databases process, or KDD.  Aside from the raw analysis step, it also involves database and data management aspects, data pre-processing, model and inference considerations,  metrics, complexity considerations, post-processing of highlighted structures, visualization, and online updating.  The difference between data analysis and data mining is that data analysis is used to test models and hypotheses on the dataset, e.g., analyzing the effectiveness of a marketing campaign, regardless of the amount of data; in contrast, data mining uses machine- learning and statistical models to uncover clandestine or hidden patterns in a large volume of data</em>.</p>
<p><sub>Source: <a href="https://en.wikipedia.org/wiki/Data_mining">Wikipedia, “Data mining.”</a></sub></p>
<p><em><strong>Watch this video in which <a href="https://www.youtube.com/watch?v=EH3bp5335IU">Stefan Chin</a> discusses how much information we are revealing and proves that it is more than we realise</strong></em>.</p>
<p></p><div><div class="u-responsive-embed">

</div>
<p class="u-italic">
  This is an additional video, hosted on YouTube.
</p>
</div>
</div><p><h2>1.12&emsp;Data analytics</h2><div class="u-typography-bold-intro">
<p>In this step, we will try to define data analytics and give you an idea of what you are getting into.</p>
<p>Wikipedia defines analytics as ‘the discovery, interpretation, and communication of meaningful patterns in data’  andd that  ‘entails applying data patterns towards effective decision making’.</p>
<p>Another, perhaps more detailed definition describes data analytics as;</p>
<p>‘<em>the process of examining data sets in order to draw conclusions about the information they contain, increasingly with the aid of specialized systems and software. Data analytics technologies and techniques are widely used in commercial industries to enable organizations to make more informed business decisions and by scientists and researchers to verify or disprove scientific models, theories and hypotheses</em>’.</p>
<p><sub>Source: <a href="https://searchdatamanagement.techtarget.com/definition/data-analytics">TechTarge</a></sub></p>
<p>Read the full text at the <a href="https://searchdatamanagement.techtarget.com/definition/data-analytics">TechTarget website.</a> It gives an excellent introduction to the topic with an overview of types of analytics and some insights into the analytics process.</p>
<p>When you read the article, I would like you to answer a simple question: What is the difference between data analytics and data science? Illustrate your answer with some examples of when each of them could be applied.</p>
<p>Post your reply in the comments section below.</p>
</div><p><h2>1.13&emsp;Statistics and data science </h2><div class="u-typography-bold-intro">
<p>Data science is a vast multidisciplinary area, and data scientists are among the top high-demand jobs. At the same time, there seem to be many (quite often) ambiguous definitions of what data science is and how it is different from statistics.</p>
<p>As it can be difficult to determine how analytics differ from statistics, we will try to explore how these disciplines intersect. You can find many interesting resources in this step, we would like you to read two:</p>
<ol>
<li>A blog post by J. Megahan ‘<a href="https://mixpanel.com/blog/2016/03/30/this-is-the-difference-between-statistics-and-data-science/">This is the difference between statistics and data science</a>’<br/></li>
<li>A journal paper by I. Carmichael and J. S. Marron entitled <a href="https://link.springer.com/article/10.1007/s42081-018-0009-3">Data science vs. statistics: two cultures</a><br/></li>
</ol>
<p>In his article, Megahan reports on his attempt to find answers to some questions related to data science and data analytics. The journal paper examines how data analytics evolved, what its relationship with statistics is and what are the emerging future trends.</p>
<p>Read the blog post and the journal paper now and share your thoughts on this topic in the comments section.</p>
<p>Posting your answers you might want to be guided by the following prompts:</p>
<ul>
<li>What is data science?</li>
<li>Is it different than statistics, and if so, what is the difference?</li>
<li>And why are they in such demand?</li>
</ul>
<p><strong>Be sure to like or respond to any comments you particularly like</strong>.</p>
</div><p><h2>1.14&emsp;A brief history of analytics</h2><div class="u-typography-bold-intro">
<p><img alt="Evolution of Man into a modern (digital) world. Space in the background" sizes="(min-width: 1695px) 729px, (min-width: 680px) 648px, 320px" src="https://ugc.futurelearn.com/uploads/assets/fe/72/hero_fe7239e0-a277-409c-8043-a00240246f00.jpg" srcset="https://ugc.futurelearn.com/uploads/assets/fe/72/small_hero_fe7239e0-a277-409c-8043-a00240246f00.jpg 320w, https://ugc.futurelearn.com/uploads/assets/fe/72/hero_fe7239e0-a277-409c-8043-a00240246f00.jpg 648w, https://ugc.futurelearn.com/uploads/assets/fe/72/large_hero_fe7239e0-a277-409c-8043-a00240246f00.jpg 729w, https://ugc.futurelearn.com/uploads/assets/fe/72/large_hero_fe7239e0-a277-409c-8043-a00240246f00.jpg 2x"/></p>
<p>The advent of data analytics can be found as far back as 3000 BC when the Egyptians were building the pyramids.</p>
<p>The Egyptians would tract daily workload by workers in order to track the volume of beer that was required.  During the second world war, both the Allied and German armies used the disciplines of maths and statistics to create a new subject known as operations research. This research allowed armies to optimize the supply and movement of supplies, men and munitions. That was essentially the roots of modern-day data analytics.</p>
<p>The use of Analytics by business can be found as far back as the 19<sup>th</sup> century when Frederick Winslow Taylor initiated time management exercises. Another example is when Henry Ford measured the speed of assembly lines. In the late 1990s, Analytics began receiving more attention as computers became faster and with the advent of cloud technologies. <br/>
<sub>Source: <a href="https://www.dataversity.net/brief-history-analytics/">Dataversity.net</a></sub></p>
<p>Click on the link below to access the timeline with a brief history or analysis and some important milestones. At the end of the timeline we have a task for you - make sure to come back to the step, follow the guidelines and participate in the discussion.</p>
<p>Let us know if there are any events that you think should be added to that timeline! We will update the timeline for the next course run.</p>
<h3 id="please-note-that-javascript-is-required-to-run-this-exercise-and-it-may-not-work-on-older-versions-of-ios"><em>Please note that javascript is required to run this exercise and it may not work on older versions of iOS.</em></h3>
</div><p><h2>1.15&emsp;Data Science Process </h2><div class="u-typography-bold-intro">
<p>In today’s world, most companies and organisations try to follow a process or methodology when completing a body of work. It helps the individuals working on the project comfortably follow the thought process and management can visually see the amount of progress being made. This is no different in the data analytics world.</p>
<h4 id="data-science-process">Data Science Process</h4>
<p><a href="https://www.tomdavenport.com/about/">Tom Davenport</a> (the father of business analytics) proposes a six step data analytics  method that is divided into three stages.</p>
<ol>
<li>
<p><strong>Stage 1 involves</strong>: (1) Problem recognition and (2) Review of previous findings.</p>
</li>
<li>
<p><strong>Stage 2 involves</strong>: (3) Modelling, (4) Data collection and (5) Data analysis</p>
</li>
<li>
<p><strong>Stage 3 involves</strong>: (6) Results Presentation</p>
</li>
</ol>
<p><img alt="An infographic  illustrating the first stage of the Data Science process that involves problem recognition and review of previous findings. 
" sizes="(min-width: 1695px) 729px, (min-width: 680px) 648px, 320px" src="https://ugc.futurelearn.com/uploads/assets/51/b7/hero_51b7fa1e-c59e-465c-9fba-28621330bb9b.png" srcset="https://ugc.futurelearn.com/uploads/assets/51/b7/small_hero_51b7fa1e-c59e-465c-9fba-28621330bb9b.png 320w, https://ugc.futurelearn.com/uploads/assets/51/b7/hero_51b7fa1e-c59e-465c-9fba-28621330bb9b.png 648w, https://ugc.futurelearn.com/uploads/assets/51/b7/large_hero_51b7fa1e-c59e-465c-9fba-28621330bb9b.png 729w, https://ugc.futurelearn.com/uploads/assets/51/b7/large_hero_51b7fa1e-c59e-465c-9fba-28621330bb9b.png 2x"/>
<sub>Source: The Ideas Lab</sub></p>
<h4 id="stage-1-problem-recognition-and-review-of-previous-findings">Stage 1: Problem recognition and review of previous findings</h4>
<p>This stage allows the analyst to become familiar with the subject area and read up on what others have done. It is similar to good research.  It is important to stress the point that we should always build some content knowledge around the problem  you are trying to solve. So, for example, if you are attempting to determine the features and variables that help predict lung cancer, then study the development of the disease. You don’t need to be an expert but try to understand what experts mean and what their conclusions have been in the past.</p>
<p><img alt="an infographic illustrating the second stage of the Data Science process that involves Modelling, data collection and data analysis." sizes="(min-width: 1695px) 729px, (min-width: 680px) 648px, 320px" src="https://ugc.futurelearn.com/uploads/assets/ac/a2/hero_aca2c9e9-13f4-4652-bb71-e5824edbee54.png" srcset="https://ugc.futurelearn.com/uploads/assets/ac/a2/small_hero_aca2c9e9-13f4-4652-bb71-e5824edbee54.png 320w, https://ugc.futurelearn.com/uploads/assets/ac/a2/hero_aca2c9e9-13f4-4652-bb71-e5824edbee54.png 648w, https://ugc.futurelearn.com/uploads/assets/ac/a2/large_hero_aca2c9e9-13f4-4652-bb71-e5824edbee54.png 729w, https://ugc.futurelearn.com/uploads/assets/ac/a2/large_hero_aca2c9e9-13f4-4652-bb71-e5824edbee54.png 2x"/>
<sub>Source: The Ideas Lab</sub></p>
<h4 id="stage-2-modelling-data-collection-and-data-analysis">Stage 2: Modelling, data collection and data analysis</h4>
<p>This is where most of you will want to play when you get into your first project. Be really careful though not to rush in. Do a thorough review in stage one  and do not underestimate the impact that stage one will have. Many students think that  they can go straight into running algorithms and models without any content knowledge. I regularly annoy various experts I work with with “dumb” questions, so don’t be afraid to try to understand the area and ask a few of your own dumb questions.</p>
<p>So what do I mean by <strong>modelling</strong>?</p>
<p>What we are really asking is: do I understand the variables (features) that influence my outcomes? Typically, I think of every analytics project as Y=a+bX problem. X are the input features/variables and Y are the outcomes. This seems trivial but can get really complicated in practice. In addition to the features/variables, we should also have some understanding of the functional form or if one exists. We will get to this later, so for now, don’t worry, and just try and understand what the input features are.</p>
<p><strong>Data collection</strong> is relatively obvious but again can take a considerable amount of time. It is vital that this is done in a way that is not going to induce bias. In pharmaceutical studies I have worked on in the past, the data collection and cleaning process was by far the most significant part of the work.</p>
<p><strong>Data analysis</strong> is the bit you all want to do. If the previous stages are done properly and plenty of pre-planning is done, then this task usually doesn’t take so long. A good rule of thumb is that steps 1 - 4 will take between 60-70% of time spent on any project.</p>
<p><img alt="An infographic illustrating the third stage of the data science process and also of the sixth stage of the data science process that involves “presentations of results”. To the right side of the stages there is a cartoon man thinking about different data findings." sizes="(min-width: 1695px) 729px, (min-width: 680px) 648px, 320px" src="https://ugc.futurelearn.com/uploads/assets/27/36/hero_2736d450-9483-42ad-98bd-0e12530b4754.png" srcset="https://ugc.futurelearn.com/uploads/assets/27/36/small_hero_2736d450-9483-42ad-98bd-0e12530b4754.png 320w, https://ugc.futurelearn.com/uploads/assets/27/36/hero_2736d450-9483-42ad-98bd-0e12530b4754.png 648w, https://ugc.futurelearn.com/uploads/assets/27/36/large_hero_2736d450-9483-42ad-98bd-0e12530b4754.png 729w, https://ugc.futurelearn.com/uploads/assets/27/36/large_hero_2736d450-9483-42ad-98bd-0e12530b4754.png 2x"/>
<sub>Source: The Ideas Lab</sub></p>
<h4 id="stage-3-results-presentation">Stage 3: Results presentation</h4>
<p>The results presentation is really important. I often call this the cream on top of the cake and is the stage that brings everything together. Remember -  a picture tells a 1000 words but you lose the impact if you have too many pictures. Always try and get your point across quickly and try not to have your presentation lasting too long.</p>
<p>So this is a simple data science life cycle process. You can access the full infographic at the end of this step or at the following <a href="https://my.visme.co/projects/9070n3j6-ca683-m1-1-15-da-process">link</a>.</p>
<p>There are many other data processing models used in DA projects. We will quickly go through the 3 most popular methods.</p>
<p><strong>CRISP-DM</strong></p>
<p>The process underlying data analytics was established and formalized under the name of CRISP-DM in 1999. CRISP stands for <a href="https://en.wikipedia.org/wiki/Data_mining#Process">Cross Industry Standard Process for Data Mining</a>.</p>
<p>Over the years, the name of the discipline has changed from data analytics to data science, and several refinements and extensions were proposed. Despite this, CRISP-DM continues to be the most widely used methodology for analytics, data mining, and data science projects.</p>
<p>There are many variations, but the  underlying six phases remain the same:</p>
<ol>
<li>Business Understanding<br/></li>
<li>Data Understanding<br/></li>
<li>Data Preparation<br/></li>
<li>Modelling<br/></li>
<li>Evaluation<br/></li>
<li>Deployment<br/></li>
</ol>
<p><img alt="An info-graphic showing the six phases of CRISP-DM ,”Business Understanding”,”Data Understanding”,”Data Preparation”,”Modelling”,”Evaluation”,”Deployment“." sizes="(min-width: 1695px) 729px, (min-width: 680px) 648px, 320px" src="https://ugc.futurelearn.com/uploads/assets/ca/c0/hero_cac0b887-20db-482e-aba4-eff7ba184d80.png" srcset="https://ugc.futurelearn.com/uploads/assets/ca/c0/small_hero_cac0b887-20db-482e-aba4-eff7ba184d80.png 320w, https://ugc.futurelearn.com/uploads/assets/ca/c0/hero_cac0b887-20db-482e-aba4-eff7ba184d80.png 648w, https://ugc.futurelearn.com/uploads/assets/ca/c0/large_hero_cac0b887-20db-482e-aba4-eff7ba184d80.png 729w, https://ugc.futurelearn.com/uploads/assets/ca/c0/large_hero_cac0b887-20db-482e-aba4-eff7ba184d80.png 2x"/></p>
<p><sub>Source: <a href="https://upload.wikimedia.org/wikipedia/commons/b/b9/CRISP-DM_Process_Diagram.png">Wikimedia Commons, Kenneth Jensen, CC BY-SA 3.0</a></sub></p>
<p><strong>KDD</strong></p>
<p>KDD stands for knowledge discovery in databases and means ‘nontrivial extraction of implicit, previously unknown and potentially useful information from data stored in databases’. <br/>
<sub>Source: <a href="https://www.geeksforgeeks.org/kdd-process-in-data-mining/">www.geeksforgeeks.org</a></sub></p>
<p>This process  is commonly defined with the following stages:</p>
<ol>
<li>Selection<br/></li>
<li>Pre-processing<br/></li>
<li>Transformation<br/></li>
<li>Data mining<br/></li>
<li>Interpretation/evaluation<br/></li>
</ol>
<p><img alt="a graphic mapping out the KDD, “knowledge discovery in databases” A diagonal process is shown with arrows to show the transitions. On the bottom of the graph it reads Data integration, which then moves from “Databases” , “Data cleaning”, “Data selection and transformation”, “data mining”, “pattern evaluation” to a final sketch of a cloud saying “knowledge”" sizes="(min-width: 1695px) 729px, (min-width: 680px) 648px, 320px" src="https://ugc.futurelearn.com/uploads/assets/21/ba/hero_21baf510-bea6-45dd-ba9d-381d628daa8a.png" srcset="https://ugc.futurelearn.com/uploads/assets/21/ba/small_hero_21baf510-bea6-45dd-ba9d-381d628daa8a.png 320w, https://ugc.futurelearn.com/uploads/assets/21/ba/hero_21baf510-bea6-45dd-ba9d-381d628daa8a.png 648w, https://ugc.futurelearn.com/uploads/assets/21/ba/large_hero_21baf510-bea6-45dd-ba9d-381d628daa8a.png 729w, https://ugc.futurelearn.com/uploads/assets/21/ba/large_hero_21baf510-bea6-45dd-ba9d-381d628daa8a.png 2x"/></p>
<p><sub>Source: <a href="https://www.geeksforgeeks.org/kdd-process-in-data-mining/">www.geeksforgeeks.org</a></sub></p>
<p><strong>TDSP</strong></p>
<p>Team Data Science Process (TDSP)  is a new data science methodology developed by Microsoft in 2016. It is based on well-known frameworks and leverages innovative tools such as git version control.</p>
<p><em>The Team Data Science Process (TDSP) is an agile, iterative data science methodology to deliver predictive analytics solutions and intelligent applications efficiently. TDSP helps improve team collaboration and learning. It contains a distillation of the best practices and structures from Microsoft and others in the industry that facilitate the successful implementation of data science initiatives. The goal is to help companies fully realize the benefits of their analytics program</em>.<br/>
<sub>Source: <a href="https://docs.microsoft.com/en-us/azure/machine-learning/team-data-science-process/overview">Microsoft.com</a></sub></p>
<p><img alt="Graphic represents data science lifecycle. At the top, there are two circles. The left hand smaller green circle is labelled start. An arrow stems from this circle going to the right and leads to a larger circle labelled business understanding. Two arrows stem from beneath this circle at 45-degree angles leading to two circles below. The circle on the left is labelled Modelling. The circle on the right is labelled Data acquisition and understanding. There is a bi-directional arrow between these two arrows with an oil barrel symbol and pipet above the arrow and a person holding a lightbulb below. Arrows lead from these two circles leading to one circle labelled deployment. To the left of the circle labelled modelling, there are three boxes, labelled: Feature engineering, model training and model evaluation respectively. To The left of the box labelled feature engineering text reads: Transform, binning, temporal, text, image feature selection. To the left of the box labelled model training text reads: Algorithms, ensembles, parameter tuning, retraining and model management. To the left of the box labelled model evaluation text reads cross-validation, model reporting and a/b testing. To the right of the circle labelled Data acquisition &amp; understanding, there are four boxes labelled Data source, pipeline, environment and wrangling, exploration and cleaning respectively. To the right of the box labelled data source, text reads on-premises vs CLoud database vs Files. To the right of the box labelled pipeline text reads streaming vs batch low vs high frequency. To the right of the box labelled environment text reads on-premises vs cloud, database vs data lake vs..., small vs medium vs big data. To the right of the box labelled wrangling, exploration &amp; cleaning text reads, structured vs unstructured data validation and cleanup visualization. To the right of the circle labelled deployment, there is an arrow labelled customer acceptance leading to a circle labelled end. Below the circle labelled deployment there is a box labelled scoring, performance monitoring, etc.. To the right of this box there are three gears each labelled model store, web services and intelligent applications respectively." src="https://docs.microsoft.com/en-us/azure/machine-learning/team-data-science-process/media/overview/tdsp-lifecycle2.png"/>
<sub>Source: <a href="https://docs.microsoft.com/en-us/azure/machine-learning/team-data-science-process/media/overview/tdsp-lifecycle2.png">Microsoft.com</a></sub></p>
<p>Are you familiar with any other models? Let us know in the comments section below.</p>
</div><p><h2>1.17&emsp;Review of Topic 1</h2><div class="u-typography-bold-intro">
<p>Listen to the audio clip above, where Andrew discusses and highlights some issues covered under this topic. He also addresses the most interesting questions raised by the learners.</p>
<p>This part of the course was about general housekeeping and introducing learners to the most basic terminology and concepts that each data analyst will need.</p>
<p>Having completed all the steps in this topic, you should be able to:</p>
<ul>
<li>Define data analytics and data mining.</li>
<li>Explain the relationship between data analytics and statistics.</li>
<li>Discuss the history of data analytics and list the main milestones in its history.</li>
<li>Demonstrate the understanding of the data mining process.</li>
<li>Work comfortably with the chosen software.</li>
</ul>
<p>Next, we will move to review some basic concepts around statistics and probability.</p>
<p>Remember that you can post any questions or comments that you may have regarding anything we have covered in the course so far in the comments section below.</p>
</div><p><h2>1.18&emsp;Next topic</h2><div class="u-typography-bold-intro">
<p><strong>Well done on completing Topic 1</strong>!</p>
<p>Let’s move forward to <a href="https://www.futurelearn.com/courses/intro-to-data-analytics/1/steps/661078">Topic 2: Probability,  regression and analysis of variance</a>.</p>
</div><p style="page-break-before: always"><h2>2.1&emsp;Welcome to Topic 2</h2><div class="u-typography-bold-intro">
<p>Numbers are everywhere. These days it is almost impossible to read a blog post or an article in a newspaper that doesn’t refer to numbers in some way.</p>
<p>In the next two topics, we will review and master the techniques that will help you to make sense of numbers and interpreting data.</p>
<p>We will start with an introduction to statistics and probability laws.</p>
<p>After completing this section, you will be able to:</p>
<ul>
<li>Explain what descriptive and inferential statistics are<br/></li>
<li>Implement some simple descriptive statistics<br/></li>
<li>Define measures of spread<br/></li>
<li>Discuss dispersion and types of data<br/></li>
<li>Define and give examples of different distribution types<br/></li>
<li>Run simple graphical presentations<br/></li>
</ul>
<p>You will also get an opportunity to do some hands-on work.</p>
<p>We have included several discussion prompts - make sure to address the issues and interact with other learners!</p>
<p>Remember if you have any questions about any of the subjects or concepts covered within this topic, you can post them in the comments section in the review step.</p>
</div><p><h2>2.2&emsp;Descriptive and Inferential Statistics</h2><div class="u-typography-bold-intro">
<p>In the previous topic, we touched on the relationship that statistics has with data analytics.</p>
<p>The video above, cover some very basic statistical concepts and will introduce concepts of <strong>Descriptive Statistics</strong> and <strong>Inferential Statistics</strong>.  But let’s start with defining the term Statistics.</p>
<h4 id="definition-of-statistics">Definition of Statistics</h4>
<p><em>The practice or science of collecting and analysing numerical data in large quantities, especially for the purpose of inferring proportions in a whole from those in a representative sample.</em><br/>
<sub><a href="https://www.lexico.com/en">Lexico</a></sub></p>
<h4 id="descriptive-and-inferential-statistics">Descriptive and Inferential Statistics</h4>
<p><strong>Descriptive Statistics</strong> are used to describe the numbers you have right in front of you and summarise these numbers as metrics or graphical displays. The average height of students in a class could be used as a means of describing student heights. Alternatively, one could summarise the range between maximum and minimum heights to describe the spread of the data.</p>
<p><strong>Inferential statistics</strong> takes part in a population and attempts to infer something about the entire population. If I select a sample of 100 students from a university can I come up with an estimation for the overall average height within the university?</p>
<p><strong>Some differences</strong>:</p>
<table>
<tbody>
<tr>
<td><strong>Descriptive statistics</strong></td>
<td><strong>Inferential Statistics</strong></td>
</tr>
</tbody>
</table>
<table>
<tbody>
<tr>
<td>Describes the dataset that you are working with.</td>
<td>Makes inferences from the sample of the target population and generalises them.</td>
</tr>
</tbody>
</table>
<table>
<tbody>
<tr>
<td>Organises analyses and presents data in a meaningful way.</td>
<td>Compares, tests and predicts future outcomes.</td>
</tr>
</tbody>
</table>
<table>
<tbody>
<tr>
<td>Uses charts and tables to represent results.</td>
<td>Uses probability scores.</td>
</tr>
</tbody>
</table>
<table>
<tbody>
<tr>
<td>Describes data that is known. Can help us summarise data.</td>
<td>Leads to make conclusions about the population that is beyond data that is known.</td>
</tr>
</tbody>
</table>
<table>
<tbody>
<tr>
<td>Tools:<br/> Measures of central tendency (mean / median / mode), Data Spread (range, standard, deviation etc.)</td>
<td>Tools:<br/> Hypothesis test and analysis of variance</td>
</tr>
</tbody>
</table>
<h4 id="the-discussion">The discussion</h4>
<p>An excellent overview of descriptive statistics is provided in this <a href="https://insights.ovid.com/crossref?an=00000539-201711000-00048">article</a> by Tomas Vetter. Read it now, and in the comments section below post your thoughts about differences between differential and inferential statistics. I have summarised them in the table above. When posting your answer you may consider the following prompts:</p>
<ul>
<li>
<p>Were you in the situation that you had to choose between the statistical method when conducting research or analysis?<br/></p>
</li>
<li>
<p>What did you consider to be a primary factor when deciding method?<br/></p>
</li>
</ul>
<h4 id="google-colab-task">Google Colab task</h4>
<p>Ok. It is time for our first real exercise! Go to the following <a href="https://drive.google.com/open?id=1ufWcQt92ToNBETpJUb5EQY945zlOR0a8">link</a> to the Google Colab for this step. (I suggest you open the link in a new tab for easier navigation between the platforms).</p>
<p>I have an example there where you are going to read in a small dataset and import pandas and numpy. These libraries will be used throughout the course.  We are then going to get some measures of central tendency.</p>
</div><p><h2>2.3&emsp;The central tendency</h2><div class="u-typography-bold-intro">
<p>In <a href="https://en.wikipedia.org/wiki/Statistics">statistics</a>, a <strong>central tendency</strong> (or <strong>measure of central tendency</strong>) is a central or typical value for a <a href="https://en.wikipedia.org/wiki/Probability_distribution">probability distribution</a>. It may also be called a <strong>centre</strong> or <strong>location</strong> of the distribution. The term <em>central tendency</em> dates from the late 1920s.<br/>
<sub>(Source: <a href="https://en.wikipedia.org/wiki/Central_tendency">Wikipedia</a>)</sub></p>
<p>In the previous step, I asked you to read an article by Tomas Vetter <a href="https://insights.ovid.com/crossref?an=00000539-201711000-00048">Descriptive Statistics: Reporting the Answers to the 5 Basic Questions of Who, What, Why, When, Where, and a Sixth, So What?</a> From that article, you should know that the mean, median, and mode are three measures of the central tendency of a set of data.</p>
<p>Let’s see how the author explained them and look at some examples.</p>
<h4 id="mean">Mean</h4>
<p>The mean can be described as the total sum of the values divided by the number of observations (sample size). Typically when someone asks us to describe a column of data, we would pick the mean because the mean gives an idea of the location of central point in the dataset. However, it is not the only measure of central tendency.</p>
<h4 id="mode">Mode</h4>
<p><em>The mode is the discrete number or value that occurs most commonly or frequently in the dataset</em>. It tells us the most fashionable value.</p>
<h4 id="median">Median</h4>
<p>If you sort all the numbers highest to lowest, then the median is the value in the centre of these numbers. The 50th percentile and the median are effectively the same thing. One of the benefits of this measure is that it is not affected by outliers.</p>
<p>You can watch the video below if you would like a bit more detailed recap on central tendency measures. 
</p><div><div class="u-responsive-embed">

</div>
<p class="u-italic">
  This is an additional video, hosted on YouTube.
</p>
</div>
<p><sub>Source: <a href="https://www.youtube.com/watch?v=kn83BA7cRNM">CrashCourse</a></sub></p>
<h4 id="google-colab-task">Google Colab task</h4>
<p>Go to the following <a href="https://drive.google.com/open?id=1Um0HlegnHXVUHctZYJfcH3ctd_9CTfdU">Google Colab</a> where I have prepared a simple exercise for you (I suggest you open the link in the new tab for easier navigation between the platforms). You will need to run the code and then calculate the three measures of central tendency.</p>
<h4 id="references">References:</h4>
<p>Vetter, T.R., 2017. Descriptive Statistics: Reporting the Answers to the 5 Basic Questions of Who, What, Why, When, Where, and a Sixth, So What?. <em>Anesthesia &amp; Analgesia</em>, 125(5), pp.1797-1802. Available <a href="https://insights.ovid.com/crossref?an=00000539-201711000-00048">here</a>.</p>
</div><p><h2>2.4&emsp;Dispersion and types of data  </h2><div class="u-typography-bold-intro">
<p>We are again going to refer to Vetter’s (2017) article mentioned in the previous steps and discuss dispersion and types of data.</p>
<h4 id="dispersion">Dispersion</h4>
<p><em>In statistics, <strong>dispersion</strong> (also called <strong>variability</strong>, <strong>scatter</strong>, or <strong>spread</strong>) is the extent to which a distribution is stretched or squeezed. Common examples of measures of statistical dispersion are the variance, standard deviation, and interquartile range</em>. <br/><sub>(Source: <a href="https://en.wikipedia.org/wiki/Statistical_dispersion">Wikipedia</a>)</sub></p>
<p>Measuring dispersion is really important when describing data. You will generally find that a measure of central tendency is not enough and understanding the spread of your data is really important also. This can quickly point out potential outliers in the data, or one can use dispersion range to calculate the likely future values.</p>
<p><strong>Don’t be fooled into thinking that outliers should be excluded.</strong> Outliers can often be described as the data miners gold.</p>
<p>There are three measures of dispersion (more often called measures of spread):</p>
<ul>
<li><strong>Range</strong> is the difference between the largest and smallest number.</li>
<li><strong>Variance</strong> is the difference between each data point and the mean, and this is then squared.</li>
<li><strong>Standard deviation</strong> is the square root of the variance.</li>
</ul>
<p>You may ask yourself why do we square the difference between the mean and the data point outlined in the variance. You may also ask yourself why the mean is used. Well, the reason is that by removing the mean we are effectively centralising the column around 0. These differences would have plus and minus signs. By squaring the differences we are now getting rid of the impact of the sign as a large plus is very similar to a large minus when trying to measure the dispersion.</p>
<p><img alt="data flow through filter with gear to separate, analyze and classify data information into specific database type." sizes="(min-width: 1695px) 729px, (min-width: 680px) 648px, 320px" src="https://ugc.futurelearn.com/uploads/assets/45/39/hero_4539729c-f9a5-4551-b7be-4266c8409409.jpg" srcset="https://ugc.futurelearn.com/uploads/assets/45/39/small_hero_4539729c-f9a5-4551-b7be-4266c8409409.jpg 320w, https://ugc.futurelearn.com/uploads/assets/45/39/hero_4539729c-f9a5-4551-b7be-4266c8409409.jpg 648w, https://ugc.futurelearn.com/uploads/assets/45/39/large_hero_4539729c-f9a5-4551-b7be-4266c8409409.jpg 729w, https://ugc.futurelearn.com/uploads/assets/45/39/large_hero_4539729c-f9a5-4551-b7be-4266c8409409.jpg 2x"/></p>
<h3 id="types-of-data">Types of Data</h3>
<p>There are two major categories of data:  <strong>qualitative/categorical</strong> and <strong>quantitative/numerical</strong>.</p>
<p>These terms are used interchangeably, and this is how I will be using them throughout the course.</p>
<h4 id="qualitative-or-categorical-data">Qualitative or categorical data</h4>
<p><strong>Qualitative</strong> or <strong>categorical</strong> data is concerned with categories rather than numbers. It measures ‘types’ or categories and may be represented by a name, symbol, or code, for example, whether you’re married or single, live in Arkansas or Alabama or have blue eyes or brown eyes.</p>
<h4 id="quantitative-data">Quantitative data</h4>
<p><strong>Quantitative</strong> or <strong>numerical data</strong> is, obviously, based on numbers. It is concerned with values or counts expressed as numbers. Numerical data can be further divided into two types:</p>
<ul>
<li><strong>Continuous data</strong>,  that can be broken down into smaller and smaller numerical pieces. For example, is the temperature in your apartment at 69°F, or 69.4°F, or 69.123°F?</li>
<li><strong>Ordinal data</strong>, that has natural ordered categories. For example, the ratings given in Yelp reviews.</li>
</ul>
<h4 id="discussion">Discussion</h4>
<p>Often practitioners use metrics such as the mean to describe a data set. Does it make sense to do this with a categorical variable such as Gender? Explain your answer.</p>
<p><strong>References</strong>:</p>
<p>Vetter, T.R., 2017. Descriptive Statistics: Reporting the Answers to the 5 Basic Questions of Who, What, Why, When, Where, and a Sixth, So What?. <em>Anesthesia &amp; Analgesia</em>, 125(5), pp.1797-1802. Available <a href="https://insights.ovid.com/crossref?an=00000539-201711000-00048">here</a>.</p>
</div><p><h2>2.6&emsp;Your good friend descriptive statistics </h2><div class="u-typography-bold-intro">
<p>You will use descriptive statistics all the time! Averages! Maximums! Minimums!</p>
<p>You will use these metrics to describe nearly every dataset you come across in your career. These are not the only metrics, but nearly every human when looking at a group of objects does this intuitively. We always want to reduce a vast amount of data down to some values we can handle. Good descriptive statistics help us describe a set of data.</p>
<p>We have already looked at a few major concepts in descriptive statistics and talked about <strong>central tendency</strong> and <strong>variability</strong>.</p>
<p>Let’s take a look at those with a simple dataset.</p>
<h4 id="google-colab-task">Google Colab task</h4>
<p>Go to the following <a href="https://drive.google.com/open?id=1IRLn66ojPDwrldq-jldYJYOO-Ga_SJhU">Google Colab</a> where I have a simple exercise for you.</p>
<p>In it, we enter some data and calculate the mean, median and mode. Play with the data and change some of the values. Observe what happens between the mean and the mode when you insert a large value into the dataset. Also look up the standard deviation (.std,.var), variance and range for the salary variable. Note what happens when you put a large value in it.  (I suggest you open the link in a new tab for easier navigation between the platforms).</p>
<p><strong>When you finish playing with the data, share your observations in the comments section</strong>.</p>
</div><p><h2>2.7&emsp;Slight detour: Making sense of .describe() </h2><div class="u-typography-bold-intro">
<p>.describe(.) is one of the commands/functions in python that will be very helpful.</p>
<p>It helps to generate descriptive statistics that summarise the central tendency, dispersion and shape of a dataset’s distribution, excluding <sub>NaN</sub> values.</p>
<p>It also analyses both numeric and object series, as well as DataFrame column sets of mixed data types. The output will vary depending on what is provided. Go to <a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.describe.html#pandas-dataframe-describe">pandas.pydata.org</a> for more detail.</p>
<h4 id="google-colab-task">Google Colab task</h4>
<p>Go to the following <a href="https://drive.google.com/open?id=1y7ZAttq95TiPPJ10WA9i5gKjg5Q5z-sy">Google Colab</a> and work with the example I prepared for you (I suggest you open the link in the new tab for easier navigation between the platforms).</p>
</div><p><h2>2.8&emsp; Box and whisker plot </h2><div class="u-typography-bold-intro">
<p>Most of us are visual creatures who like images,  infographics, colours, or maps to organize information and communicate with others.</p>
<p>It is both helpful and informative if the information is visually represented. If you get tired of looking at lists of numbers, or if you prefer to analyse data visually, there are instruments that you can use.</p>
<p>As mentioned in <a href="https://drive.google.com/open?id=1CK0eToaqoJSXBGYSFRepzTZIH7n_tpZR">Step 2.2</a>, descriptive statistics uses a visual representation of data utilising tools such as; histograms, bar charts, line charts, pie graphs, scatterplots and box and whisker plots.</p>
<p><strong>Box and whisker</strong> plots are one of the most convenient ways to visually display data distribution. You can think about Box and whisker as the visual version of .describe() as they describe the minimum, Q1, median, Q3, and maximum.</p>
<p>Watch the video below about constructing and analysing a box and whisker plot.</p>
<p></p><div><div class="u-responsive-embed">

</div>
<p class="u-italic">
  This is an additional video, hosted on YouTube.
</p>
</div>
<p>Now, let’s have a look at the example of how to use the box and whisker plot to show data visually</p>
<h4 id="google-colab-task">Google Colab task:</h4>
<p>Go to the following <a href="https://drive.google.com/open?id=1vutUdHyU60r7ODyjUc2iYuWYdkmT6tXQ">Google Colab</a> and work with the example I prepared for you (I suggest you open the link in the new tab for easier navigation between the platforms).<br/>
<br/>
<em>Tell us about other methods you have discovered to describe your data sources.</em></p>
</div><p><h2>2.9&emsp;Measures of spread</h2><div class="u-typography-bold-intro">
<p>I briefly mentioned the measures of spread in <a href="https://www.futurelearn.com/courses/intro-to-data-analytics/1/steps/661233">Step 2.4</a>, when talking about dispersion (also called <strong>variability</strong>, <strong>scatter</strong>, or <strong>spread</strong>). The measures of spread can help you to understand how well medians and means describe the data, and how reliable your conclusions are.</p>
<p>There are three measures of spread:</p>
<ul>
<li><strong>Range</strong> is the difference between the largest and smallest number.</li>
<li><strong>Variance</strong> is the difference between each data point and the mean, squared.</li>
<li><strong>Standard deviation</strong> is the square root of the variance.</li>
</ul>
<p>Watch the video below - it will give you all the basics necessary to understand measures of spread.</p>
<p></p><div><div class="u-responsive-embed">

</div>
<p class="u-italic">
  This is an additional video, hosted on YouTube.
</p>
</div>
<h4 id="google-colab-task">Google Colab task:</h4>
<p>So let’s see measures of spread in action! Go to the following <a href="https://drive.google.com/open?id=1CK0eToaqoJSXBGYSFRepzTZIH7n_tpZR">Google Colab</a> and work with the example I prepared for you (as usual, I suggest you open the link in a new tab for easier navigation between the platforms).</p>
</div><p><h2>2.10&emsp;What is probability? </h2><div class="u-typography-bold-intro">
<p>At the most basic level, probability seeks to answer the question, “What is the chance of an event happening?”</p>
<p>To calculate the chance of an event happening, we also need to consider all the other events that can occur. The quintessential representation of probability is a simple coin toss. In a coin toss, the only events that can happen are flipping heads or flipping tails.   These two events form the sample space, the set of all possible events that can occur.</p>
<p>To calculate the probability of an event occurring, we count how many times an event of interest can occur (say flipping heads), and dividing it by the sample space. Thus, probability will tell us that an ideal coin will have a 1-in-2 chance of being heads or tails.</p>
<p>By looking at the events that can occur, probability gives us a framework for making predictions about how often events will happen. However, if we try to toss a coin, we may be likely to get an abnormally high or low count of heads every once in a while. If we don’t want to make the assumption that the coin is fair, what can we do? We can gather data! We can use statistics to calculate probabilities based on observations from the real world and check how it compares to the ideal.</p>
<h3 id="from-statistics-to-probability">From statistics to probability</h3>
<p>Our data will be generated by flipping a coin ten times and counting how many times we get heads. We will call a set of ten coin tosses a trial. Our data point will be the number of heads we observe. We may not get the “ideal” 5 heads, but we won’t worry too much since one trial is only one data point. If we perform many, many trials, we expect the average number of heads over all of our trials to approach the 50%.<br/></p>
<p><a href="https://drive.google.com/open?id=1tLTs5udO1B0zRp6ZvE-bzFCmxySUB3Zu">The code in this Colab</a> simulates 10, 100, 1000, and 1,000,000 trials, and then calculate the average proportion of heads observed. Our process is summarised in the image below as well.</p>
<p><a href="https://ugc.futurelearn.com/uploads/assets/7c/af/7caff046-1034-423f-ae91-9804584f0a4c.jpg"><img alt="Graphic illustrating probability. Contains on the lefthand side American quarter with caption “from a simple coin…” underneath. Steaming from the right of the coin there is an arrow. To the right of the arrow there are three rows of text. First row “...we can generate a series of flips as our data”, second row “And then calculate the results of what we observed” and third row “If we keep generating more data sets and calculations…”. Across from each of these sentences, there are three rows of letters and percentages. Line one “T,T,H,T,H,H…”, line two “H: 43%, T: 57%” and line three reads “H, H, H, T, T, H… H:53%, T: 47%”. Below there is a break in the graphic that reads “Repeat many many times…” and underneath it reads from the right “We can take the average of these results.” and to the right “H: ~50&amp;, T: ~50%”" sizes="(min-width: 1695px) 729px, (min-width: 680px) 648px, 320px" src="https://ugc.futurelearn.com/uploads/assets/7c/af/hero_7caff046-1034-423f-ae91-9804584f0a4c.jpg" srcset="https://ugc.futurelearn.com/uploads/assets/7c/af/small_hero_7caff046-1034-423f-ae91-9804584f0a4c.jpg 320w, https://ugc.futurelearn.com/uploads/assets/7c/af/hero_7caff046-1034-423f-ae91-9804584f0a4c.jpg 648w, https://ugc.futurelearn.com/uploads/assets/7c/af/large_hero_7caff046-1034-423f-ae91-9804584f0a4c.jpg 729w, https://ugc.futurelearn.com/uploads/assets/7c/af/large_hero_7caff046-1034-423f-ae91-9804584f0a4c.jpg 2x"/></a></p>
<p>Frequently in business research, the analyst is interested in the likelihood that a particular event will occur. Some examples of that are:</p>
<ul>
<li>what is the chance that sales revenue will be higher than €100,000 next month and</li>
<li>how likely is it that an item produced by a machine will be defective?</li>
</ul>
<p>These questions can be answered, subjectively, based on a person’s or group of people’s belief or objectively (mathematically) using Probability Theory.</p>
<p>The purpose of Probability Theory is to try to calculate the probability of a given event occurring.</p>
<p>Suppose that an Event A can happen in k ways out of possible n outcomes. Then the probability of event A occurring is defined to be :</p>
<script type="math/tex; mode=display">P(A)=\frac{k}{n}</script>
<ul>
<li>
<p>P(A) = Probability of Event A occurring<br/></p>
</li>
<li>
<p>P(<script type="math/tex">\bar{A}</script>) = Probability of Event A not occurring<br/></p>
</li>
<li>
<p>P(A or B) = Probability of either Event A or Event B occurring<br/></p>
</li>
<li>
<p>P(A and B) = Probability of both Events A and B occurring<br/></p>
</li>
<li>
<p>P(A/B) =  The Probability of Event A occurring given that Event B has already occurred. This is called the conditional probability<br/></p>
</li>
</ul>
<h3 id="definitions">Definitions</h3>
<p><strong>An event</strong></p>
<p>An event is defined to be an outcome or a group of outcomes from a single random experiment or several experiments.</p>
<p><strong>Mutually exclusive events</strong></p>
<p>Two or more events are called mutually exclusive if the occurrence of any one of them excludes the possible occurrence of the others.</p>
<p><strong>Independent events</strong></p>
<p>Two events are called independent if the occurrence of one event does not affect the probability of the second event occurring.</p>
<p>Watch this episode from the Crash Course Statistics to find out more about the probability rules and Patterns.</p>
<p></p><div><div class="u-responsive-embed">

</div>
<p class="u-italic">
  This is an additional video, hosted on YouTube.
</p>
</div>
<h4 id="probability-rules">Probability Rules</h4>
<ol>
<li>
<p>Probability of an event will always take a value between 0 and 1</p>
</li>
<li>
<p>The sum of the probabilities for all possible outcomes of an experiment is equal to 1</p>
</li>
<li>
<p>P(<script type="math/tex">\bar{A}</script>)=1-P(A)</p>
</li>
<li>
<p>For two mutually exclusive events, A and B, the probability of either event A or event B occurring is given by:
P(A or B)= P(A) + P(B)
if events are not mutually exclusive: 
P(A or B)= P(A) + P(B)-P(A and B)</p>
</li>
<li>
<p>For two independent events, the probability of event A and event B occurring is given by:
P(A and B)= P(A). P(B)
if events are not independent: 
P(A and B)= P(A).P(B/A)</p>
</li>
</ol>
<h3 id="example">Example</h3>
<table>
<tbody>
<tr>
<td>Age Group</td>
<td>Bigshop</td>
<td>Mainstore</td>
<td>Buyalot</td>
<td>Total</td>
</tr>
</tbody>
</table>
<table>
<tbody>
<tr>
<td>Under 40</td>
<td>40</td>
<td>140</td>
<td>40</td>
<td>220</td>
</tr>
</tbody>
</table>
<table>
<tbody>
<tr>
<td>Over 40</td>
<td>80</td>
<td>140</td>
<td>60</td>
<td>280</td>
</tr>
</tbody>
</table>
<table>
<tbody>
<tr>
<td>Total</td>
<td>120</td>
<td>280</td>
<td>100</td>
<td>500</td>
</tr>
</tbody>
</table>
<p>From this data what is the probability that a randomly selected person is:</p>
<ol>
<li>a Bigshop customer</li>
<li>not a Mainstore customer</li>
<li>a Buyalot customer and is Over 40</li>
<li>a Bigshop customer or is Over 40</li>
<li>Over 40 given that they are Mainstore customers</li>
</ol>
<p><strong>Note</strong> in your answers, use the probability laws to demonstrate the results. I appreciate this is an easy task but when things get more complicated you may be glad of your understanding of the probability rules.</p>
</div><p><h2>2.11&emsp;Random variable</h2><div class="u-typography-bold-intro">
<p>Probability and Statistics are the foundational pillars of Data Science. In fact, the underlying principle of machine learning and artificial intelligence is nothing but statistical mathematics and linear algebra.</p>
<p>In Data Science, you often encounter situations where to understand a particular topic, you need to read some research paper which involves a lot of maths.  Because of that,  to becoming a better Data Scientist, requires a strong mathematical understanding.</p>
<ul>
<li>This step is about probability distributions. This term is commonly used in machine learning literature. If you are a beginner, then this is the right place for you to get started, as it will help you to learn about probability jargons like random variables, density curve, probability functions, etc.<br/></li>
<li>Learn about different probability distributions and their distribution functions along with some of their properties.<br/></li>
<li>Learn to create and plot these distributions in Python.<br/></li>
</ul>
<h3 id="random-variables">Random Variables</h3>
<p>A random variable is a variable whose possible values are numerical outcomes of a random phenomenon.</p>
<p>There are two types of random variables: discrete and continuous.</p>
<h3 id="discrete-random-variables">Discrete random variables</h3>
<p>A discrete random variable is one which may take on only a countable number of distinct values and thus can be quantified. For example, you can define a random variable <script type="math/tex">X</script> to be the number which comes up when you roll a fair dice. <script type="math/tex">X</script> can take values : [1,2,3,4,5,6] and therefore is a discrete random variable.</p>
<p>The probability distribution of a discrete random variable is a list of probabilities associated with each of its possible values. It is also sometimes called the <strong>probability function</strong>.</p>
<p>To have a mathematical sense, suppose a random variable <script type="math/tex">X</script> may take k different values, with the probability that <script type="math/tex">X</script>=<em>x<sub>i</sub></em> defined to be <em>P</em>(<script type="math/tex">X</script>=<em>x<sub>i</sub></em>)=<em>p<sub>i</sub></em> . Then the probabilities pi must satisfy the following:<br/>
1: 0&lt;<em>p<sub>i</sub></em> &lt;1 for each i<br/>
2: <em>p</em><sub>1</sub>+<em>p</em><sub>2</sub>+…+<em>p<sub>k</sub></em>=1<br/></p>
<p>Among the  examples of discrete probability distributions are:
- Bernoulli distribution, 
- Binomial distribution, 
- Poisson distribution, etc.</p>
<h3 id="continuous-random-variables">Continuous random variables</h3>
<p>A continuous random variable <em>X<sub>i</sub></em> takes an infinite number of possible values. For example, you can define a random variable <em>X</em> to be the height of students in a class. Since the continuous random variable is defined over an interval of values, it is represented by the area under a curve (or the integral).</p>
<p>The probability distribution of a continuous random variable, known as <strong>probability distribution functions</strong>, are the functions that take on continuous values. The probability of observing any single value is equal to 0 since the number of values which may be assumed by the random variable is infinite. For example, a random variable <script type="math/tex">X</script> may take all values over an interval of real numbers. Then the probability that <script type="math/tex">X</script> is in the set of outcomes <em>A</em>, <em>P</em>(<em>A</em>), is defined to be the area above A and under a curve. The curve, which represents a function <em>p</em>(<em>x</em>), must satisfy the following:</p>
<p>1: The curve has no negative values (<em>p</em>(<em>x</em>)&gt;0 for all <em>x</em>)
2: The total area under the curve is equal to 1.</p>
<p>A curve meeting these requirements is often known as a density curve.</p>
<p>Among the examples of continuous probability distributions are:</p>
<ul>
<li>normal distribution,<br/></li>
<li>exponential distribution,<br/></li>
<li>beta distribution, etc.<br/></li>
</ul>
<p>There’s another type of distribution that often pops up in literature which you should know about called <strong>cumulative distribution function</strong>. All random variables (discrete and continuous) have a cumulative distribution function. It is a function giving the probability that the random variable <script type="math/tex">X</script> is less than or equal to <em>x</em>, for every value <em>x</em>. For a discrete random variable, the cumulative distribution function is found by summing up the probabilities.</p>
<p>Go to the this <a href="https://drive.google.com/open?id=1M7FE-KxhHTiTj6udx60GXD0JczCQgzCx">link</a> to access the Google Colab for this step.</p>
<p>In the next steps, you will explore some important distributions and try to work them out in Python but before that import all the necessary libraries that you’ll use.</p>
</div><p><h2>2.12&emsp;Distributions types - the shape of data</h2><div class="u-typography-bold-intro">
<p>When we use the term distribution in statistics, we usually mean a probability distribution - a mathematical function that provides the probabilities of the occurrence of various possible outcomes in an event or experiment.</p>
<p>In the next step, we are going to look closer at several types of distribution:</p>
<h4 id="continuous-probability-distributions">Continuous probability distributions</h4>
<ul>
<li>Uniform Distribution</li>
<li>Normal Distribution (Gaussian)</li>
<li>Exponential Distribution</li>
</ul>
<h4 id="discrete-probability-distributions">Discrete probability distributions</h4>
<ul>
<li>Poisson Distribution</li>
<li>Binomial Distribution</li>
</ul>
<p>Before jumping to the next step and work with some examples, watch the video below. It gives a good explanation of distributions types and their use.</p>
<p></p><div><div class="u-responsive-embed">

</div>
<p class="u-italic">
  This is an additional video, hosted on YouTube.
</p>
</div>
</div><p><h2>2.13&emsp;Uniform distribution</h2><div class="u-typography-bold-intro">
<p>One of the simplest and most useful distribution types is the uniform distribution.</p>
<p>This two-minute video by 365 Data Science gives a quick introduction to the uniform distribution.</p>
<p></p><div><div class="u-responsive-embed">

</div>
<p class="u-italic">
  This is an additional video, hosted on YouTube.
</p>
</div>
<p>The probability distribution function of the continuous uniform distribution is:</p>
<script type="math/tex; mode=display">% <![CDATA[
f(x)=  \left\{\begin{array}{ll}
\frac{1}{b-a} & \text{for}~a~\leq~x~\leq~b, \\
0 & \text{for}~x <~a~ \text{or}~ x > b
\end{array}\right. %]]></script>
<p>Since any interval of numbers of equal width has an equal probability of being observed, the curve describing the distribution is a rectangle, with constant height across the interval and <em>0</em> height elsewhere.</p>
<p>Since the area under the curve must be equal to 1, the length of the interval determines the height of the curve. The following figure shows a uniform distribution in interval (a,b).  Notice since the area needs to be 1, the height is set to 1/(b-a)</p>
<p>for a &lt; x &lt; b,</p>
<p>for x &lt; a or x &gt; b</p>
<p><img alt="A uniform distribution diagram" sizes="(min-width: 1695px) 729px, (min-width: 680px) 648px, 320px" src="https://ugc.futurelearn.com/uploads/assets/1d/2a/hero_1d2ab224-350a-4c9b-a90d-374797631b4f.png" srcset="https://ugc.futurelearn.com/uploads/assets/1d/2a/small_hero_1d2ab224-350a-4c9b-a90d-374797631b4f.png 320w, https://ugc.futurelearn.com/uploads/assets/1d/2a/hero_1d2ab224-350a-4c9b-a90d-374797631b4f.png 648w, https://ugc.futurelearn.com/uploads/assets/1d/2a/large_hero_1d2ab224-350a-4c9b-a90d-374797631b4f.png 729w, https://ugc.futurelearn.com/uploads/assets/1d/2a/large_hero_1d2ab224-350a-4c9b-a90d-374797631b4f.png 2x"/></p>
<p><sub>Source: <a href="https://commons.wikimedia.org/wiki/File:Uniform_Distribution_PDF_SVG.svg">Creative Commons</a></sub></p>
<p>You can visualise uniform distribution in python with the help of a random number generator acting over an interval of numbers (a,b).  Let’s move to Google Colab environment to do that.</p>
<h4 id="google-colab-task">Google Colab task:</h4>
<p>Go the following <a href="https://drive.google.com/open?id=17ojSkn7VTWRpPKAvAsN_0SerIfiQI0Y6">Google Colab</a> and work with the example I prepared for you (as usual, suggest you open the link in the new tab for easier navigation between the platforms).</p>
</div><p><h2>2.14&emsp;Normal distribution</h2><div class="u-typography-bold-intro">
<p>A normal distribution, also known as Gaussian distribution, is ubiquitous in Data Science. You will encounter it in many places, especially in topics of statistical inference. It is one of the assumptions of many data science algorithms too.</p>
<p>A normal distribution has a bell-shaped density curve described by its mean μ and standard deviation σ. The density curve is symmetrical, centred about its mean, with its spread determined by its standard deviation showing that data near the mean are more frequent in occurrence than data far from the mean. The probability distribution function of a normal density curve with mean μ and standard deviation σ at a given point x is given by:</p>
<hr/>
<script type="math/tex; mode=display">f(x\mid\mu, \sigma^2)= \frac{1}{\sqrt{2\pi\sigma^2}}e-^\frac{(x-\mu)^2}{2\sigma^2}</script>
<p>Below is the figure describing what the distribution looks like:</p>
<p><img alt="A visual representation of what distribution is from mu -20 to mu +20." sizes="(min-width: 1695px) 729px, (min-width: 680px) 648px, 320px" src="https://ugc.futurelearn.com/uploads/assets/f7/a3/hero_f7a3fdb6-10e9-470a-970b-47c95121a71e.png" srcset="https://ugc.futurelearn.com/uploads/assets/f7/a3/small_hero_f7a3fdb6-10e9-470a-970b-47c95121a71e.png 320w, https://ugc.futurelearn.com/uploads/assets/f7/a3/hero_f7a3fdb6-10e9-470a-970b-47c95121a71e.png 648w, https://ugc.futurelearn.com/uploads/assets/f7/a3/large_hero_f7a3fdb6-10e9-470a-970b-47c95121a71e.png 729w, https://ugc.futurelearn.com/uploads/assets/f7/a3/large_hero_f7a3fdb6-10e9-470a-970b-47c95121a71e.png 2x"/></p>
<p>Almost 68% of the data falls within a distance of one standard deviation from the mean on either side and 95% within two standard deviations. Also, it is worth mentioning that distribution with mean 0 and standard deviation 1 is called a standard normal distribution.</p>
<p>You can generate a normally distributed random variable using scipy.stats module’s norm.rvs() method. The loc argument corresponds to the mean of the distribution, scale corresponds to standard deviation and size to the number of random variates. If you want to maintain reproducibility, include a random_state argument assigned to a number.</p>
<p>The normal distribution is significant to probability and statistics thanks to two factors: <strong>the Central Limit Theorem</strong> and <strong>the Three Sigma Rule</strong>.</p>
<h4 id="central-limit-theorem">Central Limit Theorem</h4>
<p>In the previous section, we demonstrated that if we repeated our 10-toss trials many, many times, the <em>average</em> heads-count of all of these trials will approach the 50% we expect from an ideal coin. With more trials, the closer the average of these trials approach the <em>true</em> probability, even if the individual trials themselves are imperfect. This idea is a key tenet of the Central Limit Theorem. In our coin-tossing example, a single trial of 10 throws produces a single <em>estimate</em> of what probability suggests <em>should</em> happen (5 heads). We call it an estimate because we know that it won’t be perfect (i.e. we won’t get 5 heads every time).</p>
<p>If we make many estimates, the Central Limit Theorem dictates that the <strong>distribution</strong> of these <strong>estimates</strong> will look like a normal distribution. The zenith of this distribution will line up with the true value that the estimates should take on. In statistics, the peak of the normal distribution lines up with the mean, and that’s exactly what we observed. Thus, given multiple “trials” as our data, the Central Limit Theorem suggests that we can hone in on the theoretical ideal given by probability, even when we don’t know the true probability. Central Limit Theorem lets us know that the average of many trials means will approach the true mean, the Three Sigma Rule will tell us how much the data will be spread out around this mean.</p>
<h4 id="three-sigma-rule">Three Sigma Rule</h4>
<p>The Three Sigma rule, also known as the empirical rule or 68-95-99.7 rule, is an expression of how many of our observations fall within a certain distance of the mean. Remember that the standard deviation (a.k.a. “sigma”) is the average distance an observation in the dataset is from the mean. The Three Sigma rule dictates that <strong>given a normal distribution</strong>, 68% of your observations will fall between one standard deviation of the mean. 95% will fall within two, and 99.7% will fall within three. A lot of complicated math goes into the derivation of these values, and as such, is out of the scope of this article. The key takeaway is to know that the Three Sigma Rule enables us to know how much data is contained under different intervals of a normal distribution.</p>
<p>Before we move further to some practical examples, watch the video below.</p>
<p></p><div><div class="u-responsive-embed">

</div>
<p class="u-italic">
  This is an additional video, hosted on YouTube.
</p>
</div>
<p>You can visualise the distribution just like you did with the uniform distribution, using seaborn’s distplot functions. The meaning of the arguments remains the same as in the last case.</p>
<h4 id="google-colab-task">Google Colab Task</h4>
<p>You can visualise the normal distribution. Go the following <a href="https://drive.google.com/open?id=1yrTQVTnoLuHa96l4MGppXoIOhYBxX_0-">Google Colab</a> and work with the example I prepared for you.</p>
</div><p><h2>2.15&emsp;Exponential distribution</h2><div class="u-typography-bold-intro">
<p>The exponential distribution describes the time between events in a Poisson point process, i.e., a process in which events occur continuously and independently at a constant average rate. It has a parameter λ called rate parameter, and its equation is described as:</p>
<script type="math/tex; mode=display">% <![CDATA[
f(x; \lambda)= \left\{\begin{array}{ll}
\lambda e^{-\lambda x} & x\ge 0, \\
0   &    x<0. \\
\end{array}\right. %]]></script>
<p>A decreasing exponential distribution looks like:</p>
<p><img alt="A graph showing a decreasing exponential distribution with a blue line" sizes="(min-width: 1695px) 729px, (min-width: 680px) 648px, 320px" src="https://ugc.futurelearn.com/uploads/assets/2b/5a/hero_2b5ad5e5-c616-459d-aae8-8bae15c07fe1.png" srcset="https://ugc.futurelearn.com/uploads/assets/2b/5a/small_hero_2b5ad5e5-c616-459d-aae8-8bae15c07fe1.png 320w, https://ugc.futurelearn.com/uploads/assets/2b/5a/hero_2b5ad5e5-c616-459d-aae8-8bae15c07fe1.png 648w, https://ugc.futurelearn.com/uploads/assets/2b/5a/large_hero_2b5ad5e5-c616-459d-aae8-8bae15c07fe1.png 729w, https://ugc.futurelearn.com/uploads/assets/2b/5a/large_hero_2b5ad5e5-c616-459d-aae8-8bae15c07fe1.png 2x"/></p>
<p>You can generate an exponentially distributed random variable using scipy.stats module’s expon.rvs() method which takes shape parameter scale as its argument which is nothing but 1/lambda in the equation. To shift distribution use the loc argument, size decides the number of random variates in the distribution. If you want to maintain reproducibility, include a random_state argument assigned to a number.</p>
<p>This video explains the exponential distribution quite well. It is 20 minutes long, but I recommend to watch before you go to the next step.</p>
<p></p><div><div class="u-responsive-embed">

</div>
<p class="u-italic">
  This is an additional video, hosted on YouTube.
</p>
</div>
<p>Again, visualising the distribution yields the curve shown in the Google Colab environment.</p>
<h4 id="google-colab-task">Google Colab Task</h4>
<p>You can visualise the normal distribution. Go the following <a href="https://drive.google.com/open?id=1s50sDqY4W6Bjnlh0MsZAFYzdLDW-2P5q">Google Colab</a> and work with the example I prepared for you.</p>
</div><p><h2>2.16&emsp;Poisson distribution</h2><div class="u-typography-bold-intro">
<p>In probability theory and statistics, the Poisson distribution, named after French mathematician Siméon Denis Poisson, is a discrete probability distribution that expresses the probability of a given number of events occurring in a fixed interval of time or space if these events occur with a known constant rate and independently of the time since the last event. The Poisson distribution can also be used for the number of events in other specified intervals such as distance, area or volume.<br/>
<br/>
<sub>Source: <a href="https://en.wikipedia.org/wiki/Poisson_distribution">Wikipedia</a></sub></p>
<p>The Poisson random variable is typically used to model the number of times an event happened in a time interval. For example, the number of users visited on a website in an interval can be thought of a Poisson process.</p>
<p>Poisson distribution is described in terms of the rate (μ) at which the events happen. An event can occur 0, 1, 2, … times in an interval. The average number of events in an interval is designated λ (lambda). Lambda is the event rate, also called the rate parameter. The probability of observing k events in an interval is given by the equation:</p>
<script type="math/tex; mode=display">p(k~\text {events in interval}) =~e^\lambda \frac{\lambda^k}{k!}</script>
<p>Note that the normal distribution is a limiting case of Poisson distribution with the parameter λ→∞. Also, if the times between random events follow an exponential distribution with rate λ, then the total number of events in a time period of length t follows the Poisson distribution with parameter λt.
The following figure shows a typical Poisson distribution:</p>
<p><img alt="A graph showing a poisson distribution. A dotted blue line is shown on the graph. On the left-hand side of the graph it is numbered “0.00”, ”0.025”, ”0.050”, ”0.075”, ”0.100” , ”0.125”, ”0.150”, ”0.175” and the bottom axis reads“0.0”, “2.5”, “5.0”, “7.5”, “10.0”, “12.5”, “15.0”, “17.5” and “20.0”." sizes="(min-width: 1695px) 729px, (min-width: 680px) 648px, 320px" src="https://ugc.futurelearn.com/uploads/assets/d9/04/hero_d9047467-28c2-4f08-9ab5-0087f0e7c22d.png" srcset="https://ugc.futurelearn.com/uploads/assets/d9/04/small_hero_d9047467-28c2-4f08-9ab5-0087f0e7c22d.png 320w, https://ugc.futurelearn.com/uploads/assets/d9/04/hero_d9047467-28c2-4f08-9ab5-0087f0e7c22d.png 648w, https://ugc.futurelearn.com/uploads/assets/d9/04/large_hero_d9047467-28c2-4f08-9ab5-0087f0e7c22d.png 729w, https://ugc.futurelearn.com/uploads/assets/d9/04/large_hero_d9047467-28c2-4f08-9ab5-0087f0e7c22d.png 2x"/></p>
<p>You can generate a Poisson distributed discrete random variable using scipy.stats module’s poisson.rvs() method which takes λ as a shape parameter. To shift distribution use the loc parameter. The command size decides the number of random variates in the distribution. If you want to maintain reproducibility, include a random_state argument assigned to a number.</p>
<p>Watch this video which explains the Poisson distribution in less than 10 minutes. 
</p><div><div class="u-responsive-embed">

</div>
<p class="u-italic">
  This is an additional video, hosted on YouTube.
</p>
</div>
<h4 id="google-coalb-task">Google Coalb Task</h4>
<p>Go the following <a href="https://drive.google.com/open?id=1cbkNDAM3qk5sv2DAOHS5ucYiMXuBQK5W">Google Colab file</a> for this step and work with the Poisson Distribution example I prepared for you. This code will show you how to simulate the Poisson distribution.</p>
</div><p><h2>2.17&emsp;Binomial distribution</h2><div class="u-typography-bold-intro">
<p>A distribution where only two outcomes are possible, such as success or failure, gain or loss, win or lose and where the probability of success and failure is the same for all the trials is called a <a href="https://en.wikipedia.org/wiki/Binomial_distribution">binomial distribution</a>.</p>
<p>The outcomes need not be equally likely, and each trial is independent of each other. The parameters of a binomial distribution are n and p where n is the total number of trials, and p is the probability of success in each trial. Its probability distribution function is given by :</p>
<script type="math/tex; mode=display">f(k,~n,~p)~=  \text{Pr}(k; n, p)~= \text{Pr}(X=k)~= \left(\frac{n}{k}\right)p^k(1-p)^{n-k}</script>
<p>where:</p>
<script type="math/tex; mode=display">\left(\frac{n}{k}\right)=~\frac{n!}{k!(n-k)!}</script>
<p>You can generate a binomial distributed discrete random variable using scipy.stats module’s binom.rvs() method which takes n (number of trials) and p (probability of success) as shape parameters. To shift distribution use the loc parameter. Size decides the number of times to repeat the trials. If you want to maintain reproducibility, include a random_state argument assigned to a number.</p>
<p>I again would like to suggest watching this video that I found on the CrashCourse youtube channel explaining. It explains the binomial distribution in 15 minutes.</p>
<p></p><div><div class="u-responsive-embed">

</div>
<p class="u-italic">
  This is an additional video, hosted on YouTube.
</p>
</div>
<sub>Source: <a href="https://www.youtube.com/channel/UCX6b17PVsYBQ0ip5gyeme-Q">CrashCourse</a></sub>
<h4 id="google-coalb-task">Google Coalb Task</h4>
<p>Go to the following <a href="https://drive.google.com/open?id=1PeYvGO_i29dGMzqJjNZ8ElX8czhMG2kG">Google Colab file</a> for this step and work with the binomial distribution example I have prepared for you.</p>
</div><p><h2>2.19&emsp;Review of Topic 2</h2><div class="u-typography-bold-intro">
<p>Congratulations on completing the second topic of the Introduction to Data Analytics and Data Mining.</p>
<p>Listen to the audio clip above, where Andrew discusses and highlights some issues covered under Topic 2.</p>
<p>In this topic, we started a revision of statistics and probability so having completed it, you should be able to:</p>
<ul>
<li>Explain what the descriptive and inferential statistics are.</li>
<li>Implement some simple descriptive statistics.</li>
<li>Define measures of spread and outliers.</li>
<li>Discuss dispersion and types of data.</li>
<li>Define and give examples of different distribution types.</li>
<li>Run simple graphical presentations.</li>
</ul>
<p>In the next topic, we will focus specifically on the most popular statistical tests, regression and analysis of variance.</p>
<p>Remember that you can post any questions or comments that you may have regarding anything we have covered in the course so far in the comments section below.</p>
</div><p><h2>2.20&emsp;Let's move on to Topic 3 </h2><div class="u-typography-bold-intro">
<p>Congratulations on completing Topic 2.</p>
<p>Let us now move forward to <a href="https://www.futurelearn.com/courses/intro-to-data-analytics/1/steps/661637">Topic 3: Probability,  regression and analysis of variance</a>.</p>
</div><p style="page-break-before: always"><h2>3.1&emsp;Welcome to Topic 3</h2><div class="u-typography-bold-intro">
<p>Welcome to the third topic of the Introduction to Data Analytics and Data Mining course by Dublin City University.</p>
<p>In this topic, we will review the most popular statistical tests and explore regression and analysis of variance.</p>
<p>After completing this topic, you will be able to:</p>
<ul>
<li>Explain the purpose of statistical tests.</li>
<li>List the most popular statistical tests.</li>
<li>Outline and discuss the process of statistical testing.</li>
<li>Discuss and compare tests such as Chi-Square test and ANOVA.</li>
<li>Define and discuss regression modelling.</li>
<li>Apply basic linear and logistic models.</li>
<li>Conduct process of hypothesis testing based on a working example.</li>
</ul>
<p>We have included several discussion prompts to encourage you to interact with your fellow learners as you go through this topic. There are also some examples and hands-on activities designed for you to support the learning.</p>
<p>Remember if you have any questions about any of the subjects or concepts covered within this topic, you can post them in the various comments sections throughout the course or in <a href="https://www.futurelearn.com/courses/intro-to-data-analytics/1/steps/661797">Review of Topic 3 step</a>.</p>
<p>Let’s get started!</p>
</div><p><h2>3.2&emsp;Why do we need statistical tests?</h2><div class="u-typography-bold-intro">
<p>In this section, we are going to review statistical testing. Before we delve into the topic, we would like you to think about how statistics are used in everyday life or how they can help you in your data analytics journey.</p>
<p>In many situations we use a sample to get a representation of the population. You will have seen this in political elections, where polls are conducted before an election. In this example, you will regularly hear about the margin of error. This means that the sample or poll is subject to % error that if we added or subtracted it from our sample proportion of votes, we would probably find the true proportion.</p>
<p>You will all have used medications or drugs during your life to cure a headache or some illness. Each of these drugs is tested using samples of patients who either take the drug or are given a placebo. These two groups were then compared using a statistical test. If the company proposing the drug did not find a statistically significant difference using an appropriate statistical test then the drug would not be permitted to be sold.</p>
<p>Statistical testing allows us to make inferences about the true population from a sample.
It is used in many areas including the social sciences, engineering and health for example.</p>
<p>Can you think of any other situations where statistical testing could be used in your life?</p>
<p>Share your thoughts in the comment section below. You might consider the following prompts when posting your answer:</p>
<ul>
<li>Do you think statistical testing could be useful in data analytics?</li>
<li>Why do you think so?</li>
<li>Can you give some other examples of how they are used?</li>
</ul>
</div><p><h2>3.3&emsp;Statistical tests</h2><div class="u-typography-bold-intro">
<p>We live in the data and information world and we use statistics to keep us informed about  what is happening in the world around us.</p>
<p>Many of us are now using sports equipment to record our training performances, or in the industry, we regularly measure key performance indicators. When we get a personal best does it really mean we have jumped forward with our training or is it just by chance that we got this result.  Or when your company gets a great sales result is this really down to the marketing campaign or is it just by chance.</p>
<p>Statistical testing helps us to check a hypothesis or a prediction about a population parameter using an estimate of that parameter calculated from a sample of the population.</p>
<p>Statistical tests have many applications in business. They can, for example, help to analyse, whether:</p>
<ul>
<li>a marketing campaign has increased a company’s sales figures.</li>
<li>a marketing campaign has increased a company’s market share (In many businesses, examples tests are used to examine differences between groups).</li>
<li>males and females spend the same amount on magazines.</li>
<li>urban or rural consumers are more likely to buy a product.</li>
<li>consumers in France, Germany, Ireland and the UK have different views on what is important when buying a car.</li>
</ul>
<h4 id="why-do-we-need-statistical-tests">Why do we need statistical tests?</h4>
<p>Consider the results of a survey that show that a sample of males spent an average of €20 a week on magazines and that a sample of females spent an average of €21 a week on magazines.</p>
<p>Does this prove that females spend more on magazines on average?</p>
<p>Not necessarily because we know that both results are only sample estimates. It is not possible to directly prove or disprove a hypothesis using sample estimates alone, because these statistics will have inherent sample errors.</p>
<p>So we need some technique or techniques to help us decide whether a sample parameter provides enough evidence to prove a hypothesis about the population parameter. These techniques are called statistical tests or hypothesis tests.
These tests are designed to check if the sample results observed are statistically significant. Put simply, in the example above we would like to know if the difference between males and females was due to random chance or due to a significant difference in expenditure between them.</p>
<p>I would like you to consider how we could use statistical testing to determine if a data point is an outlier in our data. Does this make sense? Can you explain why?
Post your answers in the comment section.</p>
</div><p><h2>3.4&emsp;6 Steps of Statistical Testing</h2><div class="u-typography-bold-intro">
<p>Regardless of the problem you have, the steps you need to follow will be the same or very similar for most statistical tests. You may want to test if your population mean is likely to be greater than some value, or that the proportion of people with a third level education is greater than 40%. The process is nearly always the same.</p>
<h2 id="steps-of-statistical-testing">6 Steps of Statistical Testing</h2>
<ol>
<li>State the Null and Alternative Hypotheses</li>
<li>Set the precision level</li>
<li>Gather sample data and calculate sample statistics</li>
<li>Choose the appropriate Statistical Test<br/>
 5  Calculate the Test Statistic</li>
<li>Interpret the Results of the Test / Examine the P-value</li>
</ol>
<p>I will explain the steps below and give you some real-life examples. I also have a Google Colab file set for you to practice completing a statistical test for the comparison of the mean between two groups using two samples.</p>
<h4 id="state-the-null-and-alternative-hypotheses">1. State the Null and Alternative Hypotheses</h4>
<p>The first step is to outline the objectives of the test. The Null and Alternative Hypothesis are opposing statements about the population parameter being tested. In general, the researcher is trying to prove the Alternative Hypothesis by showing that the null hypothesis is false. For instance, consider HR Research Ltd. who are examining pay rates in two different sectors, A and B, to test if there is a difference between them using a sample of employees from both sectors. In this case, the null hypothesis would state that the Average Salaries in both sectors are equal, while the Alternative would state that the Average Salaries are not equal.</p>
<p>Mathematically this can be written as</p>
<p>Null
<script type="math/tex">H_0</script>: <script type="math/tex">\mu_{a}</script> = <script type="math/tex">\mu_{b}</script> or <script type="math/tex">\mu_{a}-\mu_{b}</script> = 0 (Average Salary is equal in both sectors)</p>
<p>Alternative
<script type="math/tex">H_1</script>: <script type="math/tex">% <![CDATA[
\mu_{a} <> \mu_{b} %]]></script> or <script type="math/tex">% <![CDATA[
\mu_{a} - \mu_{b} <> 0 %]]></script> (Average Salary is not equal in both sectors)</p>
<h4 id="set-the-precision-level">2. Set the precision level</h4>
<p>At the initial stage, the researcher will also set the level of precision for the test. This is the level of confidence the researcher wishes to achieve before rejecting the null hypothesis. The 95% confidence level is the accepted norm in Social Science and Business Research although occasionally some researchers will use other levels (99%, 90% etc.). Please note that we can never be 100% confident about a sample result.</p>
<h4 id="gather-sample-data-and-calculate-sample-statistics">3. Gather sample data and calculate sample statistics</h4>
<p>To test the hypothesis, a sample of data is selected from the population, and the relevant sample statistics (mean, standard deviation, proportion, etc.) are calculated. In the HR Research Ltd. example shown below, they would calculate the relevant descriptive statistics for the problem. In this case, they would need the sample means, sample standard deviations and sample sizes.</p>
<p><script type="math/tex">\bar{X}_a =</script> €<script type="math/tex">30,500</script> <script type="math/tex">\bar{X}_b =</script> €<script type="math/tex">30,000</script></p>
<p><script type="math/tex">S_A =</script> €<script type="math/tex">9,400</script> <script type="math/tex">S_B =</script> €<script type="math/tex">8,900</script></p>
<p><script type="math/tex">n_A= 100</script> <script type="math/tex">n_B = 100</script></p>
<p>The t-statistic can be calculated using the following formula:</p>
<p><script type="math/tex">t=\frac{M_x-M_y}{\sqrt{\frac{S_x^2}{n_x}+\frac{ S_y^2}{n_y}}}</script>
<sub>*<em>M</em> = mean, <em>n</em> = number of scores per group</sub></p>
<p><script type="math/tex">S^2=\frac{\sum(x-M)^2}{n-1}</script><sub>*<em>x</em> = individual scores, <em>M</em> = mean, <em>n</em> = number of scores in group</sub></p>
<p>The simple python code for the above example is available in this <a href="https://drive.google.com/open?id=18f-NmvcnaGdnHuzV2WY2fMiQ67vJWS0H">Google Colab file</a>. Vary <script type="math/tex">N_a</script> and <script type="math/tex">N_b</script> and see what happens. Remember to check that you understand how the code relates to the formulas above.</p>
<h4 id="choose-the-appropriate-statistical-test">4. Choose the appropriate Statistical Test</h4>
<p>To find the appropriate test, we must look at the type of variable we are testing and what the test is trying to show. In the HR Research Ltd. example, they are looking at a difference in salaries, so the test variable is a Salary, which is a Ratio variable. They are trying to compare the salaries in two sectors, A and B, so the test is a measure of the difference between the two groups. The appropriate test for this problem type is called an Independent Sample t-Test. We will look at a sample of some of the more widely used tests later in this section.</p>
<h4 id="calculate-the-test-statistic">5. Calculate the Test Statistic</h4>
<p>To test the null hypothesis, the difference between the hypothesised value of the parameter and the sample value of the parameter is calculated. For instance, the null hypothesis for the HR Research Ltd. problem stated that the difference between salaries in the two sectors is equal to zero. However, they found in a survey that respondents in Sector A were earning on average €500 more than the respondents in Sector B. In order to test if this difference is statistically significant we calculate a test statistic (in this case a t-value). The test statistic is a measure of the relative size of the difference, taking into account the amount of variation in the data and the sample size. The t-value for the HR Research Ltd. data is equal to .386</p>
<p>At the end of this section, we will briefly look at how two of these test statistics are calculated, but in general, this calculation is performed using appropriate statistical software.</p>
<h4 id="interpret-the-results-of-the-test--examine-the-p-value">6. Interpret the Results of the Test / Examine the P-value</h4>
<p>Unfortunately, there are a large number of test statistics, and they are not always easy to interpret, without detailed knowledge of how the particular test works. What for instance does a t-value equal to .386 tell us? Fortunately, due to developments in statistical software, there is a more straightforward way of interpreting the results of a statistical test, by using what is called the p-value of the test, which is common to all tests.</p>
<p>The p-value measures the probability of calculating the observed sample value if the null hypothesis was, in fact, true.</p>
<p>The decision to reject or “accept” the null hypothesis (to test if it is false) can be made by examining the p-value. Quite simply, if the p-value is less than .05 then there is less than 5% of a chance that the null hypothesis is true and conversely more than a 95% chance that is false so that it can be rejected with 95% confidence. However, if it is above .05, then the null hypothesis cannot be rejected at this level. The p-value for the HR Research study was .7, so we cannot reject the null hypothesis. It indicates that there is no evidence to suggest that there are significant salary differences between Sectors A and B. You will see the p-value that is calculated from the Google Colab code.</p>
</div><p><h2>3.5&emsp;Popular statistical tests for differences</h2><div class="u-typography-bold-intro">
<p>There are many statistical steps used. Below, I listed some of them that are most widely used in many social science fields. I have also noted the context in which where they should be applied.</p>
<h3 id="tests-for-differences-between-two-independent-groups">Tests for differences between two independent groups</h3>
<p><strong>Test Variable</strong> - <a href="https://towardsdatascience.com/statistical-tests-when-to-use-which-704557554740">Statistical Test</a></p>
<p><strong>Nominal Variable</strong> - <a href="https://en.wikipedia.org/wiki/Chi-squared_test">Chi-Square Test</a></p>
<p><strong>Ratio Variable</strong> - <a href="https://en.wikipedia.org/wiki/Z-test">Independent Sample T-Test/Z-Test*</a> (The Independent Samples t Test compares the means of two independent groups in order to determine whether there is statistical evidence that the associated population means are significantly different.</p>
<p><strong>Ordinal Variable</strong> - <a href="https://en.wikipedia.org/wiki/Mann%E2%80%93Whitney_U_test">Mann-Whitney U Test</a> (this is a non-parametric test. This test can be used to investigate whether two independent samples were selected from populations having the same distribution.)</p>
<h3 id="tests-for-differences-between-more-than-two-independent-groups">Tests for differences between more than two independent groups</h3>
<p><strong>Test Variable</strong> - Statistical Test</p>
<p><strong>Nominal Variable</strong> - <a href="https://en.wikipedia.org/wiki/Chi-squared_test">Chi-Square Test</a></p>
<p><strong>Ratio Variable</strong> - <a href="https://www.technologynetworks.com/informatics/articles/one-way-vs-two-way-anova-definition-differences-assumptions-and-hypotheses-306553">One – Way ANOVA*</a></p>
<p><strong>Ordinal Variable</strong> - <a href="https://en.wikipedia.org/wiki/Kruskal%E2%80%93Wallis_one-way_analysis_of_variance">Kruskal – Wallis</a></p>
<p>One of the major decisions when choosing a statistical test is the assumptions that are made for each test. There are basically two families of test and they are parametric and non-parametric tests. The basic difference is parametric tests have stricter assumptions but when these assumptions are met they give better and more accurate results and have a higher degree of accuracy. Non-parametric tests have very little or no assumptions but are not as accurate.</p>
<p>We are going to look at some of them in more detail in the next few steps.</p>
</div><p><h2>3.6&emsp;Three assumptions underlying statistical tests</h2><div class="u-typography-bold-intro">
<p>Most statistical tests have several assumptions underlying them. It is essential to check these assumptions when using a test. Violations in assumptions can lead to making incorrect conclusions when interpreting the test results.</p>
<h4 id="parametric-tests-normality-assumption">1. Parametric Tests (Normality assumption)</h4>
<p>T-Tests and One-way Anova are from a family of parametric tests, which is based on the assumption that the test variable is Normally distributed. This means that strictly speaking parametric tests can only be used for ratio/interval variables, that follow a Normal Distribution. This assumption is relaxed for sample statistics calculated from a large sample (n&gt;50), but for smaller samples, the normality of the variable should be tested before using a parametric test. If they are not Normal, the non-parametric equivalent (Mann-Whitney U Test or Kruskal -Wallis) should be used instead.</p>
<h4 id="sample-size-requirements">2. Sample Size Requirements</h4>
<p>The sample size is also an important factor. The bigger the sample size, the more powerful the test and the more likely we are to identify statistically significant results.</p>
<p>As well as looking at the overall sample size, we need to look at the sample sizes in each group:</p>
<ul>
<li>The strict minimum per group for a statistical test is 3; however, the practical minimum will be higher for most tests.</li>
<li>The minimum sample size per group recommended for the parametric tests is 10.</li>
<li>There are similar requirements for non-parametric tests.</li>
<li>The Chi-Square assumes an expected minimum sample size of 5 in each cell. If this assumption is violated, it can affect the validity of the test. PASW/SPSS indicates how many cells violate this assumption.</li>
</ul>
<p>Sometimes it is necessary to collapse groups or remove a group from analysis, if the sample size within the group is too small. Consider a class where there are 40 students: 15 from Ireland, 15 from the UK, 4 from Germany, 4 from France and 2 from Spain. We cannot conduct a One Way ANOVA on the five groups due to small sample sizes. However, we may decide to either compare the Irish and UK students or combine the French, German and Spanish students into a third group.</p>
<h4 id="independent-observations">3. Independent Observations</h4>
<p>The independent assumption is one of the most important and can have the most detrimental effects on the test results. All of the tests we examine here require independent measurements. In other words, each data point must be independent of the others.
For instance, if we have a set of monthly sales figures for a product, we cannot say that these are independent as, clearly, last month’s sales will have an impact on this month’s sales.</p>
<p>I only gave you some examples of issues involved in testing assumptions. It can be a complex issue as there will always be some violations of these assumptions, so we need to look at the size and nature of the violations. There are also differing views in different disciplines as to how these violations should be tackled.</p>
<h3 id="your-task">Your task</h3>
<p>In this step, we have talked about the assumptions underpinning parametric tests.  One of the problems that many social scientists face is when we have repeated measures from the same subject. A brief introduction to repeated measures design can be found <a href="https://en.wikipedia.org/wiki/Repeated_measures_design">here</a>.</p>
<p>Can you find a test that would be appropriate when comparing pre- and post-treatment effect on a group of patients? Why can’t we use a simple two-sample independent t-test?</p>
<p>Post your reply in the comments section below. Make sure to read and comment on other learners posts!</p>
</div><p><h2>3.7&emsp;Chi-square test </h2><div class="u-typography-bold-intro">
<p>The chi-square test is used when examining the difference between two or more groups, and the test variable is for  Nominal variables. It can also be seen as a test that measures the relationship between two nominal variables.</p>
<p>I have an example of using this test, but before we look at this, watch this video by CrashCourse explaining how chi-square tests are used to measure differences in strictly categorical data and how we can use each of these tests to make comparisons.</p>
<p></p><div><div class="u-responsive-embed">

</div>
<p class="u-italic">
  This is an additional video, hosted on YouTube.
</p>
</div>
<h4 id="chi-square-test-chi2-example">Chi-square test <script type="math/tex">\chi2</script> Example</h4>
<p>Let’s use the example below, to see how to the chi-square test functions.</p>
<p>The following table shows the breakdown of students by gender and mode of transport to college, shown in Table 1:</p>
<table>
<thead>
<tr>
<th>Mode of Transport</th>
<th>Male</th>
<th>Female</th>
<th>Total</th>
</tr>
</thead>
<tbody>
<tr>
<td>Bus</td>
<td>80</td>
<td>220</td>
<td>300</td>
</tr>
<tr>
<td>Walk</td>
<td>50</td>
<td>50</td>
<td>10</td>
</tr>
<tr>
<td>Bicycle</td>
<td>40</td>
<td>10</td>
<td>50</td>
</tr>
<tr>
<td>Car</td>
<td>30</td>
<td>20</td>
<td>50</td>
</tr>
<tr>
<td>Total</td>
<td>200</td>
<td>300</td>
<td>500</td>
</tr>
</tbody>
</table>
<p><sub><strong>Table 1</strong></sub></p>
<p>Does this data suggest that males and females use different modes of transport to get to college?</p>
<p>The test variable in this example is the mode of transport, which is a Nominal variable. We are examining the difference between two groups - males and females, so the appropriate test is the chi-square test.</p>
<p>This test is a little more complex than the Z-test and involves three steps.</p>
<p><strong>1.  State the null and alternative hypotheses</strong></p>
<p><script type="math/tex">H</script><sub>0</sub>: Males and Females use the same modes of transport to get to college.<br/>
<script type="math/tex">H</script><sub>1</sub>: Males and Females do not use the same modes of transport to get to college.</p>
<p><strong>2. Calculate the Expected values</strong></p>
<p>So the Expected values are the values we would expect to get from the survey if the null hypothesis was true.</p>
<p>For instance, if there was no difference between males and females, how many males would we expect to take the bus? 300 students out of 500 take the bus, which is equal to 60%(.6) of the students, so if there was no difference between the genders, we would expect 60% of males and 60% of females to take the bus. So;</p>
<p><script type="math/tex">E(Bus-Male)=200\times300/500=120</script>
<script type="math/tex">E(Bus-Female)=300\times300/500=180</script></p>
<p>We can calculate the rest of the expected values using the same formula 
<script type="math/tex">E(AandB)=na.nb/n</script><br/>
where<br/>
<script type="math/tex">na=\text{Number in category A,}</script><br/>
<script type="math/tex">nb=\text{Number in category B,}</script><br/>
<script type="math/tex">n = \text{Total number in the sample}</script><br/></p>
<p><strong>3. Calculate the chi-square value</strong></p>
<p>The next step is to calculate the difference between the values observed(Oi) in our survey and the expected theoretical values(Ei) using the following formula:</p>
<p><script type="math/tex">\chi{2}</script>= <script type="math/tex">\frac{\sum_{i=1}^n (O_i-E_i)}{E_i}</script></p>
<p>The chi-square provides a measure of the average relative squared differences between the observed and expected values.</p>
<p>A  chi-square statistic is calculated in the code in the link below for the data outlined in Table 1 above. Have a look at this code and manipulate the data and see what happens.</p>
<p><a href="https://drive.google.com/open?id=1qn8ZZagr1zA5K12I25SwBp9RPrkXi0eg">Click here to access the table on Google Colab</a></p>
<h4 id="so-lets-interpret-these-results">So let’s interpret these results:</h4>
<p>The Chi2-stat is basically the <script type="math/tex">\chi2</script> statistic in the above formula. The degrees of freedom is calculated by the following formula:</p>
<script type="math/tex; mode=display">(Rows-1).(Columns-1)=(4-1).(2-1)=3(Rows-1).(Columns-1)=(4-1).(2-1)=3</script>
<p>The p-value is really small and is &lt;0.05. This tells us that the difference between males and females is well beyond what we would expect to get due to random chance. We can, therefore, reject the null hypothesis and accept the alternative, that there is a significant difference between males and females.</p>
<h3 id="your-task">Your task:</h3>
<p>Many of you will use classifiers in your machine learning careers. 
I would like you to go online and do a bit of research - find out what classifiers are and ask yourself if you can use chi-square tests to do a simple exploratory analysis?</p>
<p>Post your reply in the comment section below.</p>
</div><p><h2>3.8&emsp;Analysis of variance (ANOVA)</h2><div class="u-typography-bold-intro">
<p>ANOVA stands for the analysis of variance. It is a test that is used throughout scientific research and there are many variations of it.</p>
<p>An ANOVA is generally used when we are comparing more than two groups for an outcome variable. In <a href="https://www.futurelearn.com/courses/intro-to-data-analytics/1/steps/661715">Step 3.4</a>, we compared the means from two samples taken from two distinct groups. We used a t-test to determine if we would accept or reject the null hypothesis. Null hypothesis was <em>H</em><sub>0</sub>:<script type="math/tex">~\mu a~=~\mu b</script> or <script type="math/tex">\mu a~-~\mu b~=~</script>0 (Average Salary is equal in both sectors). Alternative Hypothesis was <em>H</em><sub>1</sub>:<script type="math/tex">% <![CDATA[
~\mu a<>\mu b %]]></script> or <script type="math/tex">% <![CDATA[
\mu a~-~\mu b<> %]]></script>0 (Average Salary is not equal in both sectors).</p>
<p>With an ANOVA we are extending this principle at its simplest level to more than two groups. So now our questions would be ‘Is the mean of each group the same?’ and our null hypothesis would be:</p>
<p><em>H</em><sub>0</sub>:<script type="math/tex">~\mu a~=~\mu b=\mu c=\mu d</script></p>
<p>and the alternative implies that at least one mean is different from other groups:</p>
<p><em>H</em><sub>0</sub>:<script type="math/tex">% <![CDATA[
~\mu a<>\mu b<>\mu c<>\mu d %]]></script></p>
<p>This is a one-factor analysis, and it can be extended to multi-factor studies with covariate variables, but more on this when we talk about Generalised Linear Models (GLM).</p>
<p>The ANOVA Assumptions are as follows:</p>
<ul>
<li>Residuals (experimental error) are normally distributed (Shapiro Wilks Test).</li>
<li>Homogeneity of variances (variances are equal between treatment groups) (Levene or Bartlett Test).</li>
<li>Observations are sampled independently from each other.</li>
</ul>
<h4 id="anova-test-example">ANOVA test Example</h4>
<p>Note: In ANOVA, group, factors, and independent variables are similar terms.</p>
<h4 id="google-colab-task">Google Colab task</h4>
<p>Let’s look at a dataset which gives medical data for four different groups of patients. We are going to use Google Colab to examine if there is a difference between the 4 groups of patients. Each group has been given a particular cocktail of drugs. Follow the <a href="https://drive.google.com/open?id=1TmKDcj8YR_Q9CjO02YTwdBImBSDRsiSv">Google Colab file</a> and you should get a good feel for the analysis required to complete an analysis of variance. Notice how we examine the assumptions? Make sure you are familiar with them as they can induce bias if they are not meet.</p>
<p>When you finish the activity on Google Colab, come back to the step and make sure to share your reflection on the task.</p>
<p>We have examined how we would compare more than 4 independent groups using an ANOVA across one variable/factor.</p>
<p>Can you find out if this can be done for more than 1 factor? Post your reply in the comments section below.</p>
</div><p><h2>3.9&emsp;Regression modelling - linear regression</h2><div class="u-typography-bold-intro">
<p>In this section, we will introduce the concept of regression models, with a particular focus on linear and logistic regression.</p>
<p>We will get into much more detail about regression modelling when we get to explore the Generalised Linear Models in the final course of the Data Analytics and Data Mining series.</p>
<p>Throughout this section, we are going to use simple Python examples to show you how to conduct basic regression analysis, so every so often you will be directed to use Google Colab files.</p>
<h4 id="linear-regression">Linear Regression</h4>
<p>In many business research situations, the path to decision-making lies in understanding the relationship between two or more variables. For example, can the price of airline stock be predicted using a variable like the cost of oil? Or how strong a relationship is there between a company’s sales figures and their advertising budget?</p>
<p>Regression Analysis is the process of constructing a mathematical model or function that can be used to predict one variable using another variable (simple linear regression) or variables (multiple linear regression). The basic idea is to create a line of best fit in the data, as shown in the graph below:</p>
<p><img alt="a mathematical model that shows the prediction of using one variable to find the other variable. On left hand side it reads “No. of booking (Y)” and on the (X) axis it reads “No. of Adverts”. The Y axis counts up “60”, “70”, “80”, “90” and “100”. And the X axis counts from “20”,”22”,”24”,”26”,”28”,”30”,”32” across “”34”." sizes="(min-width: 1695px) 729px, (min-width: 680px) 648px, 320px" src="https://ugc.futurelearn.com/uploads/assets/65/cb/hero_65cbac69-d04a-405f-8a85-feffddf13094.png" srcset="https://ugc.futurelearn.com/uploads/assets/65/cb/small_hero_65cbac69-d04a-405f-8a85-feffddf13094.png 320w, https://ugc.futurelearn.com/uploads/assets/65/cb/hero_65cbac69-d04a-405f-8a85-feffddf13094.png 648w, https://ugc.futurelearn.com/uploads/assets/65/cb/large_hero_65cbac69-d04a-405f-8a85-feffddf13094.png 729w, https://ugc.futurelearn.com/uploads/assets/65/cb/large_hero_65cbac69-d04a-405f-8a85-feffddf13094.png 2x"/></p>
<p>By linear, we mean that we have a function such as:</p>
<ul>
<li>
<script type="math/tex; mode=display">y_i=a+bx_i+e_i~where~i=1~to~n</script>
</li>
<li><script type="math/tex">y_i</script> is known as the dependent variable and is generally the variable we are trying to predict and <script type="math/tex">i</script> is the row number in the dataset.</li>
<li><script type="math/tex">x_i</script> is the independent variable and is the one we are using as a predictor variable.</li>
<li><script type="math/tex">a</script> is the intercept and is the expected mean when <script type="math/tex">x_i</script> is zero. <script type="math/tex">b</script> is the slope or the rate of change in <script type="math/tex">y</script> for a unit increase in <script type="math/tex">x</script>.</li>
<li><script type="math/tex">e_i</script> is <strong>independently normally distributed with a mean of 0 and a constant variance</strong>.</li>
</ul>
<p>The last point is extremely important in regression analysis and should not be forgotten.</p>
<p>Linear Regression can be considered as one of the most basic forms of machine learning or artificial intelligence. It uses a method called maximum likelihood estimation to come up with the most appropriate values of a and b. It does, however, assume that the error term is independently normally distributed with a constant variance.</p>
<p>From a data mining position, we will use it in a number of ways and they are as follows:</p>
<ul>
<li>Predict missing values.</li>
<li>Identify important features in a dataset.</li>
<li>Understand if there are hidden features.</li>
</ul>
<p>The wonderful thing about linear regression is that we can easily understand the impact independent variables have on the outcome or dependent variable. The drawback is that things can go very wrong if we break the assumptions such as normality of the error or the consistency of the error variance. These are important issues and should not be overlooked.</p>
<p>It is also worth framing in your mind the idea of <script type="math/tex">y_i=a+bx_i+e_i</script> as a basis for all prediction problems. I appreciate we cannot predict all variables with one input or independent variable or the assumption that the functional structure is linear, but when you try to solve a problem it is always good to try and ask yourself what are the output/dependent and input/independent variables.</p>
<h3 id="google-colab-task---fedbus-example">Google Colab Task - Fedbus Example</h3>
<p>Fedbus have been using radio advertisements to advertise their weekend excursions for the past 8 months. The managing director has asked the sales team to assess the effect of these ads on the number of bookings.</p>
<p>The example data for this example can be found <a href="https://drive.google.com/open?id=1w4mpBr7uQHi4sy-6InCdUCPfZgILijEGPUwOZzuCPD4">here</a>.</p>
<p><strong>Note</strong></p>
<p>Follow the instructions in the <a href="https://drive.google.com/open?id=1fqsmTZ2YDqX_8uOB7CgiZdVPL1ueDSZz">Google Colab</a> and you will understand how to read the data from the example data. You will also be able to complete a basic regression analysis to understand it effects booking numbers. <strong>You will need to save the data to your own Google Drive</strong>.</p>
<p>If that proves difficult, you might find <a href="https://towardsdatascience.com/3-ways-to-load-csv-files-into-colab-7c14fcbdcb92">this post</a> on www.towardsdatascience.com helpful. It explains <a href="https://towardsdatascience.com/3-ways-to-load-csv-files-into-colab-7c14fcbdcb92">three ways to load CSV files into Colab</a>.</p>
</div><p><h2>3.10&emsp;Regression exercise </h2><div class="u-typography-bold-intro">
<p>It is time for some hands-on activity.  In this step, I have a real-life example and data that I would like you to play with.</p>
<p>A survey on the prestige of Canadian citizens concerning their occupation can be found on the <a href="https://stat.ethz.ch/R-manual/R-devel/library/datasets/html/00Index.html">R-datasets</a>. The R-datasets is a library of datasets that are accessible to the R and Python programming languages. In this example, you will read an R dataset into Google Colabs using this <a href="https://drive.google.com/open?id=1ZICTACvKWPeVUDLLtR7_OnyVTj46AkSf">code</a>. Now use the linear regression from statsmodels to determine which variables have a potential impact on prestige.</p>
<h4 id="note">Note:</h4>
<ul>
<li>you will have to use more than one independent variable in your model.</li>
<li>for this exercise leave out the <strong><em>type</em></strong> and <strong><em>census</em></strong> variables.</li>
</ul>
<p>When you have completed your analysis, answer the questions <a href="https://www.futurelearn.com/courses/intro-to-data-analytics/1/steps/679738/quiz/introduction">in Step 3.11</a>. The solution to this exercise is available <a href="https://drive.google.com/open?id=18jli7qbLUDHTQqI43itokqXxyuNBfRif">here</a>.</p>
<p>This exercise is designed to help you understand a simple regression analysis. This is not part of the continuous assessment but I suggest you talk as a group and discuss if there was anything challenging.</p>
</div><p><h2>3.12&emsp;Regression exercise reflection</h2><div class="u-typography-bold-intro">
<p>In Step 3.10, I asked you to play with some real-life data and do a regression exercise.</p>
<p>I wonder how did you do? Can you let us know how your answers compare to those I gave at the end of the quiz in Step 3.11? Was there anything particularly challenging? If you have any comments or questions, you can post them in the comments section.</p>
<p>I also would like you to consider the following question:</p>
<ul>
<li>How would you interpret the results of the regression model?</li>
</ul>
<p>Share your suggestions in the comments section below.</p>
</div><p><h2>3.13&emsp;Regression modelling - logistic regression</h2><div class="u-typography-bold-intro">
<p>We have now recapped on inferential statistics, analysis of variance and linear regression. One of the other things that students always want to know is how do I model categorical variables, or how do I classify a piece of data.</p>
<p>Logistic regression is one of the simplest classifiers. I will give a quick introduction to it in this section.</p>
<p>The objective of basic logistic regression is to model binary outcomes. In linear regression, we modelled continuous outcomes such as sales or prices with independent variables. Logistic regression is used to model the probability of a categorical outcome/dependent variable. This variable will have two options which are either a 1 or a 0, and the logistic regression model will attempt to predict P(Y=1) as a function of X.</p>
<h4 id="the-logistic-regression-assumptions">The Logistic Regression Assumptions</h4>
<ul>
<li>Binary logistic regression requires the dependent variable to be binary.</li>
<li>For a binary regression, the factor level 1 of the dependent variable should represent the desired outcome.</li>
<li>Only meaningful variables should be included. The independent variables should be independent of each other. That is, the model should have little or no multicollinearity. Multicollinearity occurs when a linear combination of one or more independent variables is collinear with another variable. This characteristic has a habit of inflating the variance of the parameter estimate in question and can lead one to wrongly accept the null hypothesis, or that the variable is not significantly different from zero.</li>
<li>The independent variables are linearly related to the log odds.</li>
<li>Logistic regression requires quite large sample sizes.</li>
</ul>
<h4 id="example---google-colab-activity">Example - Google Colab activity</h4>
<p>Keeping the above assumptions in mind, let’s look at a dataset. In this dataset we want to determine which of the three independent variables (Graduate Management Admission Test score, GPA score and work experience)  affects the success rate of people applying for a specific position.</p>
<p>We will read the data into a dataframe called candidates and then conduct a multivariate logistic regression. Wow! That’s a mouth full. It’s pretty similar to the linear regression exercise you completed. The only difference is you have a binary class as a dependent variable, Open the <a href="https://drive.google.com/open?id=1J1VnpS58nDXiF1XPFPXXKw6xbUxhNxSj">Google Colab file</a>. Follow the steps and make sure you play with the models. Google Colab is very clear about the interpretation of the results.</p>
<p>Also, pay particular attention to the evaluation metrics in the analysis. These metrics are predominantly used when assessing classifiers. At the end of this step, you should be able to complete a simple classifier problem.</p>
</div><p><h2>3.14&emsp; Review of Topic 3</h2><div class="u-typography-bold-intro">
<p>Congratulations on completing the third topic in the Introduction to Data Analytics and Data Mining. Listen to the audio clip above, where Andrew discusses and highlights some issues covered under this topic.</p>
<p>This was the second part of the revision of statistics and probability where we focused on regression and analysis of variance.</p>
<p>If you completed all the steps and exercises in this topic, you should be able to:</p>
<ul>
<li>Explain the purpose of statistical tests.</li>
<li>List the most popular statistical tests.</li>
<li>Outline and discuss the process of statistical testing.</li>
<li>Discuss and compare tests such as Chi-Square test and ANOVA.</li>
<li>Define and discuss regression modelling.</li>
<li>Apply basic linear and logistic models.</li>
<li>Conduct process of hypothesis testing based on a working example.</li>
<li>Understand how to evaluate both linear regression and logistic regression models.</li>
</ul>
<p>Some of the important things to take away from these topics are the following:</p>
<ul>
<li>We can make inferences about a population from samples.</li>
<li>Linear and logistic regression are really useful tools when trying to understand the influence of multiple variables on an outcome variable.</li>
<li>Always examine the errors and model assumptions when you complete a regression analysis. They can tell you a lot about your proposed model and may, in fact, point you to more appropriate features.</li>
<li>Always experiment with your models. Don’t assume one model will do.</li>
</ul>
<p>By now you will have effectively built your first machine learning algorithms. In these examples, we have used them to get a better understanding of our potential features, but you could and will also use these techniques to make predictions.</p>
<p>I would like you to have a read about training, validation and test datasets in this <a href="https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets">article</a>. You should use these when building a predictive model.</p>
<p>Next, you are going to get an opportunity to apply your knowledge. In Topic 4, you will be working with data to solve a real-life problem. For this problem I only want you to use training and test datasets and ignore validation sets as there is only a small volume of data.</p>
</div><p><h2>3.15&emsp;Let's move on to Topic 4 </h2><div class="u-typography-bold-intro">
<p>Congratulations on completing Topic 3!</p>
<p>Follow this link to move on to <a href="https://www.futurelearn.com/courses/intro-to-data-analytics/1/steps/661800">Topic 4: Bring it all together</a>.</p>
</div><p style="page-break-before: always"><h2>4.1&emsp;Welcome to Topic 4</h2><div class="u-typography-bold-intro">
<p>Welcome to the last part of the Introduction to Data Analytics and Data Mining course.</p>
<p>In this section, you will get an opportunity to apply what you learnt in the previous topics.  You will use the skills and knowledge you acquired in two previous sections to attempt to solve a problem with some data that we provide.</p>
<p>This is the applied part of the course, where we expect you to come up with a simple solution to solve the problem given.</p>
<p>The problem is known as the Iris problem and was described by <a href="https://en.wikipedia.org/wiki/Ronald_Fisher">Ronald Fisher</a>, who was a very famous statistician and biologist in the 1930’s. We will give you a dataset of 50 samples of 3 species of Iris (Iris setosa, Iris virginica and Iris versicolor). As part of the dataset, you will be given 4 features. We want you to use the tools that you have been shown to date and attempt to come up with a way of identifying each species. Don’t worry about getting this wrong as it is just an exercise and we will review a suggested solution in the session at the end of the course.</p>
</div><p><h2>4.2&emsp;Project overview:  ‘the iris problem’</h2><div class="u-typography-bold-intro">
<p>This is the applied part of the course where we expect you to come up with a model that predicts the type of flower based on the given length and width of petal and sepal.</p>
<h4 id="project-overview">Project overview:</h4>
<ul>
<li>Botanists require you to help them design an algorithm to identify flower types from 4 key measurements. To do this, they need to identify appropriate variables.</li>
<li>The data is stored in iris.csv <a href="https://drive.google.com/open?id=1_7ttss_S-I4mmLqWoMjOvwddWfAvAx5Q">here</a>.</li>
<li>The image below shows where the various variables from the flower are measured.  All measurements are in centimetres.</li>
<li>Give some univariate statistics to describe the data.
What should you do with the missing data?</li>
<li>Which variables should you include in your analysis? Justify this.</li>
<li>Can you come up with a rules-based algorithm to help the botanists?</li>
<li>Don’t spend more than four hours on this project.</li>
</ul>
<p><img alt="Diagram of flower showing its various parts" sizes="(min-width: 1695px) 729px, (min-width: 680px) 648px, 320px" src="https://ugc.futurelearn.com/uploads/assets/76/46/hero_76460757-924a-43ab-b07e-66206730efcf.png" srcset="https://ugc.futurelearn.com/uploads/assets/76/46/small_hero_76460757-924a-43ab-b07e-66206730efcf.png 320w, https://ugc.futurelearn.com/uploads/assets/76/46/hero_76460757-924a-43ab-b07e-66206730efcf.png 648w, https://ugc.futurelearn.com/uploads/assets/76/46/large_hero_76460757-924a-43ab-b07e-66206730efcf.png 729w, https://ugc.futurelearn.com/uploads/assets/76/46/large_hero_76460757-924a-43ab-b07e-66206730efcf.png 2x"/></p>
<p>You will need to do a little research on Python utilities to make this work. Pandas group by function will be useful.</p>
<h4 id="project-requirements-and-submission">Project requirements and submission:</h4>
<p>A maximum of a three-page report outlining the following:</p>
<ul>
<li>The problem you are trying to solve.</li>
<li>An analysis of the data using one of the methods given in this course. You do not need to implement any other method, and you will not receive extra marks for doing so. In your analysis state clearly, the assumptions you made and why you settled on your final model.</li>
<li>Implement a train/test procedure.</li>
</ul>
<p>Write this up in the report and make sure you give a conclusions section.</p>
<p>If you have any questions regarding this assignment please post them in the comments section below.</p>
<p><strong>Submit your work in a PDF format on <a href="https://loop.dcu.ie/theme/dcu/layout/altlogin.php">LOOP</a> by 10.02.2020.</strong></p>
</div><p><h2>4.3&emsp;Project submission </h2><div class="u-typography-bold-intro">
<p>This step has been included simply as a reminder of the deadline for your project.</p>
<p>The project submission deadline is <strong>10.02 2020</strong>. 
Your results will be available on 24.02.2020</p>
<p>You can and submit your report in via <a href="https://loop.dcu.ie/theme/dcu/layout/altlogin.php">LOOP</a>.</p>
<p>Once you submit your project, mark this step as complete, and move to the next step.</p>
</div><p><h2>4.4&emsp;The Iris problem - project reflection</h2><div class="u-typography-bold-intro">
<p>Working in groups and learning from each other is really important. In this step, I would like you to learn from others.</p>
<p>In this step, I would like you to prepare a  3 - 4 slide presentation of our report and share the link to it in the comments section. When you upload the link to your own presentation, view and comment on at least two other presentations.</p>
<p>When you are commenting on other learners work, make sure to let them know what you have learnt from them!</p>
</div><p><h2>4.5&emsp;Review of Topic 4</h2><div class="u-typography-bold-intro">
<p>Congratulations on completing Introduction to Data Analytics and Data Mining.</p>
<p>This last part was about allowing you to apply what you learnt in previous topics and steps.</p>
<p>Having completed the applied project, you should be able to:</p>
<ul>
<li>Work with real data in business situations.</li>
<li>Choose the appropriate methodology for data analysis.</li>
<li>Describe data.</li>
<li>Deal with missing data.</li>
<li>Analyse data choosing appropriate variables.</li>
</ul>
<p>In the audio clip above, the Lead Educator discusses and highlights some issues covered under this topic.</p>
<p>Before we wrap up and move to the next course I would like to draw your attention to a few things. It is important to note that when you start any data analytics/data mining project that you use some of the basic techniques we showed you in topics 2 &amp; 3 of this MOOC. You should:</p>
<ul>
<li>
<p>Understand your research/business question properly.</p>
</li>
<li>
<p>Use regression analysis as a tool to determine the relationship between input or independent variables and the output or dependent variables.</p>
</li>
<li>
<p>Describe your data using simple descriptive statistics or graphical presentations.</p>
</li>
<li>
<p>Not be afraid to use pandas describe method to help you get a feel for your dataset. Describing your dataset can be incredibly insightful at the beginning of a project, and will help you make future decisions that will require you to test certain assumptions.</p>
</li>
</ul>
<p>Finally, remember that your data processing and data exploration is probably going to take 60-70% of your workload. So don’t forget to take enough time understanding your data.</p>
<p>If you have any questions or comments about the course or your applied project, please share them in the comments section.</p>
</div><p><h2>4.6&emsp;What's next?</h2><div class="u-typography-bold-intro">
<p>Congratulations on completing the Introduction to Data Analytics and Data Mining. I hope you enjoyed the course.</p>
<p>Having completed all the steps and tasks in this course, you should be able to:</p>
<ul>
<li>Define data analytics.</li>
<li>Explain the relationship between data analytics and statistics.</li>
<li>Discuss the history of data analytics and list the main milestones in its history.</li>
<li>Describe the analytics process.</li>
<li>Define probability and statistics.</li>
<li>List and explain probability laws.</li>
<li>Analyse basic linear and logistic models.</li>
<li>Conduct process of a hypothesis test.</li>
</ul>
<p>You should be ready to move on to the second course in this series: <a href="https://www.futurelearn.com/courses/pre-processing-data-and-feature-impact-calculation/1">Pre-processing data</a>.</p>
<p>Data cleaning and preprocessing accounts for over 70% of the time required to complete a data analytics project. In the next course, we will explore techniques used to handle missing data. You will learn how to detect anomalies for univariate and multivariate data sources, and how to start transforming and reducing datasets.</p>
<p>All these approaches are essential to the delivery of a successful project.</p>
</div><p style="page-break-before: always"><h2>1.1&emsp;Introduction to pre-processing data</h2><div class="u-typography-bold-intro">
<p>Welcome to Pre-processing Data, the second course in the Data Analytics and Data Mining program provided by Dublin City University.</p>
<p>In the video above the Lead Educator, Dr Andrew McCarren of Dublin City University’s School of Computing provides learners with an overview of the four topics that will be covered in this course.</p>
<p>As mentioned previously, this course is part of the MSc in Computing (Artificial Intelligence) developed by Dublin City University, in collaboration with Technology Ireland ICT Skillnet and Technology Ireland Software Skillnet.</p>
<h3 id="course-syllabus">Course syllabus</h3>
<p>There are five courses in this series of courses that are covered in the following order:</p>
<ol>
<li><strong><a href="https://www.futurelearn.com/courses/intro-to-data-analytics/1">Introduction to Data Analytics</a></strong> provided an overview of the basics of data analytics and data mining along with their relationship to statistics. During this course, you will be introduced to probability, regression and analysis of variance along with practical examples of their use.</li>
<li><strong>Pre-processing data and feature impact calculation</strong> explores techniques used to handle missing data, to detect anomalies for univariate and multivariate data sources, to transform and reduce datasets.</li>
<li><strong>Feature engineering</strong> explores and implements techniques that can help reduce the dimensionality of a dataset, and cluster datasets using unsupervised machine learning techniques.</li>
<li><strong>Introduction to processing unstructured data</strong> introduces learners to several approaches that allow them to incorporate images, text and graph data sources into data analysis.</li>
<li><strong>Point estimation and feature impact calculation</strong> examines the Generalised Linear Models and two methods of parameter estimation.</li>
</ol>
<p>Whether you are learning for your own personal interest or to obtain credit towards a degree, we recommend that you take these courses in chronological order as they appear above.</p>
<h3 id="pre-processing-data-and-feature-impact-calculation---course-structure">Pre-Processing Data and Feature Impact Calculation - Course Structure</h3>
<p>There are four topics in this course. This course has the following structure:</p>
<ul>
<li>In <strong>Topic 1</strong>, we will focus on data collection techniques such as web scraping, the use of API and extraction from data warehouses.</li>
<li><strong>Topic 2</strong> will introduce data pre-processing or data cleaning. The reasons for data pre-processing and its importance to data analytics projects will be illustrated under this topic. In particular, attention will be paid to the treatment of missing values.</li>
<li><strong>Topic 3</strong> builds directly on Topic 2 by expanding our discussion of data cleaning. Specifically, the topic will highlight a number of data cleaning techniques for dealing with noisy data. The topic will also show how to identify and handle outliers.</li>
<li><strong>Topic 4</strong> looks at data transformation, discretization and data reduction. These techniques can be used to reduce complete datasets and transform continuous variables to categorical variables while maintaining the information held within the feature. Additionally, the topic also explores the difference between principal components and Lasso regression. Finally, Topic 4 will illustrate how entropy can be used in the data discretization process.</li>
</ul>
<p><img alt="Statement of funding from Skillnet Ireland informing that: This course has been grant-aided by skillnet Ireland. Technology Ireland ICT Skillnet provide advanced training and development activities for technical and engineering staff in the ICT. Participants may be contacted by Skillnet Ireland to confirm that thy attend this course.  
Information from susan.kelly@ictskillnet.com" sizes="(min-width: 1695px) 729px, (min-width: 680px) 648px, 320px" src="https://ugc.futurelearn.com/uploads/assets/14/0b/hero_140bfdc0-2f0f-408d-bf43-4ed1e4240646.jpg" srcset="https://ugc.futurelearn.com/uploads/assets/14/0b/small_hero_140bfdc0-2f0f-408d-bf43-4ed1e4240646.jpg 320w, https://ugc.futurelearn.com/uploads/assets/14/0b/hero_140bfdc0-2f0f-408d-bf43-4ed1e4240646.jpg 648w, https://ugc.futurelearn.com/uploads/assets/14/0b/large_hero_140bfdc0-2f0f-408d-bf43-4ed1e4240646.jpg 729w, https://ugc.futurelearn.com/uploads/assets/14/0b/large_hero_140bfdc0-2f0f-408d-bf43-4ed1e4240646.jpg 2x"/></p>
<hr/>
<h5 id="copyright-disclaimer">Copyright Disclaimer</h5>
<p><sub>All images used on this course are copyright by <a href="https://www.shutterstock.com/">Shutterstock</a> unless otherwise stated. Dublin City University does not knowingly intend or attempt to offend or violate any copyright or intellectual property rights of any entity. If images posted here are in violation of copyright law, please contact us and we will gladly remove the offending images immediately.</sub></p>
</div><p><h2>1.2&emsp;Welcome to Topic 1</h2><div class="u-typography-bold-intro">
<p>Welcome to Topic 1 where we will introduce some concepts that are traditionally used to integrate data into your data analytics project.</p>
<p>Typically, in most science subjects we collect data to evaluate a hypothesis or answer a particular question we may have. The sources of data are now greater than ever, ranging from simple surveys to IoT devices that are collecting thousands of images in a short period of time. Our world has changed, now we, as individuals, continually collect data with devices such as smartphones, Fitbit or IoT devices. Collecting data is quite a challenge, as we now need to know how to interact with the various data streaming sources. Having conquered data collection, one is then generally faced with the challenge of integration and this is followed by data cleaning and preprocessing.</p>
<p>In this topic, we will introduce web scraping, APIs and data warehousing. These concepts are usually good examples of where our data sources/repositories are regularly found in industry. We don’t intend this topic to be a complete view of any of these concepts but just a very light introduction that will allow you, if you are not familiar, to get somewhat comfortable with them.</p>
<p>When attempting a data analytics project practitioners typically underestimate the amount of time it is going to harvest and clean their data. You may hear me repeat this throughout the course. Don’t underestimate it. As a very good rule of thumb, I generally try to work out how much time it is going to take and then I double this figure.</p>
<p>Fundamentally, if your data is not correct you can try all the clever machine learning tools you like, but the results will still be disappointing. So remember this little phrase: “if you put rubbish in then you will get rubbish out”. Therefore it is really important to give your data collection and pre-processing the right amount of time.
In this topic we will cover the following steps:</p>
<ul>
<li>Data collection (why it is important to do it right)</li>
<li>Web scraping</li>
<li>Using APIs</li>
<li>ERP vs CRMs</li>
<li>Data Warehousing</li>
<li>Star Schemas (Data Warehouse Example)</li>
</ul>
<p>These are samples of data repositories that are used in industry. Remember to mark each step as complete when you have completed each step.</p>
</div><p><h2>1.3&emsp;Data collection</h2><div class="u-typography-bold-intro">
<p>In this step, I am going to discuss some concepts/ideas relating to data collection. I will then ask you to review a number of articles and answer a few questions. These questions are not part of the continuous assessment (CA), but will be really useful to drive a debate amongst yourselves.</p>
<p>Data collection has been continually used throughout history. Census data was extremely important to most imperial powers including the Chinese, British and Inca empires. The Egyptians even recorded their beer usage as far back as 3000 BC. We still conduct censuses today (in Ireland, we conduct one every five years, with the next one due in 2021). This data is very important for governments in order to help them determine strategies and national budgets. But private bodies also use this data to help them determine sampling strategies for marketing campaigns for example.</p>
<p>We all intuitively collect data when we see something. It might be someone’s face that we see, and we immediately make conclusions (rightly and wrongly) about this person. In this example, we are actually using a sample of information to make conclusions about the person in general. We also do this when we see groups of objects or people. We will collect some summary data and use this to make conclusions about the overall population of objects or subjects. Obviously, there is an inherent bias in simplified sampling strategies, and in 1895 the formal recognition of sampling techniques was born. This <a href="https://drive.google.com/file/d/1Puz2TNUIFxS-5mGEo74wUt51Goj8mD5J/view">article</a> by Jelke Bethlehem discusses the history of this process. In this paper, Bethlehem tells us about the use of the Central Limit Theorem. This theorem basically tells us that if we pick a random sample of 30 or more objects and repeat this process, the sample means will follow a normal distribution. The power this gives to an experimenter is vast and allows one to draw conclusions from a sample about a population. However, one must be careful to select the subjects correctly (this is no small task) in order to avoid bias and drawing the wrong conclusions. The data typically studied during these times predominantly originated from continuous variables. We have, however, advanced significantly in this regard. We now collect data from an enormous array of devices and in reality have a number of data types. As well as straight forward continuous numbers, we now have categories, text and images. They all in some way come back to numbers but the analyst will be faced with transforming and preprocessing these data sources in order to integrate them into their analysis. This <a href="https://www.talend.com/resources/what-is-data-integration/">article</a> from Talend.com offers a nice view of how and why we need to do data integration. As you may have guessed by now, data integration has developed into a science of its own as we have expanded our data requirements.</p>
<p>The introduction of IoT and vision technology has led us into collecting vast amounts of data. This is known as big data. This article from <a href="https://www.forbes.com/sites/kalevleetaru/2019/02/17/the-big-data-revolution-will-be-sampled-how-big-data-has-come-to-mean-small-sampled-data/#7a8ea8199ec6">Forbes</a> by Leetaru, tells us that sampling is still necessary as the cost of processing large batches of data is far greater than the storage cost. Leetaru states that the differential cost between storage and analysis has not really moved. You will also find that you will be faced with similar problems over the duration of this course. You can afford the storage but not the processing time. You will probably have to sample your data in order to build and process your models. Leetaru states that you need to be really careful with this strategy, but it can be useful. When faced with this problem in the past I have used a term called ‘bootstrapping’, which effectively selects samples of data, where each sample is processed and the results aggregated.</p>
<p>In my industry career, I have seen a number of projects make serious mistakes in the data collection process. In many of these cases, the investigator was unable to ask the question they wanted as the data collection process was flawed. It’s important to note that in medical research the data collection process is key when attempting to conduct drug trials and authorities put enormous emphasis on this during clinical trial submissions. You may find this in future projects, where the assessor or client is highly concerned with the data. This makes sense as we have already said, because wrongly collected data can bias your results and lead to wrong conclusions/forecasts. Some people believe that if they have petabytes of data they can predict anything. Not True. In fact, a correctly selected, small sample can have much better results than an ill-conceived, large sample.</p>
<p>Before, we finish up I would like to draw your attention to <a href="https://www.dcu.ie/researchsupport/rec_guidance.shtml">DCU’s ethics policy</a> regarding data collection. It is worth noting that you need to be really careful here. Remember, The General Data Protection Regulation (GDPR) is now an active requirement. It requires anyone who stores personal data to put the correct conditions in place to ensure this information is secure and is used in an ethical fashion. One must be extremely careful with images, people’s identities and any potential data that could identify someone. This could be a piece of data that identifies someone who drives a particular brand of car on a specific street. You may not give their name or house number, but if this car is unique to this street then the individual can be identified.</p>
<p>Finally, I would like to add one last statement: you will learn nothing if the data is incorrectly collected. Consider the following questions:</p>
<ul>
<li>Can you think of ways that data can be incorrectly collected? Try and post two or three in the comments section below.</li>
<li>How much planning should you put into your data collection?</li>
<li>Does the question you are trying to answer come before the data collection or is it the other way round? What should it be and what actually happens in practice?</li>
</ul>
<p>These are open questions and one could argue them either way. I have opinions on these. Once you have posted some answers in the comments section below I will share my thoughts with you.</p>
</div><p><h2>1.4&emsp;Introduction to web scraping</h2><div class="u-typography-bold-intro">
<p>You will now be provided with an overview of data scraping.</p>
<p>In the following article, “strata scratch” have outlined some of the principles and techniques used to extract data from the web using <a href="https://realpython.com/python-requests/">Python “requests”</a> and “<a href="https://pypi.org/project/beautifulsoup4/">beautiful soup</a>”. This is typically known as web scraping and is an incredibly powerful tool. There are other tools such as “<a href="https://docs.python.org/2/library/urllib2.html">urllib2</a>” and “<a href="https://selenium.dev/about/">Selenium</a>” that are also very useful, but for the purposes of this step we will focus mainly on “requests” and “beautiful soup”. “requests” helps us to extract data from a website in Python using get calls and “beautiful soup” is predominantly used to “prettify” our HTML data. Web data is full of extra text that can make it hard to extract the “real” data from the web page. When I discovered how to do this it opened up a whole new world for me. When you master these ideas the data sources that are available will expand exponentially and you will have entered the world of Big Data.</p>
<p>Go to the following Google Colab to see how you fare: <a href="https://drive.google.com/open?id=1ouoaHAAJl9jZLlZyd3qSrEi8XW7iqH7K">Web scraping guide</a>.</p>
</div><p><h2>1.5&emsp;API</h2><div class="u-typography-bold-intro">
<p>API stand for Application Programming Interfaces.</p>
<p>The advent of these interfaces into government or company databases has resulted in considerable advancements in data accessibility for the common analyst. As an individual interested in the food sector, I have managed to get enormous access to food movements through the use of API’s. In fact, they have made data harvesting considerably easier. They can also be used as programming interfaces. For example, you can use the <a href="https://developers.google.com/fit/rest/v1/get-started">Google rest API</a> to interface your code with Google Drive. This allows you to expand the functionality of your programs greatly. The best way to think of them is that they give your little PC the power of the cloud to complete some action. The great thing is you don’t have to use the same programming language as the host machine. In fact, every time you use your browser you are actually using an API. The following example pulls data from the USDA (United States Department of Agriculture). You should go to this <a href="https://fdc.nal.usda.gov/api-key-signup.html">link</a> to obtain a key.</p>
<p>Go to the following Google Colab and see how you fare: <a href="https://drive.google.com/open?id=1bzZjKYvkNycI8ogf6bFPyU-4IkOczJMW">Using API</a>.</p>
</div><p><h2>1.6&emsp;A simple comparison of CRM and ERP</h2><div class="u-typography-bold-intro">
<p>In this step, an overview of CRM and ERP is provided.</p>
<p><a href="https://en.wikipedia.org/wiki/Customer_relationship_management">Customer relationship management (CRM)</a> and <a href="https://en.wikipedia.org/wiki/Enterprise_resource_planning">enterprise resource planning (ERP)</a> programs are incredibly useful when attempting to extract data on business performance. Go to the following <a href="https://simply-crm.com/blog/difference-between-erp-and-crm/">link</a> and study the content.</p>
<p>Once you have read this article try and answer the following questions:</p>
<ul>
<li>Which system is best to extract data about your customers’ future needs and behaviours?</li>
<li>What type of system is Salesforce?</li>
<li>Is there an API for Salesforce?</li>
</ul>
<p>Share your answers in the comments section below.</p>
</div><p><h2>1.7&emsp;What is a data warehouse?</h2><div class="u-typography-bold-intro">
<p>Data warehouses are generally constructed to store vast amounts of integrated organisational data and are used to help these organisations create reports.</p>
<p>They are usually derived from production data. This <a href="https://www.computing.dcu.ie/~amccarren/mcm_papers/what-is-data-warehouse-2277-1891.1000117.pdf">article</a> outlines the use of a data warehouse and why we build them. Consider the following questions:</p>
<ul>
<li>Why would organisations need a data warehouse?</li>
<li>Have you used one before and what were the benefits over just storing the data in a single database?</li>
</ul>
<p>Share your thoughts with other learners in the comments section below.</p>
</div><p><h2>1.8&emsp;Star schema</h2><div class="u-typography-bold-intro">
<p>The following article was posted by Margaret Rouse on <a href="https://searchdatamanagement.techtarget.com/definition/star-schema">WhatIs.com</a>.</p>
<p>“In data warehousing and business intelligence (BI), a star schema is the simplest form of a dimensional model, in which data is organized into facts and dimensions. A fact is an event that is counted or measured, such as a sale or login. A dimension contains reference information about the fact, such as date, product, or customer. A star schema is diagrammed by surrounding each fact with its associated dimensions. The resulting diagram resembles a star.</p>
<p>Star schemas are optimized for querying large data sets and are used in data warehouses and data marts to support online analytical processing cubes, business intelligence and analytic applications, and ad hoc queries.</p>
<p>Within the data warehouse or data mart, a dimension table is associated with a fact table by using a foreign key relationship. The dimension table has a single primary key that uniquely identifies each member record (row). The fact table contains the primary key of each associated dimension table as a foreign key. Combined, these foreign keys form a multi-part composite primary key that uniquely identifies each member record in the fact table. The fact table also contains one or more numeric measures.</p>
<p>For example, a simple Sales fact with millions of individual clothing sale records might contain a Product Key, Promotion Key, Customer Key, and Date Key, along with Units Sold and Revenue measures. The Product dimension would hold reference information such as product name, description, size, and color. The Promotion dimension would hold information such as promotion name and price. The Customer dimension would hold information such as first and last name, birth date, gender, address, etc. The Date dimension would include calendar date, week of year, month, quarter, year, etc. This simple Sales fact will easily support queries such as “total revenue for all clothing products sold during the first quarter of the 2010” or “count of female customers who purchased 5 or more dresses in December 2009”.</p>
<p>The star schema supports rapid aggregations (such as count, sum, and average) of many fact records, and these aggregations can be easily filtered and grouped (“sliced &amp; diced”) by the dimensions. A star schema may be partially normalized (snowflaked), with related information stored in multiple related dimension tables, to support specific data warehousing needs.</p>
<p>Online analytical processing databases (data warehouses and data marts) use a denormalized star schema, with different but related information stored in one dimension table, to optimize queries against large data sets. A star schema may be partially normalized, with related information stored in multiple related dimension tables, to support specific data warehousing needs. In contrast, an online transaction processing (OLTP) database uses a normalized schema, with different but related information stored in separate, related tables to ensure transaction integrity and optimize processing of individual transactions.”</p>
<p>Having read the article above, consider the following question:</p>
<ul>
<li>Can you outline which tables are facts and which are dimensions?</li>
</ul>
<p><a href="https://ugc.futurelearn.com/uploads/assets/58/d0/58d06fca-7025-4367-972e-09a7d37ebc3e.png"><img alt="Four rectangle boxes leading into one central box. Central box titled Sales. Contains text: Product ID, Order ID, Customer ID, Employer ID, Total, Quantity, Discount.
Rectangle box to the top left of central box. Titled Time. Contains text: Order  ID, Order Date, Year, Quarter, Month. Rectangle box to the bottom left of central box. Titled Customer. Contains text: Customer ID, Customer Name, Address, City, Zip. Rectangle box to the top right of central box. Titled Product. Contains text: Product: Product ID, Product Name, Product Category, Unit Price. Rectangle box to the bottom right of central box. Titled Employee. Contains text: ID, Employee Name, Title, Department, Region." sizes="(min-width: 1695px) 729px, (min-width: 680px) 648px, 320px" src="https://ugc.futurelearn.com/uploads/assets/d1/46/hero_d1462e5e-4d4f-4d16-9f7a-80d327cb9ad3.png" srcset="https://ugc.futurelearn.com/uploads/assets/d1/46/small_hero_d1462e5e-4d4f-4d16-9f7a-80d327cb9ad3.png 320w, https://ugc.futurelearn.com/uploads/assets/d1/46/hero_d1462e5e-4d4f-4d16-9f7a-80d327cb9ad3.png 648w, https://ugc.futurelearn.com/uploads/assets/d1/46/large_hero_d1462e5e-4d4f-4d16-9f7a-80d327cb9ad3.png 729w, https://ugc.futurelearn.com/uploads/assets/d1/46/large_hero_d1462e5e-4d4f-4d16-9f7a-80d327cb9ad3.png 2x"/></a>
<sub>Source: Ideas Lab, Dublin City University</sub></p>
</div><p><h2>1.9&emsp;Data collection exercise</h2><div class="u-typography-bold-intro">
<p>The following <a href="https://lab.lmnixon.org/4th/worldcapitals.html">link</a> gives all the capital cities of the world along with their latitude and longitude.</p>
<p>Scrape the data from this site and use the International Space Agency API which you can get <a href="https://www.dataquest.io/blog/python-api-tutorial/">here</a> to get the next time it passes over each location.</p>
<p>What issues did you find? Did you get a result for each city? How would you get the next 5 times for each city?</p>
<p>Submit your answer in a Google Colab on loop.</p>
<p>The solution to this exercise is available <a href="https://drive.google.com/open?id=1M7hGxmW7KQsXQ5Cdy8Itj0wostoQ898J">here</a>.</p>
</div><p><h2>1.10&emsp;Review of Topic 1</h2><div class="u-typography-bold-intro">
<p>Congratulations on completing the first topic in Pre-processing Data.</p>
<p>This topic focused on data integration and illustrated the various different sources where one can extract data from. Additionally, a number of tools of data scraping tools were highlighted such as APIs.</p>
<p>Having completed this topic you should now be able to:</p>
<ul>
<li>Implement a simple web scraper.</li>
<li>Use an API to download data.</li>
<li>Understand the differences between ERP and CRM systems.</li>
<li>Understand what a data warehouse is and how a star schema can be used to implement one.</li>
</ul>
<p>If you have questions or comments regarding any of the content that we have covered under Topic 1 of the course please post them in the comments section below.</p>
</div><p><h2>1.11&emsp;Next Topic</h2><div class="u-typography-bold-intro">
<p>Well done on completing Topic 1!</p>
<p>Let’s move on to <a href="https://www.futurelearn.com/courses/pre-processing-data-and-feature-impact-calculation/1/steps/686882">Topic 2: Data Cleaning I</a>.</p>
</div><p style="page-break-before: always"><h2>2.1&emsp;Welcome to Topic 2</h2><div class="u-typography-bold-intro">
<p>In the video above the Lead Educator, Dr Andrew McCarren introduces the various subjects that will be covered in Topic 2 of Pre-processing data.</p>
<p>This topic will introduce data cleaning and the reasons for data cleaning. Its importance to data analytics projects will also be clearly illustrated. In particular, attention will be paid to the treatment of missing values.</p>
<p>On the completion of this topic, students will be able to:</p>
<ul>
<li>Describe the three principal mechanisms for the occurrence of missing data.</li>
<li>Implement a number of different data imputation techniques.</li>
<li>Outline the hazards of imputation.</li>
</ul>
<p>As always, we encourage you to participate in the comments sections and in the discussion steps included in the course in order to interact with your fellow learners. Remember to mark each step as complete as you progress through this topic.</p>
</div><p><h2>2.2&emsp;Why pre-process data?</h2><div class="u-typography-bold-intro">
<p>This step illustrates the reasons why we pre-process data.</p>
<p>Having collected your data from various streaming or Internet of Things (IoT) devices, you will probably want to start analysing or using it to make predictions. You may have collected different types of data and they can come from structured or unstructured sources. Structured data is:</p>
<ul>
<li>Loadable into “spreadsheets”</li>
<li>Arranged into rows and columns</li>
<li>Each filled or could be filled</li>
<li>Data mining friendly</li>
</ul>
<p>On the other hand, unstructured data will come from:</p>
<ul>
<li>Word, HTML, PDF documents or PPTs</li>
<li>Non-structured cells</li>
<li>Variable record lengths such as notes free form survey-answers</li>
<li>Text is relatively sparse, inconsistent and not uniform</li>
<li>Images, video, music etc…</li>
</ul>
<p>It is worth spending some time understanding your data and effectively pre-processing/cleaning it in order to give yourself the best chance of creating a useful model. As I said in the previous topic, if you put rubbish in you will get rubbish out. You will be faced with the following challenges:</p>
<ul>
<li>How do I treat missing values in my feature set?</li>
<li>How do I detect outliers and do I delete them from my dataset?</li>
<li>What do I do with noisy data and will it impact my results?</li>
<li>Can I reduce the volume of my data and still get comparable results?</li>
</ul>
<p>These are all typical questions that we are faced with when we have collected our data, and in order to address them, we will have to use some pre-processing techniques. In the next three topics, we will outline techniques that can be used to address the questions outlined above.</p>
<p>Follow this <a href="https://drive.google.com/open?id=1aIHQUZWzNxSFNMcIPmHbcBd5UA_7VENJ">link</a> to the Google Colab for this step.</p>
</div><p><h2>2.3&emsp;Missing Data</h2><div class="u-typography-bold-intro">
<p>Missing data can occur for a number of reasons such as; operator error, a faulty device or a respondent refusing to enter their data.</p>
<p>Missing data can be value attributes, entire records or entire sections. Typically, in any data analysis project, we will find that there are missing values in our data. Many people believe it is OK to just insert the average value of the non-missing values as their estimate for the missing values.</p>
<p>In the comments section below, discuss amongst yourselves why this is not such a good idea.</p>
<p>Frankly, this is the one thing that every newcomer to data analytics does. My suspicion is you will do the same. Let’s look at some data. In it, we have a small group of subjects who had their weight measured before and after a prescribed diet. Now you will see in the code in the Google Colab for this step that the average weight for both groups increases at the end of the trial.</p>
<p>Follow this <a href="https://drive.google.com/open?id=1TFnjHwYRRcVefCriUNw_oz1VHg8Url64">link</a> to go to the Google Colab for this step.</p>
</div><p><h2>2.4&emsp;Missing data mechanisms</h2><div class="u-typography-bold-intro">
<p>When we find missing data values in our data, they are usually caused by one of three mechanisms outlined by <a href="https://www.jstor.org/stable/2335739?seq=1#metadata_info_tab_contents">Rubin (1976)</a>.</p>
<p>It is very important that you understand them as they will have a major influence on how you treat your analysis. It will also determine if it is appropriate to impute missing values. The following <a href="https://stefvanbuuren.name/fimd/sec-MCAR.html">article</a> gives a nice explanation of the area. The three mechanisms are as follows:</p>
<ul>
<li>Missing completely at random (MCAR)</li>
<li>Missing at Random</li>
<li>Missing not at Random</li>
</ul>
<p>Each of these mechanisms will now be discussed in turn.</p>
<h3 id="missing-completely-at-random-mcar">Missing Completely at Random (MCAR)</h3>
<p>Missing Completely at Random is pretty straightforward. What it means is what it says: the propensity for a data point to be missing is completely random.
There’s no relationship between whether a data point is missing and any values in the data set, missing or observed.</p>
<p>The missing data are just a random subset of the data.</p>
<h3 id="missing-at-random-mar">Missing at Random (MAR)</h3>
<p>This is where the unfortunate names come in.</p>
<p>Missing at Random means the propensity for a data point to be missing is not related to the missing data, but it is related to some of the observed data.</p>
<p>Whether or not someone answered #13 on your survey has nothing to do with the missing values, but it does have to do with the values of some other variable.</p>
<p>A better name would actually be Missing Conditionally at Random because the missingness is conditional on another variable. But that’s not what Rubin originally picked, and it would really mess up the acronyms at this point.</p>
<p>The idea is, if we can control for this conditional variable, we can get a random subset.
There is another alternative to this case and it exists where questions that have not been asked could determine if people answer the question in hand.</p>
<p>You can imagine that good techniques for data that are missing at random need to incorporate variables that are related to the missingness.</p>
<h3 id="missing-not-at-random-mnar">Missing Not at Random (MNAR)</h3>
<p>Data are missing not at random (MNAR) when the missing values on a variable are related to the values of that variable itself, even after controlling for other variables. For example, when data are missing on IQ and only the people with low IQ values have missing observations for this variable. A problem with the MNAR mechanism is that it is impossible to verify that scores are MNAR without knowing the missing values.</p>
<ul>
<li>Can you think of examples of each mechanism?</li>
<li>If I have a variable that is not missing at random can I impute the missing values?</li>
</ul>
<p>Now I would like you to download the data from this <a href="https://drive.google.com/open?id=1Rp5Sqru6riXhceZoVcw17_TegO5P3Glss6Y8AUlaPm4">link</a>. Once you have done this, I would like you to analyse the data and record which variables have missing values. The following <a href="https://drive.google.com/open?id=1Id79rM0HSZmedfZta7LXFjCmxuLK1r1R">code in the Google Colab</a> for this step should get you going. Can you tell what type of missing process is occurring for the final exam mark?</p>
<p><strong>Reference:</strong></p>
<p>Rubin, D.B., 1976. <em>Inference and missing data</em>. Biometrika, 63(3), pp.581-592. Article available <a href="https://drive.google.com/open?id=1Id79rM0HSZmedfZta7LXFjCmxuLK1r1R">here</a></p>
</div><p><h2>2.5&emsp;Imputing missing data</h2><div class="u-typography-bold-intro">
<p>As we have said previously, many real-world datasets have missing values. For many analysts, the natural thing to do with these missing values is to use some imputation technique.</p>
<p>The appropriateness of using imputation is dependent on the individual problem. Imputation techniques can be used to avoid losing important information that would be lost if one value was missing from a large row. This makes sense as datasets can be drastically reduced if every row with one missing value is removed from the analysis. Most statistical and machine learning packages will remove a row if one value is missing and generally you won’t know. The problem with imputation is as follows:</p>
<ul>
<li>When do I use it?</li>
<li>What methods do I use?</li>
</ul>
<p>There are a number of packages such as Amelia in R, statsmodels.imputation.mice.MICEData or sklearn.impute in python that will complete the imputation for you. Also, there is a substantial number of techniques ranging from replacing missing values with means of that variable to data simulators or propensity score modelling. The technique you use in your imputation is very much dependent on the cause of your missing values. My view is that you should not impute in the following situations:</p>
<ul>
<li>When the variable in question is NMAR (unless you have evidence that this is a censored variable or you have <em>apriori</em> evidence on the distribution of the variable in question).</li>
<li>When there are missing values in the outcome variable.</li>
</ul>
<p>The reason for both these situations is that I generally follow the guiding principle that your imputation technique should not bias your results. I use missing value imputation as little as possible.</p>
<p>I will regularly start any analysis I do by modelling variables with missing values against a range of other variables in my dataset. So I code a new variable as Missing (Y/N) from the variable with a large number of missing values. I will then conduct a simple logistic regression on the remaining variables with the new Missing (Y/N) variable and examine if there are any relationships between Missing (Y/N) and the remaining variables.</p>
<p>Let’s look at a simple example I have drafted below, where one variable has a number of missing values. We have 3 variables which are being proposed to predict “happiness”. The first is a measure of educational attainment, second is work experience and the third is salary. If you follow my analysis below, you will see there is a strong relationship between the salary variable, educational attainment and work experience. In fact, the correlation scores are all around the 60% mark. Work experience is correlated with some of the other variables, so you could conclude that MAR is the most likely missing value mechanism but the missing values could also be missing due to MCAR or NMAR.</p>
<p>We cannot be sure here. However, I would initially conclude in this situation the problem is a MCAR or a MNAR, because when I do a logistic regression on the Missing Y/N that I created it is not significantly related to either of the other 2 input variables.</p>
<p>Now my choice here would be MCAR because there is no intuitive reason to tell you that people of one or other experience level would choose to leave their answer out.</p>
<p>Follow this <a href="https://drive.google.com/open?id=1tSh07V0-agHBJ2RVgdbRUdf4apEQLPfR">link</a> to go to the Google Colab for this step.</p>
</div><p><h2>2.6&emsp;Univariate approach to imputation</h2><div class="u-typography-bold-intro">
<p>The next three steps are taken from the <a href="https://scikit-learn.org/stable/modules/impute.html">skikit learn repository</a>.</p>
<p>The first of these is a simple tool to help you implement univariate imputation. Ski-learn’s SimpleImputer has options to use a constant, mean, median or mode for your missing values. It is important to visualise the data before you implement this process. For example, when you have normally distributed variables you should probably use the mean. If you have outliers or skewed data use the median or the mode. The code in the Google Colab file below gives a simple example of this. I would strongly recommend that you experiment with it so you can understand the implications of your choice.</p>
<p>Follow this <a href="https://drive.google.com/open?id=15ZFPAOkL_poOa0EH8ozgu0w2J2xPORq0">link</a> to go to the Google Colab for this step.</p>
</div><p><h2>2.7&emsp;Multivariate feature imputation</h2><div class="u-typography-bold-intro">
<p>Multivariate imputation effectively uses complete variables to predict missing values. Skikit-learn describe it as follows:</p>
<p>“IterativeImputer models each feature with missing values as a function of other features and uses that estimate for imputation. It does so in an iterated round-robin fashion: at each step, a feature column is designated as output y and the other feature columns are treated as inputs X. A regressor is fit on (X, y) for known y. Then, the regressor is used to predict the missing values of y. This is done for each feature in an iterative fashion and then is repeated for max_iter imputation rounds. The results of the final imputation round are returned.”</p>
<p>The estimates of The MICE package in R uses Multivariate Imputation by chained equations. The skikit-learn iterative imputer is inspired by MICE with the exception that it only produces one set of results.</p>
<p>If you go to the following <a href="https://scikit-learn.org/stable/auto_examples/impute/plot_missing_values.html#sphx-glr-auto-examples-impute-plot-missing-values-py">link</a> you will see a rather complicated approach to estimating missing values. The objective is to build a number of methods and the results are then combined/aggregated. This is very similar to a technique you will learn about called ‘random forest regression’. I like to think of this approach as getting a bunch of different experts into a room. Then they all put their opinion on a piece of paper and this is then aggregated to get the overall group opinion.</p>
<p>Follow this <a href="https://drive.google.com/open?id=1WwRhJ2rhdWg7AM13YvlxpoYPSKrZO7AT">link</a> to go to the Google Colab for this step.</p>
</div><p><h2>2.8&emsp;Nearest neighbour imputation</h2><div class="u-typography-bold-intro">
<p><a href="https://scikit-learn.org/stable/">Scikit-learn</a> also gives you the KNNimputer which is a multivariate solution that effectively works on the rows of data rather than the columns.</p>
<p>The KNN stands for K-nearest neighbours. The algorithm effectively finds the rows with non-missing values that are closest to the row and with the missing value. The values of the non-missing neighbours are then used to create an estimate of the missing value. There are various approaches to this estimate and Scikit-learn outlines it as follows:</p>
<p>“When the number of available neighbours is less than n_neighbors and there are no defined distances to the training set, the training set average for that feature is used during imputation. If there is at least one neighbor with a defined distance, the weighted or unweighted average of the remaining neighbors will be used during imputation. If a feature is always missing in training, it is removed during transform. For more information on the methodology.”<br/>
<sub><a href="https://github.com/scikit-learn/scikit-learn/blob/master/doc/modules/impute.rst#id39">Source</a></sub></p>
<p>Let’s read the dataset from the previous example and run the algorithm in the following <a href="https://drive.google.com/open?id=1l6mOyruMWoQ8Ew7ZaMjSv98Slx7DoPRT">Google Colab notebook</a>.</p>
</div><p><h2>2.9&emsp;The hazards of imputation</h2><div class="u-typography-bold-intro">
<p>The following article from <a href="https://towardsdatascience.com/missing-data-and-imputation-89e9889268c8">Towards Science</a> provides a really useful recap on how to handle missing data.</p>
<p>I have already mentioned the pitfall of potentially biasing your results when you use the wrong technique or make unreasonable assumptions regarding missing data. It is really worth keeping focused on examining your assumptions. I continually tell students that statistics and machine learning are about trying to prove a point. Think of it as a court of law, you continually need evidence to prove your point. When you handle missing data make sure you examine your assumptions. Write them down and then complete an analysis for each one. For example, if you do decide the variable with the missing data is an MCAR then prove that the estimate that you intend to use for the missing values is appropriate. So look at the spread of the data and see if it suits a constant, mean, median or mode. I have asked students during their MSc vivas what technique they used. Nine out of ten say they imputed them with the mean and usually have no evidence to support it. <strong>This is a huge No-No.</strong></p>
<p>The final point I will make is this; if you decide to impute, then run repeated analyses of your model with differing samples from your data. If you do this, the samples with imputed missing values should not stick out. If you were making a prediction model, do the test results change when you rely on missing data? You will come across this using K-folds neural networks in the future and this is also known as bootstrapping in statistics.</p>
<p>In the next step, we will move on to a small quiz.</p>
<p>Best of luck.</p>
</div><p><h2>2.11&emsp;Review of Topic 2</h2><div class="u-typography-bold-intro">
<p>Well done on completing the second topic in Pre-processing data.</p>
<p>This topic introduced learners to data cleaning. The topic illustrated that data cleaning is an essential part of any data analytics projects. During this topic, particular attention was paid to the treatment of missing values.</p>
<p>Now that you have completed this topic you should be able to:</p>
<ul>
<li>Describe the three principal mechanisms for the occurrence of missing data.</li>
<li>Implement a number of differing data imputation techniques.</li>
<li>Outline the hazards of imputation.</li>
</ul>
<p>As always, we encourage you to post any questions or comments that you may have regarding the content that has been covered under Topic 2 of the course.</p>
</div><p><h2>2.12&emsp;Next Topic</h2><div class="u-typography-bold-intro">
<p>Well done on completing Topic 2!</p>
<p>Let’s move on to <a href="https://www.futurelearn.com/courses/pre-processing-data-and-feature-impact-calculation/1/steps/687759">Topic 3: Data Cleaning 2</a>.</p>
</div><p style="page-break-before: always"><h2>3.1&emsp;Welcome to Topic 3</h2><div class="u-typography-bold-intro">
<p>Welcome to the third topic in Pre-processing data.</p>
<p>This topic builds directly on the previous topic by expanding our discussion of data cleaning. The topic will highlight a number of data cleaning techniques for dealing with noisy data. The topic will also show how to identify and handle outliers.</p>
<p>On completion of this topic, you will be equipped with the knowledge to:</p>
<ul>
<li>Identify univariate outliers and multivariate outliers.</li>
<li>Understand what heteroscedastic data is.</li>
<li>Describe Benford’s law.</li>
<li>Use discretization techniques to handle noisy data.</li>
<li>Understand Exponential Smoothing.</li>
</ul>
<p>We have included a variety of resources to help you get through the different subjects included in this topic. These include quizzes and code snippets to implement using Google Colabs. As usual, we encourage you to interact with your fellow learners in the various comments sections under different steps in the course.</p>
</div><p><h2>3.2&emsp;Data visualisation</h2><div class="u-typography-bold-intro">
<p>When we do most things in life, we use our senses to understand and feel things.</p>
<p>Our brain absorbs an enormous amount of information, breaks it down or categorises it into groups in order to make decisions. Data visualisation is the process of making a picture from a vast amount of information. Remember, a picture paints a thousand words, so try and decide how you will present your visualisations to your final audience. It can give us the clarity we would not normally get when looking at a vast amount of data or words. For example, we have all looked at a spreadsheet or a database and gotten lost in the data. Watch the following TED talk by David McCandless on data visualisation. It is a little long (18 minutes) but it is worth it. It is incredibly interesting and shows the power of visualisation.</p>
<p></p><div><div class="u-responsive-embed">

</div>
<p class="u-italic">
  This is an additional video, hosted on YouTube.
</p>
</div>
<p>I always tell students when completing their visualisations to be selective. Imagine you go to an art gallery or museum, such as the Lourve. We always enter these places with excitement (well, I do) and having walked around for a number of hours we get desensitised and can really miss the story of a wonderful painting. This is the same with visualisations in analytics. Sometimes we can have too many pictures. It is important to be selective and to try and get to the point as quickly as possible.
This <a href="https://towardsdatascience.com/introduction-to-data-visualization-in-python-89a54c97fbed">article</a> by Towards Science provides a really nice introduction to visualisation in Python. It works on the IRIS dataset and quickly demonstrates differing charts.
Before we go to the next step I would like you to think about why we visualise data. We do it to tell a story but also to highlight characteristics such as anomalies or outliers. Ask yourself what an outlier is and what you should do with them.</p>
<p>Share your thoughts in the comments section below.</p>
</div><p><h2>3.3&emsp;Univariate outlier detection </h2><div class="u-typography-bold-intro">
<p>This step explores univariate outlier detection.</p>
<p>In 1980, Hawkins described an outlier as “Observations which deviate so much from other observations as to arouse suspicion that they were generated by a different mechanism”. For me, this description is too simple and would lead one to conclude that these observations should be excluded or deleted from our data before we complete our analysis. In fact, my view of an outlier is a data point that seems to stand out from the rest of the pack. One should not immediately think this is an erroneous data point. It could, in fact, be a quirk of our data. For example, maybe after a certain period of time, our data was generated from a system that became unstable. While this is a change in process, it tells us that this can happen.</p>
<p>Sometimes, outliers represent people or subgroups that do strange or alternative things and could potentially tell us where future fashions or trends are going. They can perhaps help us to identify what is possible and can be a data miner’s gold and thus should not be excluded when identified. When you do identify an outlier, you should have strong evidence that the data point is erroneous before you delete or exclude it. The rule I use is this:</p>
<blockquote>
<p><em>A data point cannot be excluded unless there is scientific proof that the value you get is not possible.</em></p>
</blockquote>
<p>For example, if you were measuring heart rates of people and you found one with 1000bpm then this could be excluded from your analysis, as the fastest recorded heartbeat is <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3273956/">600bpm</a>. You will note here that I researched the topic and found a paper that supported my evidence. The other thing you may say is that the data does not follow normal practice. This may be the case but you can only exclude the data point if your study is defined to be on normal or regular subjects. Let’s look at another example. Imagine you were predicting house prices and most of them ranged between €150k to €1m, but you found one or two at €40m. This is quite possible and these data points cannot be excluded unless you are going to narrow your models to houses under €1m.</p>
<p>Let’s look at some data that you can get from scikit-learn. It’s known as the Boston housing data and can be imported using the code in this <a href="https://drive.google.com/open?id=1U7CJvEQE52vb9V1QXnOA8_sp2mnOCxri">Google Colab</a>.</p>
<p><strong>Reference:</strong></p>
<p>Hawkins, D.M., 1980. Monographs on Applied Probability and Statistics.  <em>Identification of Outliers</em>. Chapman and Hall, London – New York</p>
</div><p><h2>3.4&emsp;Multivariate outlier detection</h2><div class="u-typography-bold-intro">
<p>We are now going to look at multivariate outlier detection.</p>
<p>Multivariate outlier detection is the use of more than one variable to identify an outlier. In univariate outlier detection, we look for individual outliers in a single variable. These univariate outliers may not show up if there are unusual combinations between two or more variables. This is where multivariate outlier detection comes in. The same principles as mentioned in univariate outlier detection can be applied with multivariate outliers. In other words, don’t exclude particular rows of data unless you have the scientific evidence to support this.</p>
<p>There are numerous approaches to multivariate outlier detection, some of which are outlined below:</p>
<ul>
<li><strong>Bivariate charts</strong> are based on bivariate normal assumptions.</li>
<li><strong>Depth based control charts (nonparametric)</strong> map n-dimensional data to one dimension using, for example, Mahalanobis and build control charts for depth.</li>
<li><strong>Multiscale process control wavelets</strong> detect abnormalities at multiple scales as large wavelet coefficients. They are useful for data with heteroscedasticity. For example, they can be used in chemical process control.</li>
</ul>
<p>In this section, we will specifically focus on the depth-based control charts as it is a reasonably robust process and is quite quick to set-up. The multiscale process is beyond this material at this stage.</p>
<p>Follow this <a href="https://drive.google.com/open?id=1HavyI0iPtjDqvvx37EfESM0lYWl9q0gQ">link</a> to go to the Google Colab for this step.</p>
</div><p><h2>3.5&emsp;Heteroscedasticity and linear regression</h2><div class="u-typography-bold-intro">
<p>In earlier topics, we discussed the assumptions when building regression models.</p>
<p>The simplest version of a linear regression model is <script type="math/tex">Y_i= a +b_1 +\epsilon_i</script>. Where <script type="math/tex">\epsilon_i</script> is randomly and independently normally distributed around 0 with a constant variance. Heteroscedasticity occurs when the variation in the error terms is not constant. It can be caused when:</p>
<ul>
<li>There is a large range between the largest and smallest observed values (outliers).</li>
<li>The model is not correctly specified.</li>
<li>Observations are from mixed scales (Fahrenheit to Celcius).</li>
<li>Skewness in the distribution of the regressor variable (dependent).</li>
</ul>
<p>From the above list, we can see that we are assuming a linear model, but by using a linear model and examining the errors we can expose potential outliers in our data. I am not saying we have to stick with this model but using it initially can help you identify outlying observations. This <a href="https://www.geeksforgeeks.org/heteroscedasticity-in-regression-analysis/">article</a> will help you understand what heteroscedasticity is and how to calculate it. Have a look and use the code I gave you in the first course of Data Mining and Data Analytics in <a href="https://www.futurelearn.com/courses/intro-to-data-analytics/1/steps/661792">Step  3.9</a> to help you examine Boston housing data for outliers.</p>
</div><p><h2>3.6&emsp;Benford's Law</h2><div class="u-typography-bold-intro">
<p>Have you ever observed the first digit of a number? Does the digit 1 have the same probability as 9 to be a leading digit?</p>
<p>According to Benford’s law, a.k.a. the first digit law, the frequency of occurrence of the leading digits in naturally occurring numerical distributions is predictable and nonuniform but more close to a power-law distribution. In fact, a given number is six times more likely to start with a 1 than a 9! This is very illogical, as most people would expect a uniform distribution U(1,9) where all the digits have the same likelihood to show up in the first slot so they expect a probability of 1/9 % ~ 11,1%.</p>
<p>In 1881, Simon Newcomb noticed that the earlier pages (that started with 1) were much more worn than those at the back and that’s why the leading digits were much more likely to be smaller than large. Then, in 1938, physicist Frank Benford rediscovered the theorem of Newcomb by collecting over 20,000 samples of real-world numbers, using data sources as distinct as the measurements of rivers, street addresses, numbers contained in Reader’s Digest magazine, molecular weights, baseball statistics, death rates, and more. As Benford popularized this scientific discovery, he received all of the credit.</p>
<h3 id="benfords-law">Benford’s Law</h3>
<p><a href="https://en.wikipedia.org/wiki/Benford%27s_law">Benford’s Law</a> is an observation about the distribution of the frequencies of the first digits of the numbers in many different data sets. It is frequently found that the first digits are not uniformly distributed, but follow the logarithmic distribution:</p>
<script type="math/tex; mode=display">P(d) = Log_{10} (\frac{d+1}{d})</script>
<p>That is, numbers starting with 1 are more common than those starting with 2, and so on, with those starting with 9 being the least common. The probabilities are given in the table below:</p>
<table>
<tbody>
<tr>
<td>Digit</td>
<td>Probability</td>
</tr>
<tr>
<td>1</td>
<td>0.301</td>
</tr>
<tr>
<td>2</td>
<td>0.176</td>
</tr>
<tr>
<td>3</td>
<td>0.125</td>
</tr>
<tr>
<td>4</td>
<td>0.097</td>
</tr>
<tr>
<td>5</td>
<td>0.079</td>
</tr>
<tr>
<td>6</td>
<td>0.067</td>
</tr>
<tr>
<td>7</td>
<td>0.058</td>
</tr>
<tr>
<td>8</td>
<td>0.051</td>
</tr>
<tr>
<td>9</td>
<td>0.046</td>
</tr>
</tbody>
</table>
<p>Benford’s law can be applied to many situations and is particularly useful in fraud detection. I have downloaded the 2016 US election results for Illinois from <a href="https://public.opendatasoft.com/explore/dataset/usa-2016-presidential-election-by-county/export/?disjunctive.state">here</a>. However, I have edited the file (available below) to calculate the votes from the total votes multiplied by the percentages. In this analysis, we are going to examine the Republican votes for Illinois. This can be seen in the code in the following <a href="https://drive.google.com/open?id=17KaQ8HhgoVvsWJADiFdPXHIcIi1AhGMg">Google Colab file</a>.</p>
</div><p><h2>3.7&emsp;Noisy data</h2><div class="u-typography-bold-intro">
<p>Noise can generally be considered to be a random error or variance in a measured variable.</p>
<p>It can generally come from the following situations:</p>
<ul>
<li>Faulty data collection instruments.</li>
<li>Data entry problems.</li>
<li>Data transmission problems.</li>
<li>Technology limitations.</li>
<li>Inconsistency in a naming convention.</li>
</ul>
<p>Other data problems which require data cleaning are:</p>
<ul>
<li>Duplicate records.</li>
<li>Incomplete data.</li>
<li>Inconsistent data.</li>
</ul>
<p>Can you think of an example that gives us noisy data?</p>
<p>Well, you come across many examples of noisy data in our daily lives. We have all been affected by bad mobile signals or fuzzy images on our TVs. These can come from interference or just plain old inconsistent data collection.</p>
<p>So how do we handle noisy data?</p>
<p>There are numerous techniques and as usual, there is no exact answer. We need to make sure that we do not over-process our data to the extent that it will remove inherent trends or cyclical patterns, but if there is too much noise in our data then we will be unable to make realistic predictions. So in effect, we want to be able to process our data enough so that it is useful. In addition, we may find noise in our data that is caused by some outside force or variable that we have not measured. One should also be careful to differentiate between noise and outliers. Outliers are relatively rare occurrences and noise can be systemic. Many commentators online fail in my opinion to differentiate between the two and treat them with the same processes.</p>
<p>The following techniques are often used to handle noisy data:</p>
<ul>
<li>Binning methods: smooths by bin means, bin median or by bin boundaries.</li>
<li>Clustering: detects and removes outliers.</li>
<li>Combined computer and human inspection: detects suspicious values and is checked by a human.</li>
<li>Regression: smooths by fitting the data into regression functions.</li>
<li>Moving Average/Exponential Smoothing etc: smooths time series values to remove inherent noise.</li>
<li>Wavelet Analysis/Fourier Transforms: creates new variables from alternate frequencies.</li>
</ul>
<p>In the next two steps, we will look at some discretization methods and a time series techniques such as moving average and exponential smoothing. There are many smoothing techniques and they designed to detect trends in the presence of noisy data in cases in which the shape of the trend is unknown. However, before you try any of these approaches always examine the variables/features in question using simple graphs. They can tell you a lot about your data and can certainly in a time series case, tell if there is a strong noise to <a href="https://en.wikipedia.org/wiki/Signal-to-noise_ratio">signal ratio</a>. Finally, if you smooth a variable and use this variable as a predictor for your model then remember you are removing the variation from your analysis. This can also potentially introduce bias. This will have a potential impact on the range of outcomes. So you might find that you need to adapt your models to take account of this.</p>
<p>The following links will give you a feel for how much smoothing is viable and also some of the techniques required to do this:</p>
<ul>
<li><a href="https://rafalab.github.io/dsbook/smoothing.html">Why smooth?</a></li>
<li><a href="https://www.stat.cmu.edu/~cshalizi/uADA/12/lectures/ch04.pdf">How much should we smooth?</a></li>
</ul>
<p>Take a look at these links and try and get a feel for why we would smooth our data.</p>
</div><p><h2>3.8&emsp;Discretization techniques</h2><div class="u-typography-bold-intro">
<p>In the previous topic, we briefly mentioned a number of univariate discretization techniques.</p>
<p>These techniques can be used to categorize a continuous variable or simply to smooth a variable. We will briefly expand on a number of the discretization binning techniques in this topic.</p>
<h3 id="simple-discretization-methods-binning">Simple Discretization Methods: Binning</h3>
<p>The following binning techniques are the most simple. The first is where the bins are of equal size and the volume of data points differs for each bin. This can be very useful when trying to understand the density of your data. The second is where the bin sizes vary but the density of each bin remains the same. These can be useful in understanding possible outliers and the spread of your data.</p>
<ul>
<li><strong>Equal-width (distance) partitioning</strong> divides the range into N intervals of equal size: uniform grid. If A and B are the lowest and highest values of the attribute, the width of intervals will be: W = (B-A)/N. This is the most straightforward approach but outliers may dominate the presentation if skewed data is not handled well.</li>
<li><strong>Equal-depth (frequency) partitioning</strong> divides the range into N intervals, each containing approximately the same number of samples. This approach is good data scaling and managing categorical attributes can be tricky.</li>
</ul>
<p>The diagram below shows how they work and that they typically look like histograms. Neither of the above methods examines if there is any predictive power lost as we are not taking account of the effect of binning on the outcome variable.</p>
<p><img alt="A comparison of Equi-width binning and Eui-depth binning" sizes="(min-width: 1695px) 729px, (min-width: 680px) 648px, 320px" src="https://ugc.futurelearn.com/uploads/assets/48/1f/hero_481f8960-7e91-4b3a-9cb8-b30e311738b3.png" srcset="https://ugc.futurelearn.com/uploads/assets/48/1f/small_hero_481f8960-7e91-4b3a-9cb8-b30e311738b3.png 320w, https://ugc.futurelearn.com/uploads/assets/48/1f/hero_481f8960-7e91-4b3a-9cb8-b30e311738b3.png 648w, https://ugc.futurelearn.com/uploads/assets/48/1f/large_hero_481f8960-7e91-4b3a-9cb8-b30e311738b3.png 729w, https://ugc.futurelearn.com/uploads/assets/48/1f/large_hero_481f8960-7e91-4b3a-9cb8-b30e311738b3.png 2x"/>
<sub>(Source: Theodore Johnston, At&amp;T labs)</sub></p>
<h3 id="binning-methods-for-data-smoothing">Binning Methods for Data Smoothing</h3>
<p>In this approach, we are simply using the binning technique to smooth our data. So we use the bins to calculate new values that will effectively replace the old ones. Let’s see how we do it for the following data:</p>
<p>Sorted data for price (in dollars): 4, 8, 9, 15, 21, 21, 24, 25, 26, 28, 29, 34</p>
<p>Step 1 Partition into (equi-depth) bins:</p>
<ul>
<li>Bin 1: 4, 8, 9, 15</li>
<li>Bin 2: 21, 21, 24, 25</li>
<li>Bin 3: 26, 28, 29, 34</li>
</ul>
<p>Step 2 Smoothing by bin means (you could use the median):</p>
<ul>
<li>Bin 1: 9, 9, 9, 9</li>
<li>Bin 2: 23, 23, 23, 23</li>
<li>Bin 3: 29, 29, 29, 29</li>
</ul>
<p>or Smoothing by bin boundaries:</p>
<ul>
<li>Bin 1: 4, 4, 4, 15</li>
<li>Bin 2: 21, 21, 25, 25</li>
<li>Bin 3: 26, 26, 26, 34</li>
</ul>
<p>The following code example from <a href="https://www.geeksforgeeks.org/python-binning-method-for-data-smoothing/">GeeksforGeeks</a> does the above analysis for the IRIS dataset.</p>
<p>Try it out for the remainder of the columns in the IRIS dataset in the following <a href="https://drive.google.com/open?id=1SJ0b6bqakqMZvoBMkJRsOtr4w8ByYQvS">Google Colab file</a> .</p>
</div><p><h2>3.9&emsp;Moving average and exponential smoothing</h2><div class="u-typography-bold-intro">
<p><a href="https://en.wikipedia.org/wiki/Moving_average">Moving averages</a> are used and discussed quite commonly by technical analysts and traders alike.</p>
<p>If you’ve never heard of a moving average, it is likely you have at least seen one in practice. A moving average can help an analyst filter noise and create a smooth curve from an otherwise noisy curve. Mathematically, a moving average is a type of convolution and so it can be viewed as an example of a low-pass filter used in signal processing. If you are using them for predictions then generally you will lag them on historical data (Posey, 2019). However, Python also provides an option which allows you to smooth the data by centring on a point. So values either side of this point would be included in the moving average.</p>
<p>In order to calculate a moving average, we basically apply a window to your data which slides as time moves by. In each window/period(n), you calculate the average value and create a new series with the sliding/moving averages. The simplest form of moving average is the simple moving average (SMA)and the formula is as follows:</p>
<script type="math/tex; mode=display">SMA_i= \frac{X_{i-1}+X_{i-2}+..+X_{i-n}}{n}=\frac{\sum_{i-n}^i X_i}{n}</script>
<p>The centred moving average is as follows:</p>
<script type="math/tex; mode=display">SMAcentered_i= \frac{X_{i+(n/2)}+X_{i-1+ (n/2)}+..+X_{i-(n/2)}}{n}=\frac{\sum_{i-(n/2)}^{i+(n/2)} X_i}{n}</script>
<p>The following video explains moving average smoothing really nicely.</p>
<p></p><div><div class="u-responsive-embed">

</div>
<p class="u-italic">
  This is an additional video, hosted on YouTube.
</p>
</div>
<p>There are many other moving average techniques such as the <strong>Cumulative and Weighted Moving Average</strong>.</p>
<p>During previous steps in this topic, we mentioned how smoothing can be used to remove noise. There are many different approaches to this and each one has its own strengths and weaknesses. In this step, we will focus on two very common approaches; Simple Moving Averages and Exponential Smoothing.</p>
<p>Follow this <a href="https://drive.google.com/open?id=1k3rhVIqkoU5f-mtLxihatDvCZ3T2m6-n">link</a> to Google Colab for this step.</p>
<p><strong>Reference:</strong></p>
<p>Posey, L. (2019). <em>Implementing Moving Averages in Python</em>. You can access this article <a href="https://towardsdatascience.com/implementing-moving-averages-in-python-1ad28e636f9d">here</a>.</p>
</div><p><h2>3.11&emsp;Review of Topic 3</h2><div class="u-typography-bold-intro">
<p>Congratulations on completing Topic 3. We hope that you feel you are making good progress in your understanding of data cleaning so far.</p>
<p>We looked at a rich variety of subjects in this topic. Having completed it you should be getting to grips with:</p>
<ul>
<li>Identifying univariate outliers.</li>
<li>Identifying multivariate outliers.</li>
<li>Using control charts as example techniques to identify outliers.</li>
<li>Understanding what heteroscedastic data is.</li>
</ul>
<p>Additionally, you should now be able to</p>
<ul>
<li>Describe Benford’s law.</li>
<li>Use discretization techniques to handle noisy data.</li>
<li>Understand Exponential Smoothing.</li>
</ul>
<p>We would sincerely like to hear any questions or comments that you may have regarding any of the content that we have mentioned above. Please feel free to post them in the comments section below.</p>
</div><p><h2>3.12&emsp;Next topic</h2><div class="u-typography-bold-intro">
<p>Well done on completing Topic 3!</p>
<p>Let’s move on to the <a href="https://www.futurelearn.com/courses/pre-processing-data-and-feature-impact-calculation/1/steps/688342">final topic:  Data reduction and transformation</a>.</p>
</div><p style="page-break-before: always"><h2>4.1&emsp;Welcome to Topic 4</h2><div class="u-typography-bold-intro">
<p>Congratulations on making it to the final topic in Pre-processing data. We hope that you have enjoyed the course.</p>
<p>Topic 4 looks at data transformation, discretization and data reduction. These techniques can be used to reduce complete datasets and transform continuous variables to categorical variables while maintaining the information held within the feature. Additionally, the topic also explores dimensional reduction techniques such as principal components, ridge regression and lasso regression. Finally, Topic 4 will illustrate how entropy can be used in the data discretization process.</p>
<p>On completion of this topic, you will be able to:</p>
<ol>
<li>Transform data using a number of scaling techniques.</li>
<li>Discretize a continuous univariate variable.</li>
<li>Reduce a multivariate dataset using principal components, ridge regression and lasso regression.</li>
</ol>
<p>As you progress through this topic please make sure to mark each step as complete and to interact with other learners in the courses comments sections.</p>
</div><p><h2>4.2&emsp;Data transformation</h2><div class="u-typography-bold-intro">
<p>Data transformation is the application of a functional transformation or techniques that will transform or convert data to a new variable in order to help the analyst to meet model assumptions for a given algorithm or help summarize the data.</p>
<p>The following list helps to further explain the general areas where it can be applied:</p>
<ul>
<li>Smoothing: remove noise from data</li>
<li>Aggregation: summarization, data cube construction</li>
<li>Generalization: concept hierarchy climbing</li>
<li>Normalization: scaled to fall within a small, specified range, including min-max normalization, z-score normalization and normalization by decimal scaling.</li>
</ul>
<p>Analysts will transform data as a matter of convenience in order to reduce skewness and make the data easier to view. They will transform data to convert nonlinear relationships to linear relationships or to convert multiplicative relationships to additive relationships.</p>
<p>In the previous topics, we discussed the processes of smoothing and data aggregation. We will now discuss normalization, <a href="https://www.sciencedirect.com/topics/computer-science/concept-hierarchy">concept hierarchy</a> and the use of transformation techniques to reduce our datasets without loss of information.</p>
</div><p><h2>4.3&emsp;Data normalization in data mining</h2><div class="u-typography-bold-intro">
<p>Normalization is used to scale the data of an attribute so that it falls in a smaller range, such as -1.0 to 1.0 or 0.0 to 1.0.</p>
<p>It helps certain machine learning algorithms avoid bias in their predictions.</p>
<p>Many machine learning algorithms require the input attributes to be scaled as the cost function used to optimize the weights/parameters will be influenced by values from differing scales. <strong>It is not a prerequisite for all algorithms though, and should only be used when required. Currently, there is a fashion in the machine learning world to do it as a matter of course. Any transformation approach will reduce the amount of information arising from the variable in question.</strong></p>
<p>Situations where you may want to standardise are when:</p>
<ul>
<li>The variables are measuring different physical quantities.</li>
<li>The numeric values are on vastly different scales of magnitude.</li>
<li>There is no evidence that variables with high variation should be considered more important.</li>
</ul>
<p>Situations where you will not want to standardise are when:</p>
<ul>
<li>Variables are the same physical quantity and are roughly the same magnitude.</li>
<li>Standardised variables do not change between samples. It may be worthwhile excluding them</li>
<li>You have such physically related variables, your measurement noise may be roughly the same for all variables but the signal intensity varies much more, i.e., variables with low values have higher relative noise. Standardizing would blow up the noise. In other words, you may have to decide whether you want relative or absolute noise to be standardized.</li>
</ul>
<p>In this step, we will examine the following data normalization techniques:</p>
<ul>
<li>Min-Max Normalization</li>
<li>Z-Score Normalization</li>
<li>Normalization by Decimal Scaling</li>
</ul>
<p>You can find the Google Colab for this step <a href="https://drive.google.com/open?id=13JQ7BYJEC_6GWNyX2oq9f4GH4OUzLZ2p">here</a>.</p>
<p>Once you have finished going through the Colab file, consider the following question:</p>
<ul>
<li>Can you point out a drawback to where these scaling processes have a major weakness?</li>
</ul>
<p>Let us know in the comments section below.</p>
</div><p><h2>4.4&emsp;Data and numerosity reduction</h2><div class="u-typography-bold-intro">
<p>We are all aware of the volumes of data that are currently being generated by modern technology such as multi-media, IoT or social networks.</p>
<p>In real-life situations, we may have to deal with 50,000+ features. This can be very helpful but also challenges us to come with machinery and algorithms that can handle such scale. Sometimes, this can be similar to looking for a needle in a haystack when we are trying to extract insights from such vast datasets.</p>
<p>In addition to the above, many of the machine learning algorithms that are available today specifically work as classifiers. So to use such algorithms you may need to convert a continuous variable into a categorical one.</p>
<p>So are there ways to handle such problems?</p>
<p>Well, yes. In the next couple of steps, we will address these problems as two broad groups of problems. These are groups are:</p>
<ul>
<li>Numerosity reduction</li>
<li>Dimensionality reduction</li>
</ul>
<h3 id="numerosity-reduction">Numerosity reduction</h3>
<p>Numerosity reduction allows us to transform continuous variables into categorical variables. The approaches we will look at for this group are; concept hierarchies and discretization.</p>
<p>Concept hierarchies reduce the data by collecting and replacing low-level concepts (such as numeric values for the attribute age) with higher-level concepts (such as young, middle-aged or senior).</p>
<p>Discretization reduces the number of values for a given continuous attribute by dividing the range of the attribute into intervals. Interval labels can then be used to replace actual data values. We have covered some of the basics of this subject in previous topics but we will advance this to cover the concept of entropy and information gain.</p>
<h3 id="dimensionality-reduction">Dimensionality reduction</h3>
<p>The second group covers the concept of dimensionality reduction. This is a vast area which will be covered in much more detail in the next course: Feature Engineering. However, firstly, we will introduce a number of tools that you can use to get started with. They are:</p>
<ul>
<li>A light introduction to Principal Components</li>
<li>Lasso Regression</li>
<li>Ridge Regression</li>
</ul>
<p>Now let’s get started.</p>
</div><p><h2>4.5&emsp;Concept hierarchies</h2><div class="u-typography-bold-intro">
<p>This step explores concept hierarchies.</p>
<p>The following content comes from Han et al, (2011: 143-144).</p>
<p>“A concept hierarchy defines a sequence of mappings from a set of low-level concepts to higher-level, more general concepts. Consider a concept hierarchy for the dimension location. City values for location include Vancouver, Toronto, New York and Chicago. Each city, however, can be mapped to the province or state to which it belongs. For example, Vancouver can be mapped to British Columbia and Chicago to Illinois. The provinces and states can, in turn, be mapped to the country (e.g., Canada or the United States) to which they belong. These mappings form a concept hierarchy for the dimension location, mapping a set of low-level concepts (i.e., cities) to higher-level, more general concepts (i.e., countries). This concept hierarchy is illustrated in the figure below.</p>
<p><a href="https://ugc.futurelearn.com/uploads/assets/d8/77/d8775e46-bf9b-4736-b5e0-bc01ad434c23.png"><img alt="Concept hierarchy is visualized using a tree chart steeming with four levels: All country, province or state and city. From All two branches lead to Canada and USA respectively at the country level. Stemming from Canada two branches lead to British Columbia and Ontario at Province or state level. Stemming from British Columbia two branches lead to Vancouver and Victoria at city level. Stemming from Ontario two branches lead to Toronto and Ottawa at city level. Stemming from USA two branches lead to New York and Illinois at Province or state level. Stemming from New York two branches lead to New York and BUffalo at city level. Stemming from Illinois two branches lead to Chicago and Urbana." sizes="(min-width: 1695px) 729px, (min-width: 680px) 648px, 320px" src="https://ugc.futurelearn.com/uploads/assets/d8/77/hero_d8775e46-bf9b-4736-b5e0-bc01ad434c23.png" srcset="https://ugc.futurelearn.com/uploads/assets/d8/77/small_hero_d8775e46-bf9b-4736-b5e0-bc01ad434c23.png 320w, https://ugc.futurelearn.com/uploads/assets/d8/77/hero_d8775e46-bf9b-4736-b5e0-bc01ad434c23.png 648w, https://ugc.futurelearn.com/uploads/assets/d8/77/large_hero_d8775e46-bf9b-4736-b5e0-bc01ad434c23.png 729w, https://ugc.futurelearn.com/uploads/assets/d8/77/large_hero_d8775e46-bf9b-4736-b5e0-bc01ad434c23.png 2x"/></a><br/>
<sub>Figure 4.10(a)</sub></p>
<p>Many concept hierarchies are implicit within the database schema. For example, suppose that the dimension location is described by the attributes number, street, city, province_or_state, zip_code, and country. These attributes are related by a total order, forming a concept hierarchy such as “street &lt; city &lt; province_or_state &lt; country.” This hierarchy is shown in Figure 4.10(a). Alternatively, the attributes of a dimension may be organized in a partial order, forming a lattice. An example of a partial order for the time dimension based on the attributes day, week, month, quarter, and year is “day &lt;{month &lt; quarter; week} &lt; year.”<sup>1</sup></p>
<p>Concept hierarchies may also be defined by discretizing or grouping values for a given dimension or attribute, resulting in a set-grouping hierarchy. A total or partial order can be defined among groups of values. An example of a set-grouping hierarchy for a price dimension, where an interval (X…Y) denotes the range from X  (exclusive) to  Y (inclusive). This could be the conversion of the variable age to infant, teenager young adult or pensioner, defined by some predefined standard.”</p>
<p>Finally, in order to reduce data categories or numerosity, an examination of the data warehouse fact tables or the application of predefined standards can be used to categorize data.</p>
<p><strong>References:</strong></p>
<p>Han, J., Kamber, M. and Pei, J., 2011. Data mining concepts and techniques, Third Edition. The Morgan Kaufmann Series in Data Management Systems. Full book available <a href="http://myweb.sabanciuniv.edu/rdehkharghani/files/2016/02/The-Morgan-Kaufmann-Series-in-Data-Management-Systems-Jiawei-Han-Micheline-Kamber-Jian-Pei-Data-Mining.-Concepts-and-Techniques-3rd-Edition-Morgan-Kaufmann-2011.pdf">here</a>.</p>
</div><p><h2>4.6&emsp;Discretization continued</h2><div class="u-typography-bold-intro">
<p>In Step 3.11, we discussed binning techniques that could be used to discretize a feature.</p>
<p>These processes, while useful in understanding and pre-processing our data, can also be considered data numerosity reduction techniques. Generally, we have three types of attributes/features or variables. These are nominal variables, ordinal variables and continuous. These are each discussed in turn below.</p>
<h3 id="nominal-variables">Nominal Variables</h3>
<p>Nominal variables are values that come from an unordered set such as; Male or Female, and Yes or No. They may contain more complex values such as the names of several political parties (e.g. Conservative, Labour, Liberal, Scottish National Party, Green Party).</p>
<h3 id="ordinal-variables">Ordinal Variables</h3>
<p>Oridinal variable are values that come from an ordered set, such as; Very Unsatisfied – 1, Unsatisfied – 2, Neutral – 3, Satisfied – 4, Very Satisfied – 5.</p>
<h3 id="continuous">Continuous</h3>
<p>These are real numbers and are often described as interval scales or ratio scales. Have a look at the following <a href="https://www.questionpro.com/blog/ratio-scale/">link</a> for a more detailed explanation of ratio and interval variables.</p>
<p>As we said in Topic 3 of this MOOC, discretization allows us to divide a continuous variable into intervals/categories.</p>
<p>Further to the techniques we have looked at in previous steps in this course, we will look at a very simple rule natural partitioning approach known as the ‘3-4-5 Rule’ in the next step. Following this, we will look at an entropy-based discretization method which takes account of the output variable when deciding on how to discretize an input variable.</p>
<p>Please post any comments or questions that you may have in the comments section below.</p>
</div><p><h2>4.7&emsp;3-4-5 rule for attribute discretization</h2><div class="u-typography-bold-intro">
<p>The 3-4-5 rule can be used to segment numerical data into relatively uniform, “natural” intervals.</p>
<p>If an interval covers 3, 6, 7 or 9 distinct values at the most significant digit, partition the range into 3 equal-width intervals for 3,6,9 or 2-3-2 for 7.</p>
<p>If it covers 2, 4, or 8 distinct values at the most significant digit, partition the range into 4 equi-width intervals.</p>
<p>If it covers 1, 5, or 10 distinct values at the most significant digit, partition the range into 5 equi-width intervals</p>
<h3 id="example">Example:</h3>
<ol>
<li>Suppose that profit data values for the year 2017 for a company range from -351,976 to +4,70,00,896.</li>
<li>For the practical purpose of avoiding noise, extremely high or extremely low values are not considered. So first we need to smooth out our data. Let’s discard the bottom 5% and the top 5% values.</li>
<li>Now suppose after discarding top 5% and the bottom 5% the  new values for LOW and HIGH are {-159876,1838761} respectively.</li>
<li>The Most Significant Digit (MSD) is at the million position, see highlighted digit: –159876 and 1838761.</li>
<li>The next step is to round down LOW and round up HIGH to MSD that million position.</li>
<li>So LOW = -1000000 and HIGH = 2000000. -1000000 is nearest down million to -159876 and 2000000 is nearest up million to 1838761.</li>
<li>Now let’s identify the range of this interval.</li>
<li>Range = HIGH – LOW that is 2000000 – (-1000000) = 3000000. We consider only MSD here which is 3.</li>
<li>Now that we know range MSD = 3, we can apply rule #1.</li>
</ol>
<p>Rule #1 says that we can divide this interval into three equal-size intervals:</p>
<p>Interval 1 : -1000000 to 0 Interval 2 : 0 to 1000000 Interval 3 : 1000000 to 2000000</p>
<p>You should be thinking of how 0 can be part of multiple intervals? You’re right! We should represent it as follows:</p>
<p>Interval 1 : (-1000000 … 0] Interval 2 : (0 … 1000000] Interval 3 : (1000000 … 2000000] Here (a … b] denotes range that excludes a but includes b. ( , ] is notation for half-open interval.</p>
<p>The solution to this exercise will be posted in the comments section below at a later date.</p>
</div><p><h2>4.8&emsp;Entropy</h2><div class="u-typography-bold-intro">
<p>So far any of the methods we have used for data discretization have predominantly focused on the variable/attribute itself and not on the information it contributes to the outcome variable.</p>
<p>In order to advance what we have learnt before we need to do the following when discretizing a data series:</p>
<ul>
<li>Consider the information we get from the new categories.</li>
<li>Ask how can we get the right splits in a series that will give us the most information.</li>
</ul>
<p>Entropy refers to disorder or impurity. The decision tree classifier uses this measurement of information to determine if a split at a certain point improves the information gain obtained at that split or not. In effect, what we are trying to do is to make splits that will be most beneficial when categorising an attribute from a continuous variable. The formula for Entropy can be shown below:</p>
<p>Entropy: <script type="math/tex">Ent(D_1) = \sum_{i=1}^m p1_i.log_2(p1_i)</script></p>
<p>where $p_i$ is the probability of having outcome event i,  and <script type="math/tex">D_1</script> is the first category of the proposed attribute <script type="math/tex">D</script>.
If we do this for another category for <script type="math/tex">D_2</script> we can calculate the Entropy for <script type="math/tex">D_2</script> similar to that above:</p>
<script type="math/tex; mode=display">Ent(D_2) = \sum_{i=1}^m p2_i.log_2(p2_i)</script>
<p>So if we were proposing to split a continuous attribute <script type="math/tex">D</script> in two, we would determine the information that a proposed split would give with the following equation:</p>
<script type="math/tex; mode=display">info_A(D) =  \frac{\lvert D_1 \rvert}{\lvert D \rvert}Entropy(D_1) +\frac{\lvert D_2 \rvert}{\lvert D \rvert}Entropy(D_2)</script>
<p>where <script type="math/tex">D</script> is the total number of items and <script type="math/tex">D_1</script> and <script type="math/tex">D_2</script> are the number of items in each class for the attribute split in <script type="math/tex">D</script>. The challenge we face is to pick the split that gives the most information gain from our first split.</p>
<p>The following <a href="https://natmeurer.com/a-simple-guide-to-entropy-based-discretization/">link</a> from Natalie Meurer is a lovely description of how the information gain from a split is calculated. Natalie has also provided some really nice examples that should make it much clearer.</p>
<h3 id="splitting-an-attribute-in-practice">Splitting an Attribute in Practice</h3>
<p>One way to split an attribute in practice is to model the outcome with the attribute in question, using a decision tree algorithm. The example we saw in the above link can be run through the python Scikit Learn decision tree library, using the <a href="https://scikit-learn.org/stable/modules/tree.html#classification">C4.5 algorithm</a>. The <a href="http://www.learnbymarketing.com/481/decision-tree-flavors-gini-info-gain/">C4.5 algorithm is a child of the ID3</a> algorithm and they use information gain and entropy to optimize their cost functions. We will take the example from Natalie Meurer’s webpage and use the C4.5 decision tree to determine the optimum split. The example can be seen below.</p>
<table>
<tbody>
<tr>
<td>Hourse Studied</td>
<td>A on Test</td>
</tr>
<tr>
<td>4</td>
<td>N</td>
</tr>
<tr>
<td>5</td>
<td>Y</td>
</tr>
<tr>
<td>8</td>
<td>N</td>
</tr>
<tr>
<td>12</td>
<td>Y</td>
</tr>
<tr>
<td>15</td>
<td>Y</td>
</tr>
</tbody>
</table>
<p>The code to do this is really simple. We first specify the input variable (X), then the output variable (Y) and then implement the decision tree classifier. We then use the <script type="math/tex">tree.plot_tree</script> to present the tree. The tree shows where the optimum splits based on information gain. You should be able to see that the first split is at 10. This is equivalent to that found by Natalie Meurer. It then goes further until all tree nodes have a gini index of zero where you have complete information. A gini index of 0 means all the elements of the branch in question belong to a certain class. When all the leaves have a gini index of 0, your tree cannot grow anymore.</p>
<p>You can access the Google Colab for this step <a href="https://drive.google.com/open?id=1MBldCnQ0J0lSKkqgrrm4l5p_Yfq1ROfy">here</a>.</p>
</div><p><h2>4.9&emsp;Dimensional reduction techniques</h2><div class="u-typography-bold-intro">
<p>In a previous step in this topic, we mentioned the fact that the volume of data available to analysts has exploded over the last 20 years.</p>
<p>This now presents new problems with regard to what methods we use, which features we use and whether we need to select all the data points for our analysis, in order to get insightful results. Another issue that arises is whether or not it is necessary to collect all this data. As in many cases, there will be a substantial amount of missing data. We have addressed techniques that can be used with missing data. Thankfully, there are a number of techniques which can be used to reduce the size of our datasets in terms of the number of features and the volume of data, <strong>without</strong> losing too much information. In fact, you can sometimes improve the model’s predictive performance and will definitely reduce the computational time required.</p>
<p><strong>What is dimensional reduction?</strong></p>
<p>As data generation and collection keeps increasing, visualizing it and drawing inferences becomes more and more challenging. Many of us look at our data from the perspective of two or three-dimensional charts. When you have substantially more features, how can you extract insight from these charts? In fact, we can end up being quite confused, as one chart seems to show something but the other doesn’t. Also if we have petabytes of data, do we need to use all of it? The answer to these questions is to use data reduction through a number of techniques:</p>
<ol>
<li>Aggregation (first course, <a href="https://www.futurelearn.com/courses/intro-to-data-analytics/1/todo/63688">Topic 2</a>, Steps 3-10) and discretization (first course, <a href="https://www.futurelearn.com/courses/intro-to-data-analytics/1/todo/63733">Topic 3</a>, Step 11)</li>
<li>Sampling (row reduction).</li>
<li>Dimensionality reduction/Feature subset selection.</li>
<li>Feature creation/Attribute transformation (Covered in the next course: Feature Engineering).</li>
</ol>
<p>We have covered the use of aggregation using binning techniques or histograms, so we are now going to look at row reduction using sampling techniques and dimensionality reduction. In other words, we reduce either the number of rows we need to do our analysis or the number of features we need for our analysis. We will discuss feature creation in the next course: Feature Engineering.</p>
<p>Let’s have a look at row reduction or sampling first.</p>
<h3 id="sampling">Sampling</h3>
<p>Sampling has been the traditional approach when collecting data for social surveys. We are all familiar with the use of samples to determine elections or when market research companies are assessing people’s product preferences. If the sample is picked correctly we get a sub-dataset that reflects the original dataset and thus we can make predictions about the original dataset. There are many techniques that can be used to sample your data. These include simple random sampling, systematic sampling, stratified sampling and cluster sampling. Each of these will be discussed in turn next.</p>
<p><strong>Simple random sampling</strong></p>
<p>Simple random sampling is easy to implement as each row or individual in your dataset has an equal chance of being selected. It does not guarantee that every group (if they exist) in your data will be represented.</p>
<p><strong>Systematic sampling</strong></p>
<p>Systematic sampling is easier to use than simple random sampling as the selection is based on a regular interval (every 10th row or individual). It may cause bias if there are regular fluctuations in your data. It is not really appropriate for time series data.</p>
<p><strong>Stratified sampling</strong></p>
<p>Stratified sampling clusters data into a number of subgroups or categories and is a good way of truly representing the data and reducing bias. However, it requires a detailed knowledge of the data structure.</p>
<p><strong>Clustered sampling</strong></p>
<p>Clustered sampling is similar to stratified sampling. It can introduce bias if the cluster does not represent the characteristics of the data.</p>
<p>There are many non-probabilistic techniques ranging from convenience sampling, quota sampling, judgement sampling and snowball sampling.However, these techniques are predominantly used in social science surveying and would not be particularly relevant to data analytics projects. If you want to explore this more try this <a href="http://www.csun.edu/~hbsoc126/soc4/chapter_8_outline.pdf">link</a>.</p>
<p>When choosing a sampling technique try and pick one that reflects the structure of your data. So for example, in election samples practitioners will make sure there are individuals in all the relevant socio-economic groups. With Simple Random Sampling random sampling (SRS) there is no guarantee of this. The choice of technique should be carefully thought out.</p>
<h3 id="dimensionality-reduction">Dimensionality Reduction</h3>
<p>So let’s think about the second problem (feature/dimensionality reduction). In the past few steps, we discussed two very simple concepts known as correlation and multi-collinearity. Both of these have measurements such as Pearson’s correlation coefficient <script type="math/tex">\rho</script> and VIF which allow us to measure the linear relationship between variables. In large datasets, you will probably find a considerable amount of variables that are correlated in some way. For example, blood pressure and BMI are regularly correlated in health studies. For many data analysts, the first thing we all do is review the correlation matrix. If there are variables in the matrix with extremely high correlations &gt;0.9 then this would lead us to believe we will gain nothing by having both variables in our models.</p>
<p><strong>Just a side note here: we will lose model performance if we have highly correlated variables in our models.</strong></p>
<p>The next thing we might do would be to analyse the VIF results. If we had VIF results for specific variables &gt;10, then we might consider dropping this variable. The problem with these approaches is that we don’t know which variable to select.</p>
<p>There are a large number of dimensionality reduction techniques, and they all have their positives and negatives. According to <a href="https://thenewstack.io/3-new-techniques-for-data-dimensionality-reduction-in-machine-learning/">The NEWSTACK</a> the most commonly used techniques for data-dimensionality reduction are:</p>
<ol>
<li>Ratio of missing values</li>
<li>Low variance in the column values</li>
<li>High correlation between two columns</li>
<li>Principal component analysis (PCA)</li>
<li>Candidates and split columns in a random forest</li>
<li>Backward feature elimination</li>
<li>Forward feature construction</li>
<li>Linear discriminant analysis (LDA)</li>
<li>Neural autoencoder</li>
<li>t-distributed stochastic neighbour embedding (t-SNE)</li>
<li>Multiple Correspondence Analysis (MCA).</li>
</ol>
<p>We will discuss a large number of these in the Feature Engineering section in the third course in this program, Feature Engineering. Over this and the next 2 steps we are going to outline the following techniques:</p>
<ul>
<li>Principal component Analysis (regularly used)</li>
<li>Ridge regression (used as a reference for lasso regression)</li>
<li>Lasso regression (helps select a subset of covariates)</li>
</ul>
<p><strong>Principal Component Analysis</strong></p>
<p><a href="https://en.wikipedia.org/wiki/Principal_component_analysis">Principal component analysis</a> (PCA) is a dimensionality reduction technique that is used regularly in data analytics. It examines the linear relationship between variables and attempts to create new variables that are orthogonal (not correlated). By selecting a small subset of the orthogonal variables we can maintain 90%+ of the information of the original variables, thus reducing the dimensionality of the proposed dataset. Each new orthogonal variable is known as a component and they are generally ordered with regard to the amount of variation they explain. So, for example, the first component will explain the most variation within the dataset, the second will explain the second most amount of variation and so on. The big advantage with PCA is that it reduces the dimensionality of the problem substantially and can in fact create new latent features that will allow us to group variables together without having to make a decision as to which of the original variables we should pick. The link above to Wikipedia will give you more detail on the background to PCA.</p>
<p>Now, I want to point out a very important issue that all students seem to forget when they use PCA and that is it should only be used on continuous variables. Most students I have had in the past try it on nominal variables and it frankly makes no sense to do this. You can use a method called Multiple Correspondence Analysis (MCA) to help you address this problem.</p>
<p>In the following <a href="https://drive.google.com/open?id=1MIBEnJ6wwGildCIL8eFM5EfUJwHy7kqL">Google Colab notebook</a> there is a simple implementation of a PCA on the IRIS data. Review the code and then try it on the Scikit Learn Breast Cancer data. How much of the overall variation is explained by the first two components?</p>
</div><p><h2>4.10&emsp;Ridge regression L2</h2><div class="u-typography-bold-intro">
<p>In <a href="https://www.futurelearn.com/courses/intro-to-data-analytics/1/steps/661792">Step 3.9</a> of the first course in this program, we mentioned that it is unwise to include variables in a model that possesses multicollinearity.</p>
<p>Multicollinearity can create inaccurate estimates of the regression coefficients, and inflate the standard errors of the regression coefficients. This will impact on the partial t-tests for the regression coefficients and will thus give false nonsignificant p-values and degrade the predictability of the model (and that’s just for starters).</p>
<p>There are five sources (see Montgomery et al. (2001) for more details):</p>
<ol>
<li>
<p><strong>Data collection</strong>. In this case, the data have been collected from a narrow subspace of the independent variables**. The multicollinearity has been created by the sampling methodology - it does not exist in the population. Obtaining more data on an expanded range would cure this multicollinearity problem. The extreme example of this is when you try to fit a line to a single point.</p>
</li>
<li>
<p><strong>Physical constraints of the linear model or population</strong>. This source of multicollinearity will exist no matter what sampling technique is used. Many manufacturing or service processes have constraints on independent variables (as to their range) either physically, politically or legally, which will create multicollinearity.</p>
</li>
<li>
<p><strong>Over-defined model</strong>. Here, there are more variables than observations. This situation should be avoided.</p>
</li>
<li>
<p><strong>Model choice or specification</strong>. This source of multicollinearity comes from using independent variables that are powers or interactions of an original set of variables. It should be noted that if the sampling subspace of independent variables is narrow, then any combination of those variables will increase the multicollinearity problem even further.</p>
</li>
<li>
<p><strong>Outliers</strong>. Extreme values or outliers in the X-space can cause multicollinearity as well as hide it. We call this outlier-induced multicollinearity. This should be corrected by removing the outliers before ridge regression is applied.</p>
</li>
</ol>
<p>The concept behind ridge regression, also known as L2 regularization, is to adjust the estimates that you would normally get from ordinary least squares regression to give new estimates (which have a small amount of bias). This will have unbiased variance and subsequently deals with inflated VIF and reduce overfitting. It doesn’t get rid of attributes but can point you to those which are less significant. In ridge regression, the cost function is altered by adding a penalty equivalent to the square of the magnitude of the coefficients. It can help us to identify and deal with overfitting.</p>
<p>In ordinary least squares regression, we minimise the following cost function:</p>
<p><script type="math/tex">min_{\beta}(y-X\beta)^T(y-X\beta)</script> which gives</p>
<script type="math/tex; mode=display">\hat{B}_R={(X^TX)}^{-1}X^Ty</script>
<p>In ridge regression we have the following :</p>
<p><script type="math/tex">min_{\beta}(y-X\beta)^T(y-X\beta) +\lambda(\beta^T\beta-c)</script> which gives</p>
<script type="math/tex; mode=display">\hat{B}_R={(X^TX+{\lambda}I)}^{-1}X^Ty</script>
<p>and is equivalent to saying we are going to minimize the cost function for the ordinary least squares regression under the condition below:</p>
<p>For some c&gt; 0 <script type="math/tex">% <![CDATA[
\sum_{j=0}^p \beta^T\beta<c %]]></script></p>
<p>Choosing a value for k is not a simple task, which is perhaps one major reason why ridge regression isn’t used as much as least squares or logistic regression. You can read one way to find k in Dorugade and D. N. Kashid’s paper Alternative Method for Choosing Ridge Parameter for Regression. The literature does recommend that the <script type="math/tex">\lambda</script> value be kept under 0.3.</p>
<p>Have a look at the following <a href="https://drive.google.com/open?id=1DrDdnISDq5X45A6BWaywWhQb4fI4XGX1">Google Colab</a>. In it, we do an implementation of ridge regression. Do you think it is a good solution? Let us know in the comments section.</p>
<p><strong>References:</strong></p>
<p>Montgomery, D.C., E.A. Peck and G.G. Vining, 2001. <em>Introduction to Linear Regression Analysis</em>. 3rd Edn., John Wiley and Sons, New York, USA.</p>
</div><p><h2>4.11&emsp;Lasso regression L1 </h2><div class="u-typography-bold-intro">
<p>Lasso regression is an extension of ridge regression but has one major benefit. This is due to its ability to directly help with feature reduction.</p>
<p>“The key difference between these techniques is that lasso shrinks the less important feature’s coefficient to zero thus, removing some features altogether. So, this works well for feature selection in case we have a huge number of features.”<br/>
<sub><a href="https://towardsdatascience.com/l1-and-l2-regularization-methods-ce25e7fc831c">Toward Science</a></sub></p>
<p>In ridge regression we had the following formula:</p>
<p><script type="math/tex">min_{\beta}(y-X\beta)^T(y-X\beta) +\lambda(\beta^T\beta-c)</script> which gives</p>
<script type="math/tex; mode=display">\hat{B}_R={(X^TX+{\lambda}I)}^{-1}X^Ty</script>
<p>Now, for lasso regression the formula changes slightly, we replace the <script type="math/tex">\lambda(\beta^T\beta-c)</script> with <script type="math/tex">\lambda(\lvert\beta\rvert-c)</script></p>
<p>The impact of this change is that some of the parameters estimate <script type="math/tex">\beta</script> will be set to zero, thus helping us with variable selection. There is a really nice explanation <a href="https://stats.stackexchange.com/questions/74542/why-does-the-lasso-provide-variable-selection">here</a>.</p>
<p><strong>Elastic Net</strong></p>
<p>Elastic Net is an ensemble of both the L1 and L2 regularization techniques. Generally, lasso regression will eliminate many features and reduce overfitting in your linear model. Ridge regression will reduce the impact of features that are not important in predicting your y values. Elastic Net combines feature elimination from lasso and feature coefficient reduction from the ridge model to improve your model’s predictions. We have completed a small example of it at the end of these notes.</p>
<p>Let’s look at the data we had in the ridge regression step and see how it performs under lasso regression. Follow this <a href="https://drive.google.com/open?id=15ENdWCdwYAVMugUua1ugQOp41Ilb-pwh">link</a> to go to the Google Colab file for this step.</p>
</div><p><h2>4.13&emsp;Review of Topic 4</h2><div class="u-typography-bold-intro">
<p>Congratulations on completing Topic 4 of Pre-processing data and well done on finishing the course. We sincerely hope that you have enjoyed this part of the program.</p>
<p>This topic focused on different data reduction techniques that can be used to reduce complete datasets. Different data reduction techniques were also explored to transform continuous variables to categorical variables while maintaining the information held within the feature.</p>
<p>You should now understand how some simple transformations are conducted and how entropy can be used in the data discretization process. Additionally, you should be able to comprehend the differences between principal components, ridge regression and lasso regression</p>
<p>Having completed this topic, you should also be able to:</p>
<ul>
<li>Transform data using a number of scaling techniques</li>
<li>Discretize a continuous univariate variable.</li>
<li>Reducing a multivariate dataset using principal components, ridge regression and lasso regression.</li>
</ul>
<p>As usual, you are encouraged you to post any comments or questions that you may have in the comments section below.</p>
</div><p><h2>4.14&emsp;What's next?</h2><div class="u-typography-bold-intro">
<p>Congratulations on completing Pre-processing data. I hope that you enjoyed the course.</p>
<p>Having completed all of the Topics in this course you should now be able to:</p>
<ul>
<li>Implement a simple web scraper and use an API to download data.</li>
<li>Understand the differences between ERP and CRM systems.</li>
<li>Understand what a data warehouse is and how a star schema can be used to implement one.</li>
<li>Describe the reasons for pre-processing data and three principle mechanisms for dealing with the occurrence of missing data.</li>
<li>Implement a number of differing data imputation techniques and outline the hazards of imputation.</li>
<li>Identify univariate outliers and multivariate outliers as well as what heteroscedastic data is.</li>
<li>Describe Benford’s law and use discretization techniques to handle noisy data.</li>
<li>Understand exponential smoothing.</li>
<li>Transform data using a number of scaling techniques and discretize a continuous univariate variable.</li>
<li>Reduce a multivariate dataset using principal components, ridge regression and lasso regression.</li>
</ul>
<p>The next course in this series is <strong>Feature Engineering</strong>. You can access it <a href="https://www.futurelearn.com/courses/feature-engineering/1/todo/66869">here</a>.</p>
<p>See you there.</p>
</div><p style="page-break-before: always"><h2>1.1&emsp;Introduction to Feature Engineering</h2><div class="u-typography-bold-intro">
<p>Welcome to Feature Engineering, the third course in the Data Analytics and Data Mining program provided by Dublin City University.</p>
<p>In the video above the Lead Educator, Dr Andrew McCarren of Dublin City University’s School of Computing provides learners with an overview of the four topics that will be covered in this course, Feature Engineering.</p>
<p>As mentioned previously, this course is part of the MSc in Computing (Artificial Intelligence) developed by Dublin City University, in collaboration with Technology Ireland ICT Skillnet and Technology Ireland Software Skillnet.</p>
<h3 id="course-syllabus">Course syllabus</h3>
<p>There are five courses in this series of courses that are covered in the following order:</p>
<ol>
<li><strong><a href="https://www.futurelearn.com/courses/intro-to-data-analytics/1">Introduction to Data Analytics</a></strong> provided an overview of the basics of data analytics and data mining along with their relationship to statistics. During this course, you will be introduced to probability, regression and analysis of variance along with practical examples of their use.</li>
<li><strong><a href="https://www.futurelearn.com/courses/pre-processing-data-and-feature-impact-calculation/1">Pre-processing data and feature impact calculation</a></strong> explored techniques used to handle missing data, to detect anomalies for univariate and multivariate data sources, to transform and reduce datasets.</li>
<li><strong>Feature engineering</strong> will illustrate techniques that can help reduce the dimensionality of a dataset and cluster datasets using unsupervised machine learning techniques.</li>
<li><strong>Introduction to processing unstructured data</strong> will introduce learners to several approaches that allow them to incorporate images, text and graph data sources into data analysis.</li>
<li><strong>Point estimation and feature impact calculation</strong> will examine the Generalised Linear Models and two methods of parameter estimation.</li>
</ol>
<p>We recommend that you take these courses in chronological order as they appear above.</p>
<h3 id="introduction-to-feature-engineering---course-structure">Introduction to Feature Engineering - Course Structure</h3>
<p>There are four topics in this course. This course has the following structure:</p>
<ul>
<li><strong>Topic 1</strong> explores association rule mining and how we can use it to look for casual structures, frequent patterns, correlations, associations or causal structures from data sets found in various kinds of databases such as relational databases, transactional databases, and other forms of data repositories.</li>
<li><strong>In Topic 2</strong> you will be introduced to feature engineering. This is the process of using existing features or other relevant domain knowledge information to create new features that can help analysts visualise their data better, reduce their dataset and improve their algorithms predictive power.</li>
<li>We will continue our discussion of feature engineering in <strong>Topic 3</strong> by looking at machine learning algorithms. Specifically, we will look at how Random Forest and Autoencoders can be used to produce new variables that will both reduce the dimensionality of datasets and improve predictive power.</li>
<li>Finally, in <strong>Topic 4</strong>, cluster analysis will be introduced as a way of classifying multivariate groups of data. On completion of this topic, you will understand the concepts of distance, similarity, dissimilarity and how we can integrate categorical and continuous variables together.</li>
</ul>
<p><img alt="Statement of funding from Skillnet Ireland informing that: This course has been grant-aided by skillnet Ireland. Technology Ireland ICT Skillnet provides advanced training and development activities for technical and engineering staff in the ICT. Participants may be contacted by Skillnet Ireland to confirm that they attend this course.  
Information from susan.kelly@ictskillnet.com" sizes="(min-width: 1695px) 729px, (min-width: 680px) 648px, 320px" src="https://ugc.futurelearn.com/uploads/assets/14/0b/hero_140bfdc0-2f0f-408d-bf43-4ed1e4240646.jpg" srcset="https://ugc.futurelearn.com/uploads/assets/14/0b/small_hero_140bfdc0-2f0f-408d-bf43-4ed1e4240646.jpg 320w, https://ugc.futurelearn.com/uploads/assets/14/0b/hero_140bfdc0-2f0f-408d-bf43-4ed1e4240646.jpg 648w, https://ugc.futurelearn.com/uploads/assets/14/0b/large_hero_140bfdc0-2f0f-408d-bf43-4ed1e4240646.jpg 729w, https://ugc.futurelearn.com/uploads/assets/14/0b/large_hero_140bfdc0-2f0f-408d-bf43-4ed1e4240646.jpg 2x"/></p>
<hr/>
<h5 id="copyright-disclaimer">Copyright Disclaimer</h5>
<p><sub>All images used on this course are copyright by <a href="https://www.shutterstock.com/">Shutterstock</a> unless otherwise stated. Dublin City University does not knowingly intend or attempt to offend or violate any copyright or intellectual property rights of any entity. If images posted here are in violation of copyright law, please contact us and we will gladly remove the offending images immediately.</sub></p>
</div><p><h2>1.2&emsp;Welcome to Topic 1</h2><div class="u-typography-bold-intro">
<p>Welcome to Topic 1 of Feature Engineering, Association Rule Mining.</p>
<p>In many situations in data analytics, we as analysts are looking for ways to understand relationships between variables. We have seen already how correlation can be used to 
examine the relationship between variables (see <a href="https://www.futurelearn.com/courses/distributed-ledger-technologies/1/steps/657513">Step 3.9</a> in the first course in this programme). In this topic, we will look at association rule mining. Specifically, we will examine how to use association rule mining as a procedure to find frequent patterns, correlations, associations or causal structures in datasets found in various kinds of databases. These databases may include relational databases, transactional databases and other forms of data repositories.</p>
<p>On completion of this topic, you will be able to:</p>
<ul>
<li>Describe what an association rule is.</li>
<li>How association rule mining can be used to look for casual structures in data.</li>
<li>Implement a couple of basic routines and where they could be used in practice.</li>
</ul>
<p>As with the other course in this program so far, we encourage you to make use of the comments sections under various steps to interact with your fellow learners and the lead educator.</p>
<p>We would like to take this opportunity to remind you to please be respectful of your fellow learners and towards the lead educator when posting in the comments section. If you have any questions regarding this directive you can review FutureLearn’s code of conduct <a href="https://about.futurelearn.com/terms/code-of-conduct">here</a>.</p>
</div><p><h2>1.3&emsp;Introduction to association rules</h2><div class="u-typography-bold-intro">
<p>Association rule mining is a rule-based machine learning technique that assists in the discovery of patterns in large scale databases.</p>
<p>The subject was proposed by <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.88.8047&amp;rep=rep1&amp;type=pdf">Agrawal and Shim in 1993</a>  and has been used extensively by the database and data mining community. It assumes all the data are categorical and attempts to find relationships between categories or groups of categories. It was initially used for market basket analysis with the intention of understanding customer buying patterns. So for example, if someone buys {Potatoes, Onions} then they are likely to buy Burgers. This is typically written as follows:</p>
<script type="math/tex; mode=display">\{Potatoes, Onions\} \implies \{Burgers\}</script>
<p>and there is no importance to the order. So in this example, the order of Onions and Potatoes has no significance.</p>
<p>The idea behind this concept was to identify shopping patterns. Understanding these patterns allows supermarkets to control their sales and even gives them a handle on products they need to buy. This may seem strange, but you should all be familiar with supermarket pricing promotions. So, for example, you may be offered soft drinks at a knockdown price, but when you are in the supermarket the price of potato chips has remained the same. Supermarkets will have asked the suppliers of soft drinks and potato chips for a set schedule of promotions and nine out of ten times they will coordinate these items together. They do this so they can boost their profits on potato chips and increase their margin on high volumes. The suppliers of both products are happy as they will increase their sales and will only take a minor hit on their margin.</p>
<p>The concept has not just been applied to market basket analysis but also to web usage mining, intrusion detection, continuous production and bioinformatics.</p>
<p>Can you think of another example? Post these in the comments section below. I will point out that since the advent of online selling these assumptions are being seriously tested, so I suspect the high street retailers will have to change their sales mechanism to compete.</p>
<p>In the next step, we are going to look at a number of definitions and metrics that are used in association rule mining.</p>
<hr/>
<p><strong>References:</strong></p>
<p>Agrawal, R. and Shim, K., 1993.<strong>Developing Tightly-Coupled Data Mining Applications on a Relational Database System</strong>. Available <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.88.8047&amp;rep=rep1&amp;type=pdf">here</a>.</p>
</div><p><h2>1.4&emsp;Rules and metrics definitions </h2><div class="u-typography-bold-intro">
<p>In order to get used to the terminology and jargon that is used in association rule mining, we are going to have to outline a number of definitions that are typically used. We will then outline a few examples of each definition. Get used to these terms as quickly as you can because they will be used throughout this Topic.</p>
<h1 id="definitions"><strong>Definitions</strong></h1>
<p><strong>Items I</strong>
For each transaction database, there is always a set of items that are finite (though they may be very large)</p>
<ul>
<li><script type="math/tex">I =\{i_1,i_2,...,i_m\}</script> where <script type="math/tex">i_m</script> is an individual item and <script type="math/tex">I</script> is the complete set of items.</li>
</ul>
<h3 id="transaction-t">Transaction t</h3>
<ul>
<li><script type="math/tex">t</script> is a set of items and <script type="math/tex">t \subseteq I</script></li>
</ul>
<p>and <script type="math/tex">T</script> is a set of transactions or a Transaction Database.</p>
<h3 id="transaction-database-t">Transaction Database T</h3>
<ul>
<li>
<script type="math/tex; mode=display">T= \{t_1,t_1,....,t_n\}</script>
</li>
</ul>
<p>So if we are to relate these definitions to a supermarket’s data we would have the following market basket transactions:</p>
<ul>
<li>
<script type="math/tex; mode=display">t_1 : \{bread,Cheese,milk\}</script>
</li>
<li>
<script type="math/tex; mode=display">t_2 : \{apples,eggs, butter, yogurt\}</script>
</li>
<li>
<p>.</p>
</li>
<li>
<p>.</p>
</li>
<li>
<script type="math/tex; mode=display">t_n : \{biscuit,eggs,milk\}</script>
</li>
</ul>
<p>So we are now going to define some concepts:</p>
<ul>
<li><strong>An item <script type="math/tex">i</script></strong> is an item or article in a basket.</li>
<li><strong><script type="math/tex">I</script></strong> is the set of all items sold in the store.</li>
<li><strong>A transaction</strong> is the items purchased in the basket. It will most likely have a transaction ID (TID).</li>
<li><strong>A transactional dataset</strong> is a set of transactions.</li>
</ul>
<p>So in association rule mining set theory definitions are used to describe the rule structure.</p>
<ul>
<li>
<p>A transaction t contains X, a set of items <strong>(itemset)</strong> in <script type="math/tex">I</script>, if <script type="math/tex">X \subseteq t</script>.</p>
</li>
<li>
<p>An association rule is an implication of the form:</p>
</li>
</ul>
<p><script type="math/tex">X \mapsto Y</script>, where <script type="math/tex">X, Y \subset I</script>, and <script type="math/tex">X \cap Y = \emptyset</script></p>
<ul>
<li>An itemset is a set of items:</li>
</ul>
<p>E.g. <script type="math/tex">X= \{milk, bread, cerial\}</script> is an itemset.</p>
<p>So we have now defined what a rule is but we need some metrics to assess a proposed rule:</p>
<h3 id="support">Support</h3>
<p>The support of X with respect to T is defined as the proportion of transactions <script type="math/tex">t</script> in the dataset which contains the itemset X:</p>
<script type="math/tex; mode=display">supp(X)=\frac{\lvert t\epsilon T;X \subseteq t \rvert}{\lvert T \rvert}</script>
<p>Support is an indication of how frequently an item is both. The support count of an itemset <script type="math/tex">X</script>, denoted by <script type="math/tex">X.count</script> in a dataset <script type="math/tex">T</script> is the number of transactions in <script type="math/tex">T</script> that contain <script type="math/tex">X</script>, If we assume <script type="math/tex">T</script> has n transactions then you can have support for a rule <script type="math/tex">X \mapsto Y</script>  and it can be defined as:</p>
<script type="math/tex; mode=display">supp(X \mapsto Y)=\frac{(X \cup Y).count}{n}</script>
<h3 id="confidence">Confidence</h3>
<p>We can also define the confidence of a rule <script type="math/tex">X \mapsto Y</script>, with respect to a set of transactions <script type="math/tex">T</script>, is the proportion of transactions that contains <script type="math/tex">X</script> which also contains Y. It can be defined as follows:</p>
<blockquote>
<script type="math/tex; mode=display">conf(X \mapsto Y)=\frac{supp(X \cup Y)}{supp(X)}</script>
</blockquote>
<p>Note that <script type="math/tex">supp(X\cup Y)</script> means the support of the union of the items in <script type="math/tex">X</script> and <script type="math/tex">Y</script>. This is somewhat confusing since we normally think in terms of probabilities of events and not sets of items. We can rewrite <script type="math/tex">supp (X\cup Y)</script> as the probability <script type="math/tex">P(E_{X} \cap E_{Y})</script> where <script type="math/tex">E_{X}</script> and <script type="math/tex">E_{Y}</script> are the events that a transaction contains itemset <script type="math/tex">X</script> and <script type="math/tex">Y</script>.</p>
<p>Thus confidence can be interpreted as an estimate of the conditional probability <script type="math/tex">P(E_y</script> \ <script type="math/tex">E_x)</script> the probability of finding the RHS of the rule in transactions under the condition that these transactions also contain the LHS, Hahsler(2005).</p>
<h3 id="lift">Lift</h3>
<p>Lift can be defined as</p>
<blockquote>
<script type="math/tex; mode=display">lift(X \mapsto Y)=\frac{supp(X \cup Y)}{supp(X)xsupp(Y)}</script>
</blockquote>
<p>Lift is effectively the observed support in ratio to that if the events were statistically independent.</p>
<h3 id="conviction">Conviction</h3>
<p>Conviction can be interpreted as the ratio of the expected frequency that <script type="math/tex">X</script> occurs without <script type="math/tex">Y</script>.</p>
<script type="math/tex; mode=display">Conv(X \mapsto Y)=\frac{1-sup(Y)}{1-conf(X \mapsto Y)}</script>
<h3 id="example">Example</h3>
<p>We have now had 4 metrics that will allow us to make decisions about the relationship between rules. The following is a small transactional database:</p>
<table>
<tbody>
<tr>
<td>t1:</td>
<td>Beef, chicken, Milk</td>
</tr>
<tr>
<td>t2:</td>
<td>Beef, Cheese</td>
</tr>
<tr>
<td>t3:</td>
<td>Cheese, Boots</td>
</tr>
<tr>
<td>t4:</td>
<td>Beef, Chicken, Cheese</td>
</tr>
<tr>
<td>t5:</td>
<td>Beef, Chicken, Clothes, Cheese, Milk</td>
</tr>
<tr>
<td>t6:</td>
<td>Chicken, Clothes, Milk</td>
</tr>
<tr>
<td>t7:</td>
<td>Chicken, Milk, Clothes</td>
</tr>
</tbody>
</table>
<p>Let’s do a few calculations to make sure it all makes sense:</p>
<ul>
<li>
<script type="math/tex; mode=display">supp(Beef) = 4/7</script>
</li>
<li>
<script type="math/tex; mode=display">supp(chicken) = 5/7</script>
</li>
<li>
<script type="math/tex; mode=display">supp(Beef \mapsto Chicken) = 3/7</script>
</li>
<li>
<script type="math/tex; mode=display">conf(Beef \mapsto Chicken) = \frac{\frac{3}{7}}{\frac{4}{7}}</script>
</li>
<li>
<script type="math/tex; mode=display">conf(\{Beef,Chicken\} \mapsto Cheese)=\frac{\frac{2}{7}}{\frac{3}{7}}</script>
</li>
<li><script type="math/tex">lift(Beef \mapsto Chicken)=\frac{supp(Beef \cup Chicken )}{supp(Beef).supp(Chicken)} =</script> <script type="math/tex">\frac{\frac{3}{7}}{\frac{4}{7}.\frac{5}{7}}</script></li>
</ul>
<p>Interpreting these results is a little tricky. A lift &gt; 1 indicates a positive dependence or substitution. In other words, if we have <script type="math/tex">Beef</script> in our basket is there a likelihood that there will be <script type="math/tex">Chicken</script>. This term can be very useful as it vouches if there is a high(&gt;1) or low (&lt; 1) association between products. In our case, the Lift is 1.05 which implies there is an association between the 2 products. This type of metric will help store managers to make promotions and product placements.</p>
<p>Now can you calculate the following:</p>
<ul>
<li>
<script type="math/tex; mode=display">conf(\{Beef,milk\} \mapsto Chicken)</script>
</li>
</ul>
<p>Towards Data Science have a nice <a href="https://towardsdatascience.com/association-rules-2-aa9a77241654">link</a> that will help you understand these terms a little better.</p>
</div><p><h2>1.5&emsp;Brute force approach</h2><div class="u-typography-bold-intro">
<p>So now you should understand the background and some basic definitions to association rule mining. The next question is: how do we find the rules that are most useful to us?</p>
<p>There are a number of techniques that we will examine in this topic but to understand them we really need to examine the most basic approach which is known as the brute force approach. It is as simple as it sounds, we are going to extract every possible rule that exists. Imagine you have 5 products {A,B,C,D,E} in a virtual store and that your customers can take any combination of the 5 products. They could buy {A,B} or {B,D,E} for example. So if you wanted to find out every rule possible we would simply generate all the combinations of the list of products. In this case it would would come to <script type="math/tex">{5 \choose 1}</script>+<script type="math/tex">{5 \choose 2}</script>+<script type="math/tex">{5 \choose 3}</script>+<script type="math/tex">{5 \choose 4}</script>+<script type="math/tex">{5 \choose 5}</script> combinations. Graphically this would look like the lattice laid out below:</p>
<p><img alt="Brute Frorce Approach" src="https://www.computing.dcu.ie/~amccarren/mcm_images/Brute_Force_Approach.png"/></p>
<p>We would have a total of 31 possible combinations of rules or list of potential baskets. There is a small piece of code which generates every possible combination of the virtual stores’ product list.</p>
<p>Follow this <a href="https://colab.research.google.com/drive/182GymbVjA9X5CZqQ6XdYz_q9JnpP9MsO#scrollTo=qk6_WWV5qtw-">link</a> to go to the Colab for this step.</p>
</div><p><h2>1.6&emsp;Apriori algorithm</h2><div class="u-typography-bold-intro">
<p>In the previous step, we examined the brute force algorithm and saw how even with a small number of products the potential number of baskets was very high.</p>
<p>Processing the transaction database for each basket would take excessive amounts of time if we were to do so. The Ariori algorithm, <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.88.8047&amp;rep=rep1&amp;type=pdf">Algrawal and Srikant (1984)</a>, is probably one of the best-known algorithms in association rule mining. The algorithm proceeds by identifying the frequent individual items in the database and extending them to larger and larger item sets as long as those item sets appear sufficiently often in the database, using effectively two steps. These steps are as follows:</p>
<ol>
<li>
<p>Find all itemsets that have minimum support.</p>
</li>
<li>
<p>Use frequent itemsets to generate rules.</p>
</li>
</ol>
<p>In the example below, we have seven transactions.</p>
<table>
<tbody>
<tr>
<td>t1:</td>
<td>Beef, Chicken, Milk</td>
</tr>
<tr>
<td>t2:</td>
<td>Beef, Cheese</td>
</tr>
<tr>
<td>t3:</td>
<td>Cheese, Boots</td>
</tr>
<tr>
<td>t4:</td>
<td>Beef, Chicken, Cheese</td>
</tr>
<tr>
<td>t5:</td>
<td>Beef, Chicken, Clothes, Cheese, Milk</td>
</tr>
<tr>
<td>t6:</td>
<td>Chicken, Clothes, Milk</td>
</tr>
<tr>
<td>t7:</td>
<td>Chicken, Milk, Clothes</td>
</tr>
</tbody>
</table>
<p>So we can see from this transactional database that the rule <script type="math/tex">Clothes {\mapsto}Milk,Chicken</script> has of <script type="math/tex">[sup= 3</script> \ <script type="math/tex">7,conf =3</script> \ <script type="math/tex">3]</script> which is a frequent itemset. But if we started with <script type="math/tex">Boots</script>, we would get support of <script type="math/tex">1</script> \ <script type="math/tex">7</script> which means that we could not generate frequent itemsets from it. So there wouldn’t be any point starting with boots or in other words going down this part of the tree if we were looking for frequent itemsets. The Apriori algorithm can be explained in the with the following pseudo-code:</p>
<p><a href="https://ugc.futurelearn.com/uploads/assets/a5/79/a5792fdd-244b-457c-ba8f-b2edb0d47fa1.png"><img alt="Infographic showing process. Each step shown in box. Step 1: Scan the transaction database to get the support of S each 1 – itemset, compare S with min_sup, and get a support of 1-itemset, L1. Step 2: Use L­k-1 join Lk-1 to generate a set of candidate k-itemsets. And use Apriori property to prune the unfrequented k-itemsets from this set. Step 3: Scan the transaction database to get the support S of each candidate k-itemset in the find set, compare S with min_up, and get a set of frequent k-itemsets Lk. Step 4 The candidate set = Null. Two arrows stem from this box. one leading back to Step 2 labelled no and one leading to Step 5 labelled yes. Step 5: For each frequent itemset 1, generate all nonempty subsets of 1. Step 6: For every nonempty subsets of 1, output the rule “s=&gt;(1-s)” if confidence C of the rule “s=&gt;(1-s)” (=support s of 1/support S of s)’ min_conf" sizes="(min-width: 1695px) 729px, (min-width: 680px) 648px, 320px" src="https://ugc.futurelearn.com/uploads/assets/a5/79/hero_a5792fdd-244b-457c-ba8f-b2edb0d47fa1.png" srcset="https://ugc.futurelearn.com/uploads/assets/a5/79/small_hero_a5792fdd-244b-457c-ba8f-b2edb0d47fa1.png 320w, https://ugc.futurelearn.com/uploads/assets/a5/79/hero_a5792fdd-244b-457c-ba8f-b2edb0d47fa1.png 648w, https://ugc.futurelearn.com/uploads/assets/a5/79/large_hero_a5792fdd-244b-457c-ba8f-b2edb0d47fa1.png 729w, https://ugc.futurelearn.com/uploads/assets/a5/79/large_hero_a5792fdd-244b-457c-ba8f-b2edb0d47fa1.png 2x"/></a></p>
<p>Let’s look at another example and examine how it works. See the table below.</p>
<table>
<tbody>
<tr>
<td><strong>Transaction number</strong></td>
<td><strong>Items</strong></td>
</tr>
<tr>
<td>1</td>
<td>{Bread, Milk}</td>
</tr>
<tr>
<td>2</td>
<td>{Bread, Diapers, Beer, Eggs}</td>
</tr>
<tr>
<td>3</td>
<td>{Milk, Diapers, Beer, Cola}</td>
</tr>
<tr>
<td>4</td>
<td>{Bread, Milk, Diapers, Beers}</td>
</tr>
<tr>
<td>5</td>
<td>{Bread, Milk, Diapers, Cola}</td>
</tr>
</tbody>
</table>
<p>So we have the following unique items in our database and they are {Bread, Milk, Diapers, Beer, Eggs, Cola}. We can see from the database that Milk occurs four times and Eggs only occurs once. If we were to examine each individual item and the items in couples, triples and so we would get a pattern such as that shown below. Now notice how we have a term called Minimum support.</p>
<p><a href="https://ugc.futurelearn.com/uploads/assets/05/f1/05f1f242-dc07-467b-808e-55372091cedc.png"><img alt="Figure illustrates that we move from one itemset to two and so-on. What we are trying to do is build as large an itemset as possible given the minimum support constraints.alt tex" sizes="(min-width: 1695px) 729px, (min-width: 680px) 648px, 320px" src="https://ugc.futurelearn.com/uploads/assets/c8/4c/hero_c84c09df-a186-4abf-8702-929e5c13bd6b.png" srcset="https://ugc.futurelearn.com/uploads/assets/c8/4c/small_hero_c84c09df-a186-4abf-8702-929e5c13bd6b.png 320w, https://ugc.futurelearn.com/uploads/assets/c8/4c/hero_c84c09df-a186-4abf-8702-929e5c13bd6b.png 648w, https://ugc.futurelearn.com/uploads/assets/c8/4c/large_hero_c84c09df-a186-4abf-8702-929e5c13bd6b.png 729w, https://ugc.futurelearn.com/uploads/assets/c8/4c/large_hero_c84c09df-a186-4abf-8702-929e5c13bd6b.png 2x"/></a></p>
<p>In the next step, we will complete a working example of the Apriori algorithm in Python.</p>
</div><p><h2>1.7&emsp;Apriori example</h2><div class="u-typography-bold-intro">
<p>This step will provide a working example of the Apriori algorithm using Python.</p>
<p>The example we are going to use to demonstrate the Apriori algorithm comes from <a href="https://www.geeksforgeeks.org/implementing-apriori-algorithm-in-python/">GeeksforGeeks</a>. In it, we will pull down an online retail dataset and attempt to find the most frequent itemsets. I have concentrated the following code on UK sales as these possess the most transactions. Now you will notice that when I am importing the data into Google Colabs that I have used “<a href="https://en.wikipedia.org/wiki/ISO/IEC_8859-1">ISO-8859-1</a>” encoding.</p>
<p>You should also look at how the Apriori algorithm needs to be set up. Each product must be set up as a hot encoded column for each invoice (basket). This <a href="http://rasbt.github.io/mlxtend/api_subpackages/mlxtend.frequent_patterns/">link</a> should help you here.</p>
<p>In the final rules dataset, you will see two columns (‘antecedents’,’consequents’) that might need some explanation. In the previous steps, we referred to them as <script type="math/tex">X \mapsto Y</script>. <script type="math/tex">X</script> is the antecedent and <script type="math/tex">Y</script> is the consequent.</p>
<p>The remaining columns describe the metrics that we have explained in <a href="https://www.futurelearn.com/courses/feature-engineering/1/steps/712161">Step 1.4</a>. Experiment with the data and see if it all makes sense.</p>
<p>You can access the Google Colab file <a href="https://colab.research.google.com/drive/1_0I2s5-69QwOVnakzPoWm1D5xcWdnq6J#scrollTo=YOuknZJZMliT">here</a>.</p>
</div><p><h2>1.8&emsp;MS-Apriori algorithm</h2><div class="u-typography-bold-intro">
<p>The Apriori algorithm that we looked at in <a href="https://www.futurelearn.com/courses/feature-engineering/1/steps/712210">Step 1.6</a> assumed all items had the same minimum support and that all items have a similar frequency.</p>
<p>In reality, this is not the case as some items appear very frequently in our data, while others rarely appear. For example, in a supermarket, people buy food processors and cooking pans much less frequently than they buy bread and milk. If the frequencies of items vary a great deal, we will encounter two problems. These are as follows:</p>
<ol>
<li>
<p>If <em>minsup</em> is set too high, those rules that involve rare items will not be found.</p>
</li>
<li>
<p>To find rules that involve both frequent and rare items, minsup has to be set very low. This may cause a combinatorial explosion because those frequent items will be associated with one another in all possible ways.</p>
</li>
</ol>
<p>You may ask yourself why would you care if something is a rare item. Well if any of you shop in Lidl or Aldi then you will have noticed their promotions on rare items such as tents or wet suits. These supermarkets know what items will be sold with these items and this knowledge allows them to plan their marketing strategy. People won’t buy these things every week but the supermarket knows that if they do a promotion on these items then associated items will be sold as well.</p>
<p>So the answer to this problem is to have multiple minimum supports. The minimum support of a rule is expressed in terms of minimum item supports (MIS) of the items that appear in the rule. Each item can have a minimum item support, and by providing different MIS values for different items, the user effectively expresses different support requirements for different rules.</p>
<p>So let’s go a little deeper:</p>
<p>Let <script type="math/tex">MIS(a_i)</script> be the the MIS value if item <script type="math/tex">i</script>. The minsup of a rule <script type="math/tex">R</script> is the lowest value of the items in the rule.</p>
<p>In other words a rule</p>
<p><script type="math/tex">R: a_1,a_2,..,a_k \mapsto a_{k+1},...,a_r</script> satisfies its minimum support if its actual support is <script type="math/tex">\ge</script> <script type="math/tex">min(MIS(a_1),MIS(a_2),...,MIS(a_r))</script></p>
<p>Let’s look at an example; consider the following items:</p>
<script type="math/tex; mode=display">[bread, shoes, clothes ]</script>
<p>The user-specified MIS values are as follows:
<script type="math/tex">MIS(bread) = 2\%</script>, <script type="math/tex">MIS(shoes) = 0.1\%</script>, <script type="math/tex">MIS(bread) = 0.2\%</script></p>
<p>The following rule doesn’t satisfy its minsup:</p>
<p><script type="math/tex">clothes \mapsto bread</script> <script type="math/tex">[ sup=0.15\%,conf =70\%]</script></p>
<p>Why? well the support for <script type="math/tex">clothes \mapsto bread</script> is <script type="math/tex">15\%</script> which is less than the</p>
<p><script type="math/tex">Min(MIS(bread),MIS(bread)</script> = <script type="math/tex">Min(2\%,0.2\%)</script></p>
<p><script type="math/tex">clothes \mapsto shoes</script> <script type="math/tex">[ sup=0.15\%,conf =70\%]</script></p>
<p>Why? well the support for <script type="math/tex">clothes \mapsto shoes</script> is <script type="math/tex">15\%</script> which is greater than the</p>
<p><script type="math/tex">Min(MIS(bread),MIS(shoes)</script> = <script type="math/tex">Min(2\%,0.1\%)</script></p>
<h3 id="benefits-of-ms-apriori">Benefits of MS-Apriori</h3>
<p>The benefits of MS-Apriori are:</p>
<ul>
<li>Multiple minsup model subsumes the single support model.</li>
<li>It is a more realistic model for practical applications.</li>
<li>The model enables us to found rare item rules yet without producing a huge number of meaningless rules with frequent items.</li>
<li>By setting MIS values of some items to 100% (or more), we effectively instruct the algorithms not to generate rules only involving these items.</li>
</ul>
<p>I am not going to describe the algorithm in detail, however, for those that are interested, you can download my slides for association rule mining <a href="https://www.computing.dcu.ie/~amccarren/mcm_slides/Lecture_6_association_rules.ppt">here</a>.</p>
<p>Now let’s have a look at a working example in Python in the next step.</p>
</div><p><h2>1.9&emsp;MS-Apriori example</h2><div class="u-typography-bold-intro">
<p>The step demonstrates the MS-Apriori algorithm.</p>
<p>The MS-apriori example I am going to show you here was written by Bing Liu and is located on <a href="https://github.com/sachinbiradar9/MS-Apriori">Github</a>. Additionally, this code is not supported by Python 3.7 so you will have to set the runtime to Python 2 in the options above.</p>
<p>Normally, I would use the Google Drive ID’s to reference files, but in this situation I want you to follow the instructions exactly, so I have mounted Google Drive as a virtual drive in the following Google Colab available <a href="https://drive.google.com/open?id=1edytZC1eI7XgoY5SqGazuRttUCvggPNI">here</a>. There are other versions of this online and you may find this <a href="https://github.com/pavvu/FindFrequentItemSetWithMultipleItemSupport/blob/master/README.md">one</a> more useful.</p>
<p>Now you will also have to download the <a href="https://drive.google.com/open?id=1RyPO44xoEf60CSi1OmhbRSRSAOeA3hEl">transaction.txt</a> and <a href="https://drive.google.com/open?id=1kee1SdYZLVbBqiA3Qz6GvtPa0iJOIBq-">parameter.txt</a> files and store them on your Google Drive, or where ever you want to run them from.</p>
</div><p><h2>1.10&emsp;Mining class association rules</h2><div class="u-typography-bold-intro">
<p>Normal association rule mining does not have any target, as it finds all possible rules that exist in data. For example, any item can appear as a consequent or a condition of a rule. However, in some applications, the user is interested in some targets. For example, the user has a set of text documents from some known topics. He/She wants to find out what words are associated with or correlated with each topic.</p>
<h3 id="problem-definition">Problem definition</h3>
<ul>
<li>Let <script type="math/tex">T</script> be a transaction data set consisting of n transactions. Each transaction is also labelled with a class <script type="math/tex">y</script>.</li>
<li>Let I be the set of all items in <script type="math/tex">T</script>, <script type="math/tex">Y</script> be the set of all class labels and <script type="math/tex">I \cap Y = \emptyset</script>.</li>
<li>A class association rule (CAR) is an implication of the form <script type="math/tex">X \mapsto y</script>, where <script type="math/tex">X \subseteq I</script>, and <script type="math/tex">y \; \epsilon \; Y</script>.</li>
<li>The definitions of support and confidence are the same as those for normal association rules.</li>
</ul>
<h3 id="an-example">An example</h3>
<p>Let’s look at a  text document data set. In it we have documents that have keywords in them and each set of words is categorised with a label becomes:</p>
<table>
<thead>
<tr>
<th><strong>Document</strong></th>
<th><strong>Relevant Words</strong></th>
<th><strong>Category</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td>doc 1:</td>
<td>Student, Teach, School</td>
<td>Education</td>
</tr>
<tr>
<td>doc 2:</td>
<td>Student, School</td>
<td>Education</td>
</tr>
<tr>
<td>doc 3:</td>
<td>Teach, School, City, Game</td>
<td>Education</td>
</tr>
<tr>
<td>doc 4:</td>
<td>Baseball, Basketball</td>
<td>Sport</td>
</tr>
<tr>
<td>doc 5:</td>
<td>Basketball, Player, Spectator</td>
<td>Sport</td>
</tr>
<tr>
<td>doc 6:</td>
<td>Baseball, Coach, Game, Team</td>
<td>Sport</td>
</tr>
<tr>
<td>doc 7:</td>
<td>Basketball, Team, City, Game</td>
<td>Sport</td>
</tr>
</tbody>
</table>
<p>In this example, we want to examine the support and confidence for a group of words.</p>
<ul>
<li>Let <script type="math/tex">minsup = 20%\</script> and <script type="math/tex">minconf = 60\%</script>.</li>
</ul>
<p>The following are two examples of class association rules:</p>
<ul>
<li>
<p><script type="math/tex">Student, School \mapsto Education</script> <script type="math/tex">\;\;\;[sup= 2/7, conf = 2/2]</script></p>
</li>
<li>
<p><script type="math/tex">game\mapsto Sport</script> <script type="math/tex">\;\;\;  [sup= 2/7, conf = 2/3]</script></p>
</li>
</ul>
<p>Unlike normal association rules, CARs can be mined directly in one step.</p>
<p>The key operation is to find all rule items that have support above minsup. A rule item is of the form:</p>
<p>where condset is a set of items from I (i.e., <script type="math/tex">condset \subseteq I</script>), and  <script type="math/tex">y \;\epsilon \; Y</script> is a class label.</p>
<p>Each rule item basically represents a rule:</p>
<p>?<script type="math/tex">condset \mapsto y</script>,</p>
<p>The Apriori algorithm can be modified to generate CARs. The multiple minimum support idea can also be applied here. The user can specify different minimum supports to different classes, which effectively assign a different minimum support to the rules of each class. 
For example, we have a data set with two classes, Yes and No. We may want rules of class Yes to have the minimum support of 5% and rules of class No to have the minimum support of 10%. By setting minimum class supports to 100% (or more for some classes), we tell the algorithm not to generate rules of those classes. This is a very useful trick in applications.</p>
<p>Can you think of any other examples that we could use CARs for? Let us know in the comments section below.</p>
</div><p><h2>1.12&emsp;Review of Topic 1</h2><div class="u-typography-bold-intro">
<p>Congratulations on completing Topic 1, association rule mining.</p>
<p>We learned that association rule mining is a procedure which is meant to find frequent patterns, correlations, associations, or causal structures from data sets found in various kinds of databases such as relational databases, transactional databases, and other forms of data repositories.</p>
<p>On completion of this topic you should now be able to:</p>
<ul>
<li>Understand what an association rule is.</li>
<li>Implement a couple of basic routines.</li>
<li>Identify where an association rule could be used in practice to look for casual structures in our data.</li>
</ul>
<p>In the comments section below please feel free to share any questions that you may have regarding the content that we have covered in the course so far.</p>
<p>In the next topic, we will begin our exploration of feature engineering.</p>
</div><p style="page-break-before: always"><h2>2.1&emsp;Welcome to Topic 2</h2><div class="u-typography-bold-intro">
<p>Welcome to Topic 2 of Feature Engineering.</p>
<p>In the last topic, we focused on association rule mining. We learned that it is a procedure which is meant to find such things as frequent patterns, correlations, and associations from datasets.</p>
<p>In this topic, we will begin our discussion of feature engineering. Feature engineering is the process of using existing features or other relevant domain knowledge information to create new features that can help analysts visualise their data better, reduce their dataset and improve their algorithms predictive power. There is a wide range of techniques available and we will give students a flavour of the most popular.</p>
<p>At the end of this topic, you will be able to:</p>
<ol>
<li>Define feature engineering.</li>
<li>Implement a wide range of methods that are typically used in data analytics projects.</li>
<li>Understand the impact multicollinearity has on analysis.</li>
</ol>
<p>We have included a number of practical illustrations using Google Colab files in this topic. We have also included a number of discussion prompts to encourage you to interact with your fellow learners.</p>
<p>Finally, we would like to remind you to mark each step as complete as you go through this topic in order to help you keep track of your progress.</p>
<p>See you in the comments section.</p>
<p><img alt="Statement of funding from Skillnet Ireland informing that: This course has been grant-aided by skillnet Ireland. Technology Ireland ICT Skillnet provides advanced training and development activities for technical and engineering staff in the ICT. Participants may be contacted by Skillnet Ireland to confirm that they attend this course.  
Information from susan.kelly@ictskillnet.com" sizes="(min-width: 1695px) 729px, (min-width: 680px) 648px, 320px" src="https://ugc.futurelearn.com/uploads/assets/14/0b/hero_140bfdc0-2f0f-408d-bf43-4ed1e4240646.jpg" srcset="https://ugc.futurelearn.com/uploads/assets/14/0b/small_hero_140bfdc0-2f0f-408d-bf43-4ed1e4240646.jpg 320w, https://ugc.futurelearn.com/uploads/assets/14/0b/hero_140bfdc0-2f0f-408d-bf43-4ed1e4240646.jpg 648w, https://ugc.futurelearn.com/uploads/assets/14/0b/large_hero_140bfdc0-2f0f-408d-bf43-4ed1e4240646.jpg 729w, https://ugc.futurelearn.com/uploads/assets/14/0b/large_hero_140bfdc0-2f0f-408d-bf43-4ed1e4240646.jpg 2x"/></p>
</div><p><h2>2.2&emsp;Correlation and multicollinearity </h2><div class="u-typography-bold-intro">
<p>In steps <a href="https://www.futurelearn.com/courses/intro-to-data-analytics/1/steps/661794">3.12</a> In Introduction to Data Analytics and in <a href="https://www.futurelearn.com/courses/pre-processing-data-and-feature-impact-calculation/1/steps/688506">Step 4.9</a>, in Pre-processing Data and Feature Impact Calculation, we discussed correlation and multi-collinearity. Multi-collinearity can be defined as a phenomenon in which one predictor variable in a model can be linearly predicted from the remaining variables.</p>
<p>I appreciate that most of you will feel we are repeating ourselves. However, the impact and conclusions drawn from proposed algorithms can be extremely important if Multi-Collinearity exists in the data. Now many of you might recap what was said in Step 3.12 (mentioned above) about the effect of Multicollinearity can have on our models. In linear regression, it potentially inflates the standard error of some of the parameter estimates. When this happens we are inclined to drop variables that are relevant and keep irrelevant variables. Many of you will say why is this such a big deal, as all we are interested in is making predictions. In theory, this is true. Multi-collinearity doesn’t affect the predictions. But understanding which variables are relevant to our analysis is extremely important. We will now discuss reasons as to why we should deal with it below.</p>
<ul>
<li>Under perfect multicollinearity, Ordinary Least Squares (OLS) estimates simply don’t exist. In OLS the solution we are trying to find the best estimates of <script type="math/tex">\beta</script> to the following equation:</li>
</ul>
<script type="math/tex; mode=display">y=\beta X+\epsilon$ where $\epsilon$ ~ $N(o,\sigma)</script>
<p>The solution to this problem can be shown to be:</p>
<script type="math/tex; mode=display">\hat{\beta}$ = $(X^TX)^{-1}X^Ty</script>
<p>Please note that we have <script type="math/tex">(X^TX)^{-1</script> in this equation. This means that we have to be able to invert the <script type="math/tex">(X^TX)</script>. If any of the independent variables are perfectly collinear with others, then this matrix will not be invertible as the rank of the matrix will be less than the number of dimensions+1.</p>
<p>Many of you might think that the variables will not be perfectly correlated, and you might be right here. Well if there is a “strong” but not perfect collinearity between variables then you probably will find that your model will be quite sensitive to minor changes in the data. This is caused by ill-conditioning and many libraries will struggle to converge. Even if you get your library to work it will most likely come up with the volatile weights which will cause errors with regard to variable importance.</p>
<p>There will be those among you who will think that you are going to use a non-parametric model such as a neural network and this issue won’t matter. Well, I am sorry to burst your bubble, but you will still have issues because neural networks use algorithms such as steppest descent to optimize the cost function. These algorithms struggle to optimize when there are non-orthogonal features in the cost function (perfect orthogonality effectively means we have no multi-collinearity in our input features/weights). The net effect is we may not converge or in some cases, we will end up in a nonlinear “hole”, thus we are less likely to find the global optimum.</p>
<p>Finally, you will come across a lot of material about multi-collinearity on-line. From a data mining perspective, it is very important to understand the amount of multi-collinearity in our data as it will allow us to understand which variables are important in our models and to determine the impact that these variables have with our outcome variables. One should note that in clinical trials and loan approval models authorities such as the FDA or central banks insist on understanding the impact of each variable.</p>
<p>Now were are going to implement the Variance Inflation Factor (VIF) in the next step using the Python “statsmodels” library.</p>
</div><p><h2>2.3&emsp;Measuring multicollinearity</h2><div class="u-typography-bold-intro">
<p>In the Google Colab file in <a href="https://www.futurelearn.com/courses/intro-to-data-analytics/1/steps/661796">Step 3.13</a> in the first course in this program, we have shown how Variance Inflation Factor (VIF) can be used to measure multicollinearity in a feature dataset.</p>
<p>VIF is calculated by regressing an independent variable against all the other variables and using the following formula:</p>
<ul>
<li>
<script type="math/tex; mode=display">X_1=\alpha + X_2+...+X_n</script>
</li>
</ul>
<p>We then get the <script type="math/tex">R^2</script> from each variables regression model and then calculate the VIF using the following equation:</p>
<ul>
<li>
<script type="math/tex; mode=display">V.I.F. = 1 / (1 - R^2).</script>
</li>
</ul>
<p>If your VIF factor is &gt;10 then you really need to drop variables from your model, but if it is between 5-10 then you need to consider it.</p>
<p>Let’s look at the Boston housing data from Scikit Learn in the Google Colab for this step is <a href="https://colab.research.google.com/drive/1EeLbLtxEDobwYJaOWS_-Gdk1DC_ixV1H#scrollTo=cDnQ9YJPz6qo">here</a>.</p>
</div><p><h2>2.4&emsp;Principal component analysis</h2><div class="u-typography-bold-intro">
<p>In <a href="https://www.futurelearn.com/courses/pre-processing-data-and-feature-impact-calculation/1/steps/688506">Step 4.9</a>, in Preprocessing Data and Feature Impact Calculation, we introduced the concept of principal component analysis (PCA) in order to deal with dimensionality reduction.</p>
<p>In this step, we will expand on our discussion of the working of PCA and explain how it can also be used to create new features. We will also explain the background to PCA and what an eigenvalue or eigenvector is as well as how they relate to principal components.</p>
<p>You might ask what is so important about PCAs. Well, they can help us accomplish the following:</p>
<ul>
<li>Data visualization</li>
<li>Data reduction</li>
<li>Data classification</li>
<li>Trend analysis</li>
<li>Factor analysis</li>
<li>Noise reduction</li>
</ul>
<p>PCA is the most common form of factor analysis. Factor analysis is a process where we try to impute latent variables or variables that are not directly observable from our data.</p>
<p>So let’s try and explain in simple terms what we are trying to do with PCA. In PCA we are trying to create a new set of variables that explain the variation in the original dataset subject to the constraint that they are orthogonal (not correlated) to each other. This is an easy concept to understand but the maths behind it is a little tricky. So imagine we have a set of variables <script type="math/tex">X=[x_1,x_2,...,x_n]</script>. Now we are going to find a transformation that creates a new axis that describes the greatest variance in the data subject to the condition that each new variable is orthogonal. We can see in the figure below that we have created a new axis <script type="math/tex">PC_1</script> and <script type="math/tex">PC_2</script> which are orthogonal.</p>
<p><img alt="Graph showing how if we change the transform the axis from the original to PC1 and PC2 that most of the variation will be explained on PC1." sizes="(min-width: 1695px) 729px, (min-width: 680px) 648px, 320px" src="https://ugc.futurelearn.com/uploads/assets/35/14/hero_3514132e-9f01-4e12-b843-777a24adfd80.png" srcset="https://ugc.futurelearn.com/uploads/assets/35/14/small_hero_3514132e-9f01-4e12-b843-777a24adfd80.png 320w, https://ugc.futurelearn.com/uploads/assets/35/14/hero_3514132e-9f01-4e12-b843-777a24adfd80.png 648w, https://ugc.futurelearn.com/uploads/assets/35/14/large_hero_3514132e-9f01-4e12-b843-777a24adfd80.png 729w, https://ugc.futurelearn.com/uploads/assets/35/14/large_hero_3514132e-9f01-4e12-b843-777a24adfd80.png 2x"/></p>
<p>So lets go back a little and do a recap on what a covariance matrix is. The covariance between two variables <script type="math/tex">x</script> and <script type="math/tex">y</script> can be given by the following equation:</p>
<ul>
<li>
<script type="math/tex; mode=display">\sigma(x,y)=\frac{1}{n-1}\sum_{i=1}^n(x_i-\bar{x_i})(y_i-\bar{y_i})</script>
</li>
</ul>
<p>now if we center each variable around their means, with the following transformation:</p>
<ul>
<li>
<script type="math/tex; mode=display">\hat{x}=x_i-\bar{x_i}</script>
</li>
</ul>
<p>then the covariance between <script type="math/tex">\hat{x}</script> and <script type="math/tex">\hat{y}</script> can be described as follows:</p>
<ul>
<li>
<script type="math/tex; mode=display">\sigma(\hat{x},\hat{y})=\frac{1}{n-1}\sum_{i=1}^n(\hat{x_i})(\hat{y_i})\approx\hat{x}\hat{y}</script>
</li>
</ul>
<p>Now if we center all the variables in <script type="math/tex">X</script> above we can describe the covariance matrix as follows:</p>
<ul>
<li>
<script type="math/tex; mode=display">\sigma = \hat{X}.\hat{X^T}</script>
</li>
</ul>
<p>Now we want to transform our <script type="math/tex">\hat{X}</script> matrix with new vectors <script type="math/tex">B</script> to give us a new feature matrix <script type="math/tex">B\hat{X}</script>. We also want to maximise the variance explained by new <script type="math/tex">B\hat{X}</script> subject to them being orthogonal. To have this condition then <script type="math/tex">BB^T</script> will have to be the identify matrix, which confirms that we will have no colinearity in the <script type="math/tex">B</script> vectors and the new vectors <script type="math/tex">B\hat{X}</script> will be orthogonal.</p>
<p>We can accomplish all of this by minimising <script type="math/tex">B^T \hat{X}.\hat{X^T}B</script> subject to <script type="math/tex">B^TB=I</script>.</p>
<p>We can do this using a method called <a href="http://mathonline.wikidot.com/the-method-of-lagrange-multipliers">Lagrange Multipliers</a> which states that if we optimize the following equation with respect to <script type="math/tex">B</script>:</p>
<ul>
<li>
<script type="math/tex; mode=display">B^T \hat{X}.\hat{X^T}B -\lambda B^TB=0</script>
</li>
</ul>
<p>and using partial differentiation we will get</p>
<ul>
<li>
<script type="math/tex; mode=display">\hat{X}.\hat{X^T}B =\lambda B</script>
</li>
</ul>
<p>Those of you who remember a little linear algebra will notice that this equation is the same as the derivation of two terms known as the eigenvalue <script type="math/tex">\lambda</script> and the eigenvectors <script type="math/tex">B</script> of the covariance matrix <script type="math/tex">\hat{X}.\hat{X^T}</script>. Now the great thing about all of this is the largest eigenvalue and the eigenvector corresponding to it multiplied by the <script type="math/tex">\hat{X}</script> matrix will give you the first component or the component that describes the most variation and the second component describes the second highest amount of the variation, and so on. I have regularly found datasets with 50 or more variables where the first component accounts for over 50% of the overall variation within the data. You will also find that the proportion of each eigenvalues to the total sum of the eigenvalues gives you the % explained by each component or:</p>
<ul>
<li>% Explained by <script type="math/tex">PC_i=\frac{\lambda_i}{\sum_{i=1}^n\lambda_i}*100</script></li>
</ul>
<p><img alt="Barchart showing the variation explained by each principal component" sizes="(min-width: 1695px) 729px, (min-width: 680px) 648px, 320px" src="https://ugc.futurelearn.com/uploads/assets/c9/5d/hero_c95de44f-7e49-4e89-87ea-2e3781720b74.png" srcset="https://ugc.futurelearn.com/uploads/assets/c9/5d/small_hero_c95de44f-7e49-4e89-87ea-2e3781720b74.png 320w, https://ugc.futurelearn.com/uploads/assets/c9/5d/hero_c95de44f-7e49-4e89-87ea-2e3781720b74.png 648w, https://ugc.futurelearn.com/uploads/assets/c9/5d/large_hero_c95de44f-7e49-4e89-87ea-2e3781720b74.png 729w, https://ugc.futurelearn.com/uploads/assets/c9/5d/large_hero_c95de44f-7e49-4e89-87ea-2e3781720b74.png 2x"/></p>
<p>You can see from the figure above that the amount of variance explained diminishes with the number of components. You do lose some information, but if the eigenvalues are small, you don’t lose much.  So if you start with <script type="math/tex">n</script> dimensions you will probably end up with considerably less as a small number of components can explain the variance of a high dimensional dataset.</p>
<p>We mentioned this in the previous course, Preprocessing Data and Feature Impact Calculation, but I will repeat it again. <strong>Don’t use PCA on categorical data</strong>. It really is not appropriate. You can use a technique called multiple correspondence analysis to do this and we will look at this in <a href="https://www.futurelearn.com/courses/feature-engineering/1/steps/712754">Step 2.7</a> of this topic.</p>
<p>If you are finding this a little difficult, have a look at a tutorial from <a href="http://www-labs.iro.umontreal.ca/~pift6080/H09/documents/papers/pca_tutorial.pdf">Lindsay Smith</a> or this one from medium.com by <a href="https://medium.com/@aptrishu/understanding-principle-component-analysis-e32be0253ef0">Rishav Kumar</a>.</p>
<p>We will now look a very simple example of how we calculate the PCA’s in the next step.</p>
</div><p><h2>2.5&emsp;Implementing a PCA analysis from first principles</h2><div class="u-typography-bold-intro">
<p>We are now going to complete an example of how you would implement the steps of a principal component analysis (PCA). There are routines in Python or R that would allow you to do it automatically, but working it out from first principles is a really good exercise.</p>
<p>Let’s get started by following this <a href="https://colab.research.google.com/drive/1VEelHRZwNpDp_1yqbJHQoMUD6lLzt6Hs#scrollTo=dyLOHeFPEYz9">link</a> to go to the Google Colab for this step.</p>
</div><p><h2>2.6&emsp;Independent component analysis</h2><div class="u-typography-bold-intro">
<p>Imagine you are a spy at an embassy cocktail party and in attendance are various VIPs and diplomats.</p>
<p>Now, there are two major groups at the party, the first group is the Americans and the second is the Russians. If you were trying to record these conversations with one microphone you would get extremely distorted sound quality. You may think I can just put two microphones in the room and you would be correct, but you will still get levels of noise from each conversation. An additional problem is that you will also not know where to put the microphones exactly.</p>
<p>You can see in Fig 1 below that each source is mixed together and the outputs in <script type="math/tex">s_1</script> and <script type="math/tex">S_2</script> and mixed by some matrix A. So this is where Independent Component Analysis (ICA) comes in. It helps filter conflicting sources into their original source.</p>
<p><img alt="Figure of Cocktail party example of latent sources (s1 and s2" sizes="(min-width: 1695px) 729px, (min-width: 680px) 648px, 320px" src="https://ugc.futurelearn.com/uploads/assets/1c/58/hero_1c58868a-1247-4ae7-93f2-217ed9bee67f.png" srcset="https://ugc.futurelearn.com/uploads/assets/1c/58/small_hero_1c58868a-1247-4ae7-93f2-217ed9bee67f.png 320w, https://ugc.futurelearn.com/uploads/assets/1c/58/hero_1c58868a-1247-4ae7-93f2-217ed9bee67f.png 648w, https://ugc.futurelearn.com/uploads/assets/1c/58/large_hero_1c58868a-1247-4ae7-93f2-217ed9bee67f.png 729w, https://ugc.futurelearn.com/uploads/assets/1c/58/large_hero_1c58868a-1247-4ae7-93f2-217ed9bee67f.png 2x"/></p>
<p>ICA (also known as Blind Signal Separation) allows us to identify &amp; separate a mixtures of sources with little prior information. There are many applications and these include the following:</p>
<ul>
<li>Audio Processing</li>
<li>Medical data</li>
<li>Finance</li>
<li>Array processing (beamforming)</li>
<li>Coding</li>
</ul>
<p>and most applications where Factor Analysis and PCA is currently used.</p>
<p>While PCA seeks directions that represents data best, ICA seeks such directions that are mostly independent from each other.</p>
<p>In this step, We are going to rely on notes written by <a href="https://github.com/akcarsten/Independent_Component_Analysis">Klein Carsten</a>. I, however, use Sklearn FastICA instead of Klein’s method, as Klein’s version is quite long. But, I would suggest you implement and compare the results.</p>
<p>As we saw from fig 1 ICA assumes we generate the observed data from an underlying unknown independent process <script type="math/tex">x</script> with mixing signals <script type="math/tex">A</script>.</p>
<ul>
<li>
<script type="math/tex; mode=display">x=As</script>
</li>
</ul>
<p>You can access the Google Colab for this step by following this <a href="https://colab.research.google.com/drive/1IZcvCMklk2QOQSeNby0Y2fvviF_S4dux#scrollTo=zDECGFm-Jor-">link</a>.</p>
</div><p><h2>2.7&emsp;Multiple correspondence analysis</h2><div class="u-typography-bold-intro">
<p>In this step we will discuss correspondence analysis.</p>
<p>When we have contingency tables such as the one shown in Table 1 below we are not just interested in the relationship between the two factors but also the inherent components that exist within it. The values that are shown in a contingency table are cross-tabulations of dummy variables that have been derived from categorical factors.</p>
<p><img alt="Table with 8 rows. First row three columns central column titled smoking category. Second row 6 columns, text: staff group, (1) None, (2) light, (3) medium, (4) Heavy, Row totals. Third row, 6 columns (1) Senior Managers,  4, 2, 3, 2, 11, fourth row, six columns, text: Junior Managers 4, 3, 7, 4, 18. Fifth row six columns, text: Senior Employees, 25, 10, 12, 4, 51, sixth row, 5 columns, text: (4) Junior Employees, 18, 24, 33, 13, 88. Seventh row, 5 columns, text: (5) Secretaries, 10, 6, 7, 2, 25, Eight row, 5 columns text column totals, 61, 45, 62, 25, 193) " src="https://www.computing.dcu.ie/~amccarren/mcm_images/MCA_Smoking_data.png"/>
<sub>Table 1 (Greenacre (1984, p. 55)</sub></p>
<p>Now you might remember we used the <script type="math/tex">\chi^2</script> distribution on contingency tables to determine if there was a relationship between two factors such as those given in table 1, but it did not tell us what the components are, or the contribution that explained the relationship between rows. Recall that we used principal component analysis on continuous variables to both reduce the dimensionality, handle correlation and tell us the contribution each row had to a factor (eigenvectors). Correspondence analysis (Multi) can be described as a descriptive/exploratory technique designed to analyze simple two-way and multi-way tables containing some measure of correspondence between the rows and columns. The results allow one to explore the structure of categorical variables included in the table.</p>
<p>In a typical <a href="https://en.wikipedia.org/wiki/Correspondence_analysis">correspondence analysis</a>, a crosstabulation table of frequencies is first standardized, so that the relative frequencies across all cells sum to 1.0. One way to state the goal of a typical analysis is to represent the entries in the table of relative frequencies in terms of the distances between individual rows and/or columns in a low-dimensional space. This is best illustrated by a simple example, which will be described below. There are several parallels in interpretation between correspondence analysis and factor analysis, and some similar concepts will also be pointed out below.</p>
<p>For a comprehensive description of this method, computational details and its applications refer to the classic by Greenacre (1984). These methods were originally developed primarily in France by Jean-Paul Benzécri in the early 1960s and 1970s (see Benzécri, 1973).</p>
<h3 id="multiple-correspondence-analysis-mca">Multiple Correspondence Analysis (MCA)</h3>
<p>Multiple correspondence analysis (MCA) originates from correspondence analysis. In correspondence analysis, we predominantly work with two factors but this is extended to multiple factors in MCA.</p>
<p>In the next step, we will demonstrate how MCA can be implemented on a toy dataset. Another, more complex example can be found <a href="http://vxy10.github.io/2016/06/10/intro-MCA/">here</a>. This example uses a 3-dimensional categorical table and also calculates all the measurements from scratch and uses the Python MCA Library.</p>
<hr/>
<p><strong>Reference:</strong></p>
<p>Greenacre, M.J., 1984. <em>Theory and Applications of Correspondence
Analysis</em>. London: Academic Press.</p>
<p>Benzécri, J.P., 1973. L’analyse des données [Data analysis]. Volume II. <em>L’Analyse des Correspondances [Correspondence Analysis]</em>. Paris, France: Dunod.</p>
</div><p><h2>2.8&emsp;Multiple Correspondence Analysis Example</h2><div class="u-typography-bold-intro">
<p>In the example provided in <a href="https://colab.research.google.com/drive/13xofqepzkAf7z4GirICWDVubIV8YiIZW#scrollTo=_OEIXGQ4vDC3">this Google Colab</a>, we are using a toy dataset known as the <a href="https://drive.google.com/open?id=1HXSdDFY9ZUPZTbMNUrZlrOJLeOV8JvK7">balloons dataset</a> which was taken from <a href="https://archive.ics.uci.edu/ml/index.php">UCI datasets</a>. This dataset follows the most common format for categorical variables.</p>
<p>Please post any questions you may have about the MCA example in the comment section below.</p>
</div><p><h2>2.9&emsp;Factor analysis of mixed data</h2><div class="u-typography-bold-intro">
<p>In the previous four steps, we have examined how we can use principal component and multiple correspondence analysis to help us search for latent factors for both continuous and categorical data respectively. The problem we have is that most of our datasets have both categorical and quantitative data.</p>
<p>Factor Analysis of Mixed Data (FAMD) is a method dedicated to exploring data with both continuous and categorical variables. It can be seen roughly as a mix between PCA and Multiple Correspondence Analysis (MCA). More precisely, the continuous variables are scaled to unit variance and the categorical variables are transformed into a set of dummy variables. They are then scaled using the specific scaling of MCA. This ensures to balance the influence of both continuous and categorical variables in the analysis. It means that both variables are on an equal foot to determine the dimensions of variability. This method allows one to study the similarities between individuals taking into account mixed variables and to study the relationships between all the variables. It also provides graphical outputs such as the representation of the individuals, the correlation circle for the continuous variables and representations of the categories of the categorical variables. It also provides specific graphs to visualize the associations between both type of variables, <a href="https://www.rdocumentation.org/packages/FactoMineR/versions/2.0/topics/FAMD">RDocumentation</a>.</p>
<p>Now the issue we have is that the libraries that are available in Python are not as well established as those provided by R, the statistical programming language. You can find out how to install R <a href="https://cran.r-project.org/bin/windows/base/">here</a>, and this <a href="http://www.sthda.com/english/articles/22-principal-component-methods-videos/72-famd-in-r-using-factominer-quick-scripts-and-videos/#course-video">video</a> shows you how to do FAMD using the R FactoMineR library. There is an implementation of a FAMD using both Python <a href="https://pypi.org/project/light-famd/#factor-analysis-of-mixed-data-famd">“light-FAMD”</a> and “prince”.</p>
<p>You can access the Google Colab for this step by following this link <a href="https://drive.google.com/open?id=1DlT9WF3ZbDY3AO3bmursO18jLk2WceS-">here</a>.</p>
</div><p><h2>2.11&emsp;Review of Topic 2</h2><div class="u-typography-bold-intro">
<p>Well done on finishing Topic 2 of Feature Engineering. We hope that you are enjoying the course so far.</p>
<p>This topic looked at the process of using existing features or other relevant domain knowledge information to create new features to help analysts visualise their data better, reduce their dataset and improve their algorithm’s predictive power. There is a wide range of techniques available and we will give students a flavour of the most popular.</p>
<p>Having completed this topic you should now be able to:</p>
<ol>
<li>Define what feature engineering is and implements a wide range of methods that are typically used in data analytics projects.</li>
<li>Understand the impact that multicollinearity has on analysis.</li>
</ol>
<p>In the next topic, we will delve deeper into feature engineering. Specifically, we will look at machine learning algorithms that can also be used as data reduction or feature engineering techniques.</p>
<p>Please feel free to post any questions or thoughts that you may have about what we have covered in this section of the course in the comments section below.</p>
<p>See you in the next topic.</p>
</div><p><h2>2.12&emsp;Jump to Topic 3</h2><div class="u-typography-bold-intro">
<p>Follow this <a href="https://www.futurelearn.com/courses/feature-engineering/1/todo/69228">link</a> to go to Topic 3.</p>
</div><p style="page-break-before: always"><h2>3.1&emsp;Welcome to Topic 3</h2><div class="u-typography-bold-intro">
<p>Welcome to Topic 3 of Feature Engineering.</p>
<p>In Topic 2 we introduced the most popular processes in feature engineering and explored some of the traditional techniques used in feature engineering.</p>
<p>In this topic, we will delve further into feature engineering by looking at machine learning algorithms. Specifically, Topic 3 will describe how these algorithms can be used in data reduction.</p>
<p>By the end of this topic, you will understand:</p>
<ul>
<li>What a random forest is.</li>
<li>How a random forest can be used as a feature engineering technique.</li>
<li>What an autoencoder is and the various offshoots of this algorithm.</li>
<li>The way in which autoencoders can reduce the dimensionality of datasets and improve predictive power.</li>
</ul>
<p>As with previous topics, we encourage you to make use of the comments section below to interact with your fellow learners and lead educator.</p>
<p>We have a lot to cover so let’s get to work.</p>
<p><img alt="Statement of funding from Skillnet Ireland informing that: This course has been grant-aided by skillnet Ireland. Technology Ireland ICT Skillnet provides advanced training and development activities for technical and engineering staff in the ICT. Participants may be contacted by Skillnet Ireland to confirm that they attend this course.  
Information from susan.kelly@ictskillnet.com" sizes="(min-width: 1695px) 729px, (min-width: 680px) 648px, 320px" src="https://ugc.futurelearn.com/uploads/assets/14/0b/hero_140bfdc0-2f0f-408d-bf43-4ed1e4240646.jpg" srcset="https://ugc.futurelearn.com/uploads/assets/14/0b/small_hero_140bfdc0-2f0f-408d-bf43-4ed1e4240646.jpg 320w, https://ugc.futurelearn.com/uploads/assets/14/0b/hero_140bfdc0-2f0f-408d-bf43-4ed1e4240646.jpg 648w, https://ugc.futurelearn.com/uploads/assets/14/0b/large_hero_140bfdc0-2f0f-408d-bf43-4ed1e4240646.jpg 729w, https://ugc.futurelearn.com/uploads/assets/14/0b/large_hero_140bfdc0-2f0f-408d-bf43-4ed1e4240646.jpg 2x"/></p>
</div><p><h2>3.2&emsp;Random forest feature selection</h2><div class="u-typography-bold-intro">
<p>In this step, we are going to outline how the random forest machine learning approach can help in determining the importance of a feature.</p>
<p>Random forests are one of the most popular machine learning algorithms. They are so successful because, in general, they provide good predictive performance, low overfitting, and easy interpretability. This interpretability is given by the fact that it is straightforward to derive the importance of each variable on the decision tree. In other words, it is easy to compute how much each variable is contributing to the decision. In the Scikit-learn random forest library, the relative rank (i.e. depth) of a feature used as a decision node in a tree can be used to assess the relative importance of that feature with respect to the predictability of the target variable. Features used at the top of the tree contribute to the final prediction decision of a larger fraction of the input samples. The expected fraction of the samples they contribute to can thus be used as an estimate of the relative importance of the features. For further reading on this please follow this <a href="https://scikit-learn.org/stable/modules/ensemble.html#forest">link</a>.</p>
<p>Random forests are also incredibly robust and generally are very easy to implement. In every project I give, students always use random forests for predictions. Lately, I have also found that the students use them for dimensionality reduction. However, there is an issue with them that Sebastian Raschka explains really nicely below:</p>
<p>“The random forest technique comes with an important gotcha that is worth mentioning. For instance, if two or more features are highly correlated, one feature may be ranked very highly while the information of the other feature(s) may not be fully captured. On the other hand, we don’t need to be concerned about this problem if we are merely interested in the predictive performance of a model rather than the interpretation of feature importances.” - Python Machine Learning by <a href="https://sebastianraschka.com/books.html">Sebastian Raschka</a>.</p>
<p>This “gotcha” is really important to understand and in a way negates the use of random forests for feature importance and thus the use of them for feature reduction and engineering.</p>
<p>Have a look at the code below and play about with the standardisation methods from Scikit learn. The description of how the random forest classifier works can be found <a href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html?highlight=randomforest#sklearn.ensemble.RandomForestClassifier">here</a>.</p>
<p>Please follow this <a href="https://colab.research.google.com/drive/1exeOqpBaLZ1SjbiedkcENx8uiRiwcS5r#scrollTo=UsIyHV_L6_7r">link</a> to go to the Google Colab for this step.</p>
<hr/>
<p><strong>References:</strong></p>
<p>Raschka, S., 2015. <em>Python machine learning</em>. Third Edition, Packt Publishing Ltd.</p>
</div><p><h2>3.3&emsp;Neural Networks</h2><div class="u-typography-bold-intro">
<p>In this step, we will explore Neural Networks.</p>
<p>In previous courses, we learned how to use principal component analysis (PCA), Lasso and Ridge regression to reduce the dimensionality of datasets. One of the underlying assumptions with these methods is that the compression or dimensionality reduction is linear in nature. PCA effectively creates new latent variables that are new representations of the original data but of a significantly lower dimension. These techniques can be very useful when dealing with variables that are highly correlated. They can also be useful when you just want to simply reduce the volume of your input variables in order to improve the processing speed of your algorithms.</p>
<p>So by now, you may have come across the concept of Neural Networks. If you haven’t then had a look at this <a href="https://pathmind.com/wiki/neural-network#define">site</a>. Neural Networks are algorithms that are loosely modelled on the human brain. The concept can be best described in Figure 1 below.</p>
<p><img alt="An artificial neural network with one single hidden layer" src="https://www.computing.dcu.ie/~amccarren/mcm_images/neural_network.png"/></p>
<p><sub> (Figure 1: <a href="https://en.wikipedia.org/wiki/">An artificial neural network  with one single hidden layer</a>)</sub></p>
<p>You will definitely come across neural networks in other courses. They can be used to build prediction models. They can also be used as part of autoencoder models to reduce datasets and remove noise from our datasets.</p>
<p>The principle behind them is that the input or our <script type="math/tex">X</script> data is passed into the system through the input nodes. An input function is applied and this information is then transferred to the hidden nodes, where an activation function transfers that information to the output function, see Figure 2.</p>
<p><img alt="alt: Data transformation process in a Neural Network" src="https://www.computing.dcu.ie/~amccarren/mcm_images/perceptron_node.png"/></p>
<p><sub>(Figure 2: <a href="https://pathmind.com/wiki/neural-network#define">Neural Network Elements</a>)</sub></p>
<p>The objective is to minimize the difference between the outputs and the results of the activation function, by optimising the input and hidden node weights. Normally, optimization is done using algorithms such as <a href="https://en.wikipedia.org/wiki/Gradient_descent">gradient descent</a>. Neural Networks will introduce a level of non-linearity and complexity into our data models, thus allowing for greater predictive power. The architecture of these systems can vary with the number of nodes in the hidden layer and the extension to deep learning architectures can be achieved through the addition of multiple hidden layers. In reality, this process will be equivalent to a linear regression model if we remove the hidden nodes and apply a linear activation function.</p>
<p>Now we are going to implement a simple neural network regressor on the Boston housing dataset. It is important that you understand this simple example as it will help you  build an autoencoder in the next step.</p>
</div><p><h2>3.4&emsp;Introduction to autoencoders</h2><div class="u-typography-bold-intro">
<p>In the previous step, we described how methods such as neural networks or linear regression could be used to make predictions. We also discussed in previous steps how principal components could be used to both reduce dimensionality and help remove noise.</p>
<p>Autoencoders are a form of unsupervised technique which use neural networks to help reduce dimensionality or remove noise by compressing our input data through a hidden layer back to an original representation of the input layer. Figure 1 shows how the network decompresses the data before recreating it in the output layer.</p>
<p><img alt="Graphic illustrating a network decompressing data before recreating it in the output layer" src="https://www.computing.dcu.ie/~amccarren/mcm_images/autoencoder.png"/></p>
<p><sub>(Figure 1: <a href="https://www.jeremyjordan.me/autoencoders/">JeremyJordan</a>)</sub></p>
<p>We examine this process a little more by looking at Figure 2 where we see there is an “Encoder” and “Decoder” used to create “h” the latent representation. The latent representation can then be used as a decompressed or dimensionally reduced dataset.</p>
<p><img alt='Graphic illustration the “Encoder" and "Decoder" used to create "h" latent representation which is then used to as a decompressed or dimensionally reduced dataset' src="https://www.computing.dcu.ie/~amccarren/mcm_images/Autoencoder_2.png"/></p>
<p><sub>(Figure 2: <a href="https://towardsdatascience.com/deep-inside-autoencoders-7e41f319999f">Towards Data Science</a>)</sub></p>
<p>Autoencoders are only useful when there are relationships between the input variables, as mathematically it is difficult to compress the features without substantial loss of information if there was no correlation between the variables. However, if there is a relationship then the structure can be learned and we can effectively compress our data into a hidden layer. The outcome variable is a transformed version of the hidden nodes. Thus the output variables can be considered to be denoised.</p>
<p>Now we have outlined that autoencoders can be used for dimensionality reduction as we will be creating a lower number of latent variables than the number of original input variables. This is similar to principal component analysis (PCA). So what are the differences?</p>
<ul>
<li>
<p>PCA is a linear transformation of the data and assumes the new latent variables are a linear combination of the original variables.</p>
</li>
<li>
<p>PCA features are not linearly correlated but autoencoders might have correlations.</p>
</li>
<li>
<p>PCA is less computationally intensive than autoencoders.</p>
</li>
<li>
<p>A single-layered autoencoder with a linear activation function is very similar to PCA.</p>
</li>
<li>
<p>Autoencoders may require regularization as they are prone to overfitting.</p>
</li>
</ul>
<p>Please go to this <a href="https://drive.google.com/open?id=1kcgQt7ucinl5DigW-_Iob755XIHuh73P">link</a> to access the Google Colab file for this step.</p>
</div><p><h2>3.5&emsp;Sparse autoencoders</h2><div class="u-typography-bold-intro">
<p>In the previous step, the input layer was constrained by the size of the hidden layer (128). In such a situation, what typically happens is that the hidden layer is learning an approximation of principal component analysis(PCA).</p>
<p>Another way to constrain the representations to be compact is to add a sparsity constraint on the activity of the hidden representations, so fewer units would “fire” at a given time. In <a href="https://keras.io/">Keras</a>, this can be done by adding an activity_regularizer to our Dense layer. The code below outlines how this works. In particular, look at the line in the code that specifies the regularization:</p>
<ul>
<li>“activity_regularizer=regularizers.l1(10e-5)”</li>
</ul>
<p>Remember L1 regularization constraints the cost function with an absolute value of the magnitude of the weights</p>
<ul>
<li><script type="math/tex">\lambda \sum_{j=1}^p\lvert \beta_j \rvert</script> where <script type="math/tex">p</script> is the number of weights</li>
</ul>
<p>L1 regularization shrinks the less important feature’s coefficient to zero thus, removing some feature altogether.</p>
<p>In the following example we use the  <a href="https://towardsdatascience.com/understanding-binary-cross-entropy-log-loss-a-visual-explanation-a3ac6025181a">“binary_crossentropy”</a> loss or cost function. This term effectively tries to maximise the Log-loss function and is very similar to the entropy calculations we did when attempting to discretize a continuous variable. In this approach we are trying to match pixels rather than match the intensity of them which the “mean_squared_error” does.</p>
<p>Now when you print out the hidden feature set you will notice that a number of them are zero. This means we have introduced sparsity into the autoencoder.</p>
<p>Have a go at changing the loss function and changing the optimizers in the following <a href="https://colab.research.google.com/drive/1_7bPYNBIhyuyb-Ef6ljv7jJtlbWpof3d#scrollTo=GC42QRJn0oZ6">Google Colab</a>. See how your results change.</p>
<p>Share your thoughts in the comment section below.</p>
</div><p><h2>3.6&emsp;Convolutional autoencoders</h2><div class="u-typography-bold-intro">
<p>In the next course, Processing Unstructured Data, we will talk about image pre-processing.</p>
<p>Specifically, we will look at how we can use terms known as convolutions, max pooling and upsampling to help us pre-process an image. Some of you may be familiar with these terms so this step will be relatively easy to extend from the regular autoencoder. If you are not familiar with them then go to this <a href="https://pathmind.com/wiki/convolutional-network">link</a>.</p>
<p>In <a href="https://www.futurelearn.com/courses/feature-engineering/1/steps/713049">Step 3.4</a> we outlined how an autoencoder works. This process is very similar for convolutional autoencoders. Figure 1 shows the structure of a Convolutional Neural Network (CNN) autoencoder. Convolutional Autoencoders can be used to reduce the levels of noise found in images and can be a very useful pre-processing step.</p>
<p><img alt="Diagram of convolutional autoencoders reducing level of noise found in images with encoder and decoder" sizes="(min-width: 1695px) 729px, (min-width: 680px) 648px, 320px" src="https://ugc.futurelearn.com/uploads/assets/92/95/hero_92957b5f-587b-4990-b5cb-9466098b1f9e.png" srcset="https://ugc.futurelearn.com/uploads/assets/92/95/small_hero_92957b5f-587b-4990-b5cb-9466098b1f9e.png 320w, https://ugc.futurelearn.com/uploads/assets/92/95/hero_92957b5f-587b-4990-b5cb-9466098b1f9e.png 648w, https://ugc.futurelearn.com/uploads/assets/92/95/large_hero_92957b5f-587b-4990-b5cb-9466098b1f9e.png 729w, https://ugc.futurelearn.com/uploads/assets/92/95/large_hero_92957b5f-587b-4990-b5cb-9466098b1f9e.png 2x"/></p>
<p><sub>(Figure 1: CNN Autoencoder, Guo et al. (2017))</sub></p>
<p>As you can see from Figure 1, it is very similar to regular autoencoder, with the exception that it has multiple dimensions. I won’t be going into to much detail here as you will learn all about CNN’s in Machine learning with Prof Tomas Ward. However, I want to run through an example of how a convolutional autoencoder works using a simple example.</p>
<p>Now we will use the Modified National Institute of Standards and Technology (MNIST) dataset from the Keras dataset library, as we did in previous examples, but this time we will add some noise to each image. Make sure you set TensorFlow to version 2.x and open a folder that you want to work in.</p>
<p>You can access the Google Colab for this step <a href="https://colab.research.google.com/drive/159C7nV_Vmdr-OffKF3hx0Ltfyyp8kl8F#scrollTo=cQa9w37LmkG7">here</a>.</p>
<hr/>
<p><strong>References:</strong></p>
<p>Guo, X., Liu, X., Zhu, E. and Yin, J., 2017. <em>Deep clustering with convolutional autoencoders</em>. In International conference on neural information processing (November, 373-382). Springer, Cham.</p>
</div><p><h2>3.7&emsp;Variational autoencoding</h2><div class="u-typography-bold-intro">
<p>Variational Autoencoders belongs to the general family of autoencoders but are less similar to the approaches we came across in earlier steps of this topic.</p>
<p>Before we move on, it might be worthwhile looking at the following video below:</p>
<p></p><div><div class="u-responsive-embed">

</div>
<p class="u-italic">
  This is an additional video, hosted on YouTube.
</p>
</div>
<p>It provides a nice recap of autoencoders and gives a good description of variational autoencoders. Alternatively, if you are really struggling with the whole concept of autoencoders have a look at this article from <a href="https://towardsdatascience.com/the-variational-autoencoder-as-a-two-player-game-part-i-4c3737f0987b">Towards Data Science</a>. It provides a concise and effective overview of the whole area.</p>
<p>In the first autoencoder example in <a href="https://www.futurelearn.com/courses/feature-engineering/1/steps/713049">Step 3.4</a>, we mapped the original data back to itself through a <em>fixed</em> function. We did this to try and reduce the dimensionality of our dataset and to reduce noise, as shown in the previous step. The mapping can be shown as follows:</p>
<ul>
<li>
<script type="math/tex; mode=display">X->f(x)->X'</script>
</li>
</ul>
<p>where <script type="math/tex">f(X)</script> is a reduced feature set of the original <script type="math/tex">X</script>.</p>
<p>Notice how the latent variables have fixed values (these attributes are fictional) and how “regimented” this process is. If we put a new image into the autoencoder we would get fixed values from the <script type="math/tex">f(X)</script> function. In fact, it will only really work with the dataset that we have and will not really create realistic features if new data is introduced. In a way what we are looking for is smooth latent state representations of the input data. The following two figures try to highlight the differences between regular autoencoders and variational autoencoders. They are taken from a blog by <a href="https://www.jeremyjordan.me/variational-autoencoders/">Jeremy Jordon</a> and hopefully will give you an intuitive interpretation of the difference between them. Now if you look at Figure 1 below, you will see that from the image inputted you will get a latent feature with a fixed value.</p>
<p><img alt="Starting at the left of Image: Mans face with arrow to the right leading to latent attribute box. Box contains following information: Smile: 0.99, Skin tone: 0.85, Gender: -0.73, Beard: 0.85, Glasses; 0.002 and Hair color: 0.68. To the right of this box, an arrow labelled decoder leads to the same image of man’s face" src="https://www.computing.dcu.ie/~amccarren/mcm_images/autoencoder_3.png"/>
<sub>(Figure 1: A typically blocked layer interpretation of a convolutional autoencoder)</sub></p>
<p>Now intuitively if you or I were looking at this picture we might score the smile with a value but with some variation. So we might say this is a smile score of <script type="math/tex">\bar{x} \pm \sigma</script>. In Figure 2 we can see there are varying sureties of the degree of smile occurring.</p>
<p><img alt="Graph illustrating the discrete value of four images and the smile probability distribution for these four images" src="https://www.computing.dcu.ie/~amccarren/mcm_images/variaiotnal_autoencoder_2.png"/>
<sub>(Figure 2: Probability distribution of a latent feature (smile))</sub></p>
<p>So now we can create an encoder and decoder that have as the bottleneck not a fixed function but a probability distribution. Figure 3 shows how the introduction of a probabilistic encoder gives a nice example of how an image goes through a typical variational encoder.</p>
<p><img alt="An example of how an image is processed through a variational autoencoder" src="https://www.computing.dcu.ie/~amccarren/mcm_images/variational_autoencoder.png"/></p>
<p><sub>(Figure 3: <a href="https://lilianweng.github.io/lil-log/2018/08/12/from-autoencoder-to-beta-vae.html">Variational autoencoder architecture</a>)</sub></p>
<p>Now, the benefits of this type of encoder are that it will not just create latent vectors but it will also allow you to fill in gaps or missing sections of data. It has a lot of similarities with Generative Adversarial Networks (GANs) although the structures can be said to be reversed. A Variational Autoencoder Architecture (VAE) reduces its hidden features while GANs expand on them. According to Towards Data Science, “a variational autoencoder can be defined as being an autoencoder whose training is regularised to avoid overfitting and ensure that the latent space has good properties that enable generative process”. So we now have an algorithm that can potentially generate content and could be considered as a missing value imputation technique.</p>
<p>The following code comes from <a href="https://github.com/fchollet/keras/blob/master/examples/variational_autoencoder.py">Github</a> and was written by the Keras team. It was based on the original work from <a href="https://arxiv.org/abs/1312.6114">Kingma, Diederik P., and Max Welling, “Auto-Encoding Variational Bayes.”</a>. I have adapted it to work on Google Colab. You will notice I have commented the pieces out where arguments are expected from the command line.</p>
<p><strong>Note</strong></p>
<p>An isotropic gaussian is one where the covariance matrix is represented by the simplified matrix:
* <script type="math/tex">\Sigma=\sigma^2I</script></p>
<p>Where <script type="math/tex">\Sigma</script> a diagonal matrix and each diagonal element is equal to <script type="math/tex">\sigma</script> implying that all dimensions are independent.</p>
<p>Please go to the following <a href="https://colab.research.google.com/drive/1LdkiXPDAGyfnf90XIPl-9CcUbSFgjzoq#scrollTo=uYPqkg7R_8Wf">Google Colab</a> for this step now.</p>
</div><p><h2>3.9&emsp;Review of Topic 3</h2><div class="u-typography-bold-intro">
<p>Congratulations on making so much progress on the course so far.</p>
<p>Topic 3 focused predominantly on machine learning algorithms. In particular, it looked at Random Forest and at various types of autoencoders to produce new variables. We learnt that these variables, in turn, reduce the dimensionality of datasets and also improve predictive power.</p>
<p>Now that you have completed this topic you should be able to:</p>
<ul>
<li>Implement a Random Forest as a feature engineering technique.</li>
<li>Understand what an autoencoder is and the various offshoots of this algorithm.</li>
</ul>
<p>Please feel free to ask questions or post any comments you may have about any of the content that we have covered under this topic in the comments section at the bottom of this step.</p>
</div><p><h2>3.10&emsp;Jump to Topic 4</h2><div class="u-typography-bold-intro">
<p>Please follow this <a href="https://www.futurelearn.com/courses/feature-engineering/1/todo/69229">link</a> to go to the next Topic in Feature Engineering.</p>
<p><img alt="Statement of funding from Skillnet Ireland informing that: This course has been grant-aided by skillnet Ireland. Technology Ireland ICT Skillnet provides advanced training and development activities for technical and engineering staff in the ICT. Participants may be contacted by Skillnet Ireland to confirm that they attend this course.  
Information from susan.kelly@ictskillnet.com" sizes="(min-width: 1695px) 729px, (min-width: 680px) 648px, 320px" src="https://ugc.futurelearn.com/uploads/assets/14/0b/hero_140bfdc0-2f0f-408d-bf43-4ed1e4240646.jpg" srcset="https://ugc.futurelearn.com/uploads/assets/14/0b/small_hero_140bfdc0-2f0f-408d-bf43-4ed1e4240646.jpg 320w, https://ugc.futurelearn.com/uploads/assets/14/0b/hero_140bfdc0-2f0f-408d-bf43-4ed1e4240646.jpg 648w, https://ugc.futurelearn.com/uploads/assets/14/0b/large_hero_140bfdc0-2f0f-408d-bf43-4ed1e4240646.jpg 729w, https://ugc.futurelearn.com/uploads/assets/14/0b/large_hero_140bfdc0-2f0f-408d-bf43-4ed1e4240646.jpg 2x"/></p>
</div><p style="page-break-before: always"><h2>4.1&emsp;Welcome to Topic 4</h2><div class="u-typography-bold-intro">
<p>Welcome to the final topic in Feature Engineering, Cluster Analysis.</p>
<p>Cluster analysis is generally used to classify multivariate groups of data. It is typically known as unsupervised learning. These techniques can also be used to create new features. In particular, they can potentially reduce the volume of data we have.</p>
<p>In this topic, we will explore the concepts of distance, similarity and dissimilarity. Building on these concepts we will look at methods to integrate categorical and continuous variable. In addition, they will be introduced to a range of techniques that will give them a broad experience of cluster analysis.</p>
<p>Once you have finished this topic you will be able to:</p>
<ul>
<li>Understand the various different approaches to measuring “similarity”.</li>
<li>Integrate variables of differing type in your cluster analysis.</li>
<li>Apply a number of algorithms to create new features from your data.</li>
<li>Comprehend the reasons why certain algorithms work in certain situations.</li>
</ul>
<p>As usual, we encourage to mark each step as complete and to voice any comments or questions that you may have in the comments section under various steps in the course.</p>
<p>Good luck, and we hope that you enjoy this final topic in Feature Engineering.</p>
<p><img alt="Statement of funding from Skillnet Ireland informing that: This course has been grant-aided by skillnet Ireland. Technology Ireland ICT Skillnet provides advanced training and development activities for technical and engineering staff in the ICT. Participants may be contacted by Skillnet Ireland to confirm that they attend this course.  
Information from susan.kelly@ictskillnet.com" sizes="(min-width: 1695px) 729px, (min-width: 680px) 648px, 320px" src="https://ugc.futurelearn.com/uploads/assets/14/0b/hero_140bfdc0-2f0f-408d-bf43-4ed1e4240646.jpg" srcset="https://ugc.futurelearn.com/uploads/assets/14/0b/small_hero_140bfdc0-2f0f-408d-bf43-4ed1e4240646.jpg 320w, https://ugc.futurelearn.com/uploads/assets/14/0b/hero_140bfdc0-2f0f-408d-bf43-4ed1e4240646.jpg 648w, https://ugc.futurelearn.com/uploads/assets/14/0b/large_hero_140bfdc0-2f0f-408d-bf43-4ed1e4240646.jpg 729w, https://ugc.futurelearn.com/uploads/assets/14/0b/large_hero_140bfdc0-2f0f-408d-bf43-4ed1e4240646.jpg 2x"/></p>
</div><p><h2>4.2&emsp;What is clustering?</h2><div class="u-typography-bold-intro">
<p>Clustering is the process of finding groups of objects such that the objects in a group
will be similar (or related) to one another and different from (or unrelated to) the objects in other groups.</p>
<p><img alt="Shows how we aim to minimize the Intra cluster and maximize the inter cluster distances" src="https://www.computing.dcu.ie/~amccarren/mcm_images/cluster_analysis.png"/></p>
<p><sub> (Figure 1: Chiara Renso,KDD-LAB,ISTI- CNR, Pisa, Italy)</sub></p>
<p>Clustering falls into a category of machine learning known as unsupervised learning. The idea behind it is to apply an algorithm (for which there are many) which will use a similarity or distance function, for example, to separate the data you have into groups. Unlike Principal Component Analysis (PCA) which combines columns/features, cluster analysis generally groups by row. So you are really trying to combine groups of people or objects which would have a multitude of variables/features. So, for example, you may have done a survey about the reading habits of people. You could, for example, have asked people if they worked in various sectors, had a certain level of education, where they lived or what their parents read. Now cluster analysis will attempt to group all the people based on their answers to the four questions.</p>
<p>Clustering is really useful when attempting to determine intrinsic grouping among unlabeled data. For example, we may want to subdivide the market into segmented components as follows:</p>
<h4 id="market-segmentation">Market Segmentation:</h4>
<ul>
<li>Goal: subdivide a market into distinct subsets of customers where any subset may conceivably be selected as a market target to be reached with a distinct marketing mix.</li>
</ul>
<h4 id="approach">Approach:</h4>
<ul>
<li>Collect different attributes of customers based on their geographical and lifestyle-related information.</li>
<li>Find clusters of similar customers.</li>
<li>Measure the clustering quality by observing buying patterns of customers in the same cluster vs. those from different clusters.</li>
</ul>
<p>Or alternatively, we may want to cluster documents:</p>
<h4 id="document-clustering">Document Clustering</h4>
<ul>
<li><strong>Goal</strong>: To find groups of documents that are similar to each other based on the important terms appearing in them, Figure 2.</li>
<li><strong>Approach</strong>: To identify frequently occurring terms in each document. Form a similarity measure based on the frequencies of different terms. Use it to cluster.</li>
<li><strong>Gain</strong>: Information Retrieval can utilize the clusters to relate a new document or search term to clustered documents.</li>
</ul>
<p>Clustering Points: 3204 Articles of Los Angeles Times
Similarity Measure: How many words are common in these documents after some word filtering.</p>
<table>
<thead>
<tr>
<th><strong>Category</strong></th>
<th><strong>Total Articles</strong></th>
<th><strong>Correctly Placed</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td>Financial</td>
<td>555</td>
<td>354</td>
</tr>
<tr>
<td>Foreign</td>
<td>341</td>
<td>260</td>
</tr>
<tr>
<td>National</td>
<td>273</td>
<td>36</td>
</tr>
<tr>
<td>Metro</td>
<td>943</td>
<td>746</td>
</tr>
<tr>
<td>Sports</td>
<td>738</td>
<td>573</td>
</tr>
<tr>
<td>Entertainment</td>
<td>354</td>
<td>278</td>
</tr>
</tbody>
</table>
<p>As I said previously there are many cluster algorithms, and they can be categorized as follows:</p>
<h4 id="density-based-methods">1. Density-Based Methods</h4>
<p>These methods consider the clusters as the dense region having some similarity and different from the lower dense region of the space. These methods have good accuracy and the ability to merge two clusters. Example Density-Based Spatial Clustering of Applications with Noise (DBSCAN) and Ordering Points to Identify Clustering Structure (OPTICS).</p>
<h4 id="hierarchical-based-methods">2. Hierarchical Based Methods</h4>
<p>The clusters formed in this method form a tree-type structure based on the hierarchy. New clusters are formed using the previously formed one. It is divided into two categories:</p>
<ul>
<li>Agglomerative (bottom-up approach)</li>
<li>Divisive (top-down approach).</li>
</ul>
<p>Examples of this include; Clustering Using Representatives (CURE) and Balanced Iterative Reducing Clustering and using Hierarchies (BIRCH).</p>
<h4 id="partitioning-methods">3. Partitioning Methods</h4>
<p>These methods partition the objects into k clusters and each partition forms one cluster. This method is used to optimize an objective criterion similarity function such as when the distance is a major parameter example K-means, Clustering Large Applications based upon randomized Search (CLARANS) etc.</p>
<h4 id="grid-based-methods">4. Grid-based Methods</h4>
<p>In this method, the data space is formulated into a finite number of cells that form a grid-like structure. All the clustering operation done on these grids are fast and independent of the number of data objects example Statistical Information Grid (STING), wave cluster and CLustering In Quest (CLIQUE).</p>
<p>Now if you would like some detailed notes on cluster analysis please take a look at these <a href="https://drive.google.com/open?id=1hKeS0rdeLEyGKmwK-cNzUtP4_kME9Fn6">slides</a>. I usually give these out as supplementary material in my lectures. They are very detailed and will take a bit of reading, but they are a good guide to determining which algorithm might suit your needs.</p>
<p>In the next number of steps, I will outline a number of metrics that are used to measure similarity. I will then go through a number of clustering algorithms which will include K-mean, K-medoids and DBSCAN.  Finally, we will show you how to assess your clustering techniques with a comparison of the within-cluster variation compared to the between cluster variation.</p>
<p>Read the notes in the slides and ask yourself how would you know if you have “good clusters”. As usual, post any thoughts that you may have in the comments section below.</p>
</div><p><h2>4.3&emsp;Similarity metrics</h2><div class="u-typography-bold-intro">
<p>The Euclidean distance is discussed in this step.</p>
<p>In the previous step, we talked about clustering and where it can be used. At the end of the step, I asked the question of how do we know if we have a good cluster? Well, typically a good clustering technique has the following:</p>
<ul>
<li>Will produce high-quality clusters with</li>
<li>High intra-class similarity.</li>
<li>Low inter-class similarity.</li>
</ul>
<p>The quality of a clustering result depends on both the similarity measure used by the method and its implementation and can be measured by its ability to discover some or all of the hidden patterns.</p>
<p>Measuring the quality of the clustering technique is based on how we calculate the distance/similarity. Dissimilarity/Similarity metric is usually expressed in terms of a distance function, typically a metric: <script type="math/tex">d(i, j)</script>. In addition to the <script type="math/tex">d(i,j)</script> we need a separate “quality” function that measures the “goodness” of a cluster. This could be the Mean Square Error (MSE) for example. The definitions of distance functions are usually very different for interval-scaled, boolean, categorical, ordinal ratio, and vector variables. Additionally, the weights of the importance of particular variables will be application-specific.  It can be very hard to define “similar enough” or “good enough” as the answer is typically highly subjective.</p>
<p>So generally we want the following requirements from a clustering algorithm:</p>
<ul>
<li>Scalability.</li>
<li>Ability to deal with different types of attributes.</li>
<li>Ability to handle dynamic data.</li>
<li>Discovery of clusters with arbitrary shape.</li>
<li>Minimal requirements for domain knowledge to determine input parameters.</li>
<li>Able to deal with noise and outliers.</li>
<li>Insensitive to order of input records.</li>
<li>High dimensionality.</li>
<li>Incorporation of user-specified constraints.</li>
<li>Interpretability and usability.</li>
</ul>
<h3 id="similarity">Similarity</h3>
<p>This is a considerable list and I generally give the most important to the last request for interpretability and usability. Now if you remember in <a href="https://www.futurelearn.com/courses/pre-processing-data-and-feature-impact-calculation/1">Pre-processing Data and Feature Impact Calculation</a> we discussed entropy and used the information criteria to create a discrete variable that maintained as much information from the original continuous variable. Now finding the right clustering approach is similar. We are looking for clusters that can give us new information and not just cluster for clustering sake. So the first step in this process is to decide how we calculate similarity or dissimilarity.</p>
<p>Similarity can be described as follows:</p>
<ul>
<li>Numerical measure of how alike two data objects are.</li>
<li>Is higher when objects are more alike.</li>
<li>Often falls in the range [0,1].</li>
</ul>
<p>Dissimilarity can be described as follows:</p>
<ul>
<li>Numerical measure of how different two data objects are.</li>
<li>Lower when objects are more alike.</li>
<li>Minimum dissimilarity is often 0.</li>
<li>Upper limit varies.</li>
</ul>
<p>The first measure dissimilarity we will look at is probably the most well known and is known as the Euclidean distance.</p>
<h3 id="euclidean-distance">Euclidean Distance</h3>
<p>The Euclidean distance between two points is the length of the path connecting them. Pythagorean theorem gives this distance between two points. <a href="http://en.wikipedia.org/wiki/Euclidean_distance">Euclidean distance</a> is regularly the default distance for most practitioners.</p>
<p>In Cartesian coordinates, if <strong>p</strong> = (<em>p</em><sub>1</sub>, <em>p</em><sub>2</sub>,…, <em>p</em><sub><em>n</em></sub>) and <strong>q</strong> = (<em>q</em><sub>1</sub>, <em>q</em><sub>2</sub>,…, <em>q</em><sub><em>n</em></sub>) are two points in Euclidean <em>n</em>-space, then the distance (d) from <strong>p</strong> to <strong>q</strong>, or from <strong>q</strong> to <strong>p</strong> is given by the Pythagorean formula.</p>
</div><p><h2>4.4&emsp;Manhattan distance</h2><div class="u-typography-bold-intro">
<p>The Manhattan distance gets its name from the New York City taxi drivers problem. The goal is to calculate the shortest distance between 2 points.</p>
<p>With the euclidean distance, the shortest distance will be that calculated by Pythagoras or the hypotenuse of a triangle (the green line in Figure 1). But obviously, if you are a taxi driver you cannot drive through buildings so you have to do an up and across motion.</p>
<p><img alt="Graphic representing the Manhattan grid pattern and the direction  of four links to get from the bottom left corner of grid to top right. " src="https://www.computing.dcu.ie/~amccarren/mcm_images/Manhattan_distance.svg.png"/></p>
<p><sub>(Figure 1: <a href="https://en.wikipedia.org/wiki/Taxicab_geometry">Taxicab geometry</a>)</sub></p>
<p>So we now realise we cannot drive through buildings, which rules out the Euclidean distance as an appropriate formula:</p>
<ul>
<li>
<script type="math/tex; mode=display">d(p,q)={\sqrt {\sum _{i=1}^{n}(q_{i}-p_{i})^{2}}}</script>
</li>
</ul>
<p>If we want to reflect the taxi drivers path then we should use the sides of each triangle. This  is effectively the Manhattan distance:</p>
<ul>
<li>
<script type="math/tex; mode=display">d(p,q)=\sum_i^n\lvert (q_{i}-p_{i}) \rvert</script>
</li>
</ul>
<p>This Manhattan distance metric is also known as Manhattan length, rectilinear distance, L1 distance or L1 norm, city block distance, Minkowski’s L1 distance, taxi-cab metric, or city block distance. In some cases, it can be preferable to use as a distance measure as it can put less weight on outlying points. It has also been found to be beneficial when you have a very high dimensional dataset.</p>
<p>The code below is a very simple example of how it works using the Scikit learn <a href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.manhattan_distances.html">manhattan_distances library</a>. Play about with it and compare it to the Euclidean distance. Post your thoughts on the comments section below.</p>
<p>You can access the Google Colab for this step <a href="https://colab.research.google.com/drive/1l9-VTjJIACpMN16rsEe00eEmblfpIsEO#scrollTo=S_VrPV_GYOVY">here</a>.</p>
</div><p><h2>4.5&emsp;Cosine similarity</h2><div class="u-typography-bold-intro">
<p>Cosine similarity has been used to assess how similar documents are to each other.</p>
<p>Where Euclidean distance measures the magnitude of the separation between 2 vectors, cosine similarity gives a measure of the angle between two multidimensional vectors. It is very similar to correlation where the <a href="https://brenocon.com/blog/2012/03/cosine-similarity-pearson-correlation-and-ols-coefficients/">cosine similarity</a> between centred versions of x and y, again bounded between -1 and 1.
Cosine similarity is a metric used to measure how similar the documents are irrespective of their size.</p>
<p>The maths behind this measure are derived from the Euclidean dot product:</p>
<p><img alt="{\displaystyle \mathbf {A} \cdot \mathbf {B} =\left\|\mathbf {A} \right\|\left\|\mathbf {B} \right\|\cos \theta }" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/fb9fc371e46e02d0ef51e781e7397629425856b5"/></p>
<p>this can be converted to similarity by doing some simple algebra:</p>
<p><img alt="{\displaystyle {\text{similarity}}=\cos(\theta )={\mathbf {A} \cdot \mathbf {B}  \over \|\mathbf {A} \|\|\mathbf {B} \|}={\frac {\sum \limits _{i=1}^{n}{A_{i}B_{i}}}{{\sqrt {\sum \limits _{i=1}^{n}{A_{i}^{2}}}}{\sqrt {\sum \limits _{i=1}^{n}{B_{i}^{2}}}}}},}" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/1d94e5903f7936d3c131e040ef2c51b473dd071d"/></p>
<p>Cosine similarity is generally used when the magnitude of the vector does not matter. The term <script type="math/tex">\vec{A}\|\cos{\theta}</script> tells us where the projection of vector <script type="math/tex">\vec{A}</script> lands on vector <script type="math/tex">\vec{B}</script> in Figure 1.</p>
<p><img alt="Projection of vector A landing on vetor B" src="https://www.computing.dcu.ie/~amccarren/mcm_images/Dot_Product.png"/></p>
<p><sub>(Figure 1: the projection of <script type="math/tex">\vec{A}</script> on <script type="math/tex">\vec{B}</script>, <a href="https://en.wikipedia.org/wiki/Vector_projection">Wikipedia</a>)</sub></p>
<p>The following code from Github implements Scikit learns cosine_similarity. You should notice how two vectors going in the same direction but with differing magnitudes have the same cosine similarity with <script type="math/tex">\vec{X}</script> and <script type="math/tex">\vec{Y}</script>:</p>
<script type="math/tex; mode=display">\vec{z} = [1~ 1~ 1~ 1]</script>
<script type="math/tex; mode=display">\vec{z_2} = [100 ~ 100~ 100~ 100]</script>
<p>but when we get the euclidean distance there is a vast difference.</p>
<p>Again, play with these measures. Would you expect z and z2 to be correlated with each other? Place your thoughts on the comments section below.</p>
<p>Follow this <a href="https://colab.research.google.com/drive/1VlXzF2fkbRokfFihQrwE7o-otGMi4A4d#scrollTo=yylA0jwbHLoL">link</a> to go to the Google Colab for this step.</p>
</div><p><h2>4.6&emsp;Jacardian Index</h2><div class="u-typography-bold-intro">
<p>So far in all our examples of distance/similarity measure, we have dealt with variables/features that are continuous in nature.</p>
<p>The following is an example where we compare the results of 4 tests between 3 patients. The test results are Positive/Negative and all other information is categorical.</p>
<p><img alt="Table with four rows and eight columns. First row columns named: name, gender, fever, cough, test-1, test-2, test 3 and test-4, Row two, columns named: Jack, M, Y, N, P, N, N, N, third row, columns named, Mary, F, Y, N, P, N, P, N. Fourth row, columns named, Jim, M, Y, P, N, N, N, N " src="https://www.computing.dcu.ie/~amccarren/mcm_images/jaccardian_example.png"/></p>
<p><sub>(Figure 1: Comparison of 3 patients and the results from 4 medical tests)</sub></p>
<p>In this situation, it would not make sense to use the Euclidean distance or the cosine similarity measure and so we introduce the Jaccardian index. It is also known as the Intersection over Union or <a href="https://en.wikipedia.org/wiki/Jaccard_index#Similarity_of_asymmetric_binary_attributes">Jaccard similarity coefficient</a>. The idea is to basically compare the rows in the dataset by counting the number of times both rows or subjects in our case have a positive occurrence at the same time.</p>
<p>So from the example above, we would create a contingency table such as that shown in Figure 2 below:</p>
<p><img alt="Figure 2: shows the intersection between subjects A and B" src="https://www.computing.dcu.ie/~amccarren/mcm_images/jaccardian_contigency_table.png"/></p>
<p><sub>(Figure 2: Contingency table of the comparison between 2 rows)</sub></p>
<p>So to calculate the Jaccard index we would use the following formula for a symetric variable:</p>
<script type="math/tex; mode=display">J_s=\frac{M_{00}+M_{11}}{M_{10}+M_{01}+M_{00}+M_{11}}</script>
<p>The next question you might ask is; what is the asymmetrical variable? An asymmetrical variable is a categorical variable where the likely outcome is uneven between the possible outcomes. So if you were testing patients for a rare disease then we would not expect many people to have it, but when they do we want our metrics to score this as a high occurrence. So in our example above the only symmetrical variable is gender (we are assuming an equal probability of the occurrence of male/female). Now the Jaccardian distance for both cases is effective <script type="math/tex">1-J</script> for both the symmetrical and asymmetrical cases.</p>
<p>So let’s try and calculate the Jaccardian distance for the example in Figure 1 between Jack and Mary. We will assume the following for this example:</p>
<ul>
<li>
<p>Gender is a symmetric attribute.</p>
</li>
<li>
<p>The remaining attributes are asymmetric binary.</p>
</li>
<li>
<p>Let Y and P be set to 1 and the value N be set to 0.</p>
</li>
</ul>
<p>Now the Jaccardian distance between Jack and Mary is as follows:</p>
<script type="math/tex; mode=display">d(Jack,Mary)=\frac{0~+~1}{2~+~0~+~1}=0.33</script>
<p>or the Jaccardian index is <script type="math/tex">1-d=0.666</script></p>
<p>Note that because Gender is symmetrical it was left out of the calculations. If we include Gender it will bias the results, to the point that the distance would go from 0.33 to 0.5. To answer this question one really needs to look at the problem being addressed. In some cases, the Gender variable may be asymmetrical. For example, in a breast cancer study, it is rare for a male to show up, but it is possible.</p>
<h4 id="multinomial-data">Multinomial data</h4>
<p>A generalization of the binary variable in that it can take more than 2 states, e.g., red, yellow, blue, green is as follows:</p>
<p><script type="math/tex">m</script>: # of matches, <script type="math/tex">p</script>: total # of variables</p>
<script type="math/tex; mode=display">d(i,j)=\frac{p-m}{p}</script>
<h4 id="ordinal-data">Ordinal Data</h4>
<p>If you have an ordinal variable do the following:</p>
<ul>
<li>replace <script type="math/tex">x_{if}</script> by their rank <script type="math/tex">r_{if} \in \{1,..M_f\}</script></li>
</ul>
<p>map the range of each variable onto [0, 1] by replacing i-th object in the f-th variable by</p>
<script type="math/tex; mode=display">Z_{if}=\frac{r_{if}-1}{M_f-1}</script>
<p>compute the dissimilarity using methods for interval-scaled variables.</p>
<p>If you have a ratio-scaled variable (e.g. weight) do not treat it as an interval scaled variable as the scale can be distorted. The solution is to apply a logarithmic transformation.</p>
<h3 id="example">Example</h3>
<p>The following <a href="https://colab.research.google.com/drive/1IoUPaOd6Nhz2yUp0m0Vug9psU1kYkgpe#scrollTo=xEnRwpr8aMJb">code</a> shows you how to calculate the Jaccardian distance for the example I gave in Figure 1. It relies on the <a href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.jaccard.html">Python Scipy</a> library and assumes asymmetry. Have a go at trying to calculate the overall distance matrix for the whole dataset. As usual, leave your thoughts on the comments board.</p>
</div>